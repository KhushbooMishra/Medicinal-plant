{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm==0.9.2 transformers sentence-transformers torchmetrics scikit-learn statsmodels\n",
        "# If you use BLIP captioning and sentence embeddings uncomment:\n",
        "!pip install -q transformers sentence-transformers"
      ],
      "metadata": {
        "id": "hd_cAqcqLn-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSlTYfwEL38L",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763828350104,
          "user_tz": -330,
          "elapsed": 31088,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "e09a88dc-fb2f-4fca-e772-8e9cb76e23de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "# !cp '/content/drive/MyDrive/projectResearchPaper/plant/kaggle.json' ~/.kaggle/\n",
        "!cp '/content/drive/MyDrive/test/kaggle.json' ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "uBKQz0F5Lzxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "!kaggle datasets download warcoder/indian-medicinal-plant-image-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CcTPc2SL71A",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763828358062,
          "user_tz": -330,
          "elapsed": 5968,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "f4804915-1af5-46da-ca6b-4e35984b4f0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/indian-medicinal-plant-image-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "jUr-ByJNL-oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os, random, math, time, json, zipfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# TensorFlow / Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, applications\n",
        "\n",
        "# PyTorch & timm\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from statsmodels.stats.contingency_tables import mcnemar"
      ],
      "metadata": {
        "id": "OWhl-6G8MLJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repro\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "eUZ_1NLFlMeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763828397070,
          "user_tz": -330,
          "elapsed": 67,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "4cab8440-1a80-484d-e940-a52dd9f2487a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration (edit as needed)\n",
        "# -------------------------\n",
        "DATA_DIR = '/content/Medicinal plant dataset'  # change if different\n",
        "OUT_ROOT = '/content/experiments'                      # where results will go\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "PLOTS_DIR = os.path.join(OUT_ROOT, 'plots'); os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "DATA_DIR = DATA_DIR\n",
        "IMG_SIZE = (224,224)            # H,W\n",
        "BATCH_SIZE_TF = 24\n",
        "NUM_WORKERS = 4\n",
        "EPOCHS_TF = 12\n",
        "EPOCHS_PY = 12\n",
        "FINE_TUNE_EPOCHS_TF = 10\n",
        "LR_TF = 1e-3\n",
        "LR_PY = 2e-5\n",
        "FINE_TUNE_LR_TF = 1e-5\n",
        "WEIGHT_DECAY = 1e-5\n",
        "REPEATS = 1    # reduce during debugging; set >1 if you want repeats\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", DEVICE)\n",
        "import pathlib, os, random, shutil\n",
        "data_dir = '/content/Medicinal plant dataset'\n",
        "data_dir_path = pathlib.Path(data_dir)\n",
        "classes = [d.name for d in data_dir_path.iterdir() if d.is_dir()]"
      ],
      "metadata": {
        "id": "Ub4EmuR3lidK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763828397080,
          "user_tz": -330,
          "elapsed": 17,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "cbfc07a5-2679-46ec-9e5b-46938d6e4bd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Dataset (Keras-style listing)\n",
        "# -------------------------\n",
        "class FolderDatasetListing:\n",
        "    def __init__(self, root_dir):\n",
        "        self.root = Path(root_dir)\n",
        "        classes = sorted([p.name for p in self.root.iterdir() if p.is_dir()])\n",
        "        self.class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "        self.items = []\n",
        "        for c in classes:\n",
        "            for f in (self.root / c).glob('*'):\n",
        "                if f.suffix.lower() in ('.jpg','.jpeg','.png','.webp'):\n",
        "                    self.items.append((str(f), self.class_to_idx[c]))\n",
        "        print(f\"Found {len(self.items)} images across {len(classes)} classes.\")\n",
        "        self.classes = classes\n",
        "        self.num_classes = len(classes)\n",
        "\n",
        "full_dataset = FolderDatasetListing(DATA_DIR)\n",
        "num_classes = full_dataset.num_classes\n",
        "idx_to_class = {v:k for k,v in full_dataset.class_to_idx.items()}\n"
      ],
      "metadata": {
        "id": "z4Mx4AY6l2-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763828397202,
          "user_tz": -330,
          "elapsed": 63,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "6d8d5b7f-8977-4db8-ac09-67634be34a89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train/val/test split indices\n",
        "n = len(full_dataset.items)\n",
        "train_n = int(0.8 * n)\n",
        "val_n = int(0.1 * n)\n",
        "test_n = n - train_n - val_n\n",
        "indices = list(range(n))\n",
        "random.shuffle(indices)\n",
        "train_idxs = indices[:train_n]\n",
        "val_idxs   = indices[train_n:train_n+val_n]\n",
        "test_idxs  = indices[train_n+val_n:]"
      ],
      "metadata": {
        "id": "SFV80-G9l8T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to build tf.data from indices\n",
        "def make_tf_dataset_from_indices(indices_list, batch_size=BATCH_SIZE_TF, transform=None, shuffle=False):\n",
        "    paths = [full_dataset.items[i][0] for i in indices_list]\n",
        "    labels = [full_dataset.items[i][1] for i in indices_list]\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    def _load(path, label):\n",
        "        image = tf.io.read_file(path)\n",
        "        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "        image = tf.image.resize(image, IMG_SIZE)\n",
        "        return image, label\n",
        "    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(1024)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds_tf = make_tf_dataset_from_indices(train_idxs, batch_size=BATCH_SIZE_TF, shuffle=True)\n",
        "val_ds_tf   = make_tf_dataset_from_indices(val_idxs, batch_size=BATCH_SIZE_TF, shuffle=False)\n",
        "test_ds_tf  = make_tf_dataset_from_indices(test_idxs, batch_size=BATCH_SIZE_TF, shuffle=False)\n"
      ],
      "metadata": {
        "id": "W-Wdb8IBmBdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Also keep lists for PyTorch inference\n",
        "train_items = [full_dataset.items[i] for i in train_idxs]\n",
        "val_items = [full_dataset.items[i] for i in val_idxs]\n",
        "test_items = [full_dataset.items[i] for i in test_idxs]\n"
      ],
      "metadata": {
        "id": "O3On3B-8mIc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# TensorFlow / Keras Models (from your code) - fixed and ready\n",
        "# -------------------------\n",
        "from tensorflow.keras import applications, optimizers, callbacks, regularizers\n",
        "\n",
        "def build_mobilenetv2(input_shape=(*IMG_SIZE,3)):\n",
        "    base = applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable=False\n",
        "    x = layers.GlobalAveragePooling2D()(base.output)\n",
        "    #  add L2 and lower dropout to reduce underfitting\n",
        "    x = layers.Dense(128, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(base.input, out)\n",
        "    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_resnet152(input_shape=(*IMG_SIZE,3)):\n",
        "    base = applications.ResNet152(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable=False\n",
        "    x = layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = layers.Dense(128, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(base.input, out)\n",
        "    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_xception(input_shape=(*IMG_SIZE,3)):\n",
        "    base = applications.Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable=False\n",
        "    x = layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = layers.Dense(256, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(base.input, out)\n",
        "    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_mobilenetv3(input_shape=(*IMG_SIZE,3), model_type='large'):\n",
        "    try:\n",
        "        if model_type=='large':\n",
        "            base = applications.MobileNetV3Large(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "        else:\n",
        "            base = applications.MobileNetV3Small(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    except Exception as e:\n",
        "        print(\"MobileNetV3 not available; falling back to MobileNetV2. Error:\", e)\n",
        "        return build_mobilenetv2(input_shape)\n",
        "    base.trainable=False\n",
        "    x = layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = layers.Dense(128, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(base.input, out)\n",
        "    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_resnet50_for_feats(input_shape=(*IMG_SIZE,3)):\n",
        "    base = applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base.trainable=False\n",
        "    x = layers.GlobalAveragePooling2D()(base.output)\n",
        "    model = Model(base.input, x)\n",
        "    return model\n",
        "\n",
        "def build_vit_custom_improved(input_shape=(*IMG_SIZE,3), num_classes=num_classes,\n",
        "                              patch_size=16, projection_dim=128, transformer_layers=6,\n",
        "                              num_heads=8, mlp_dim=256):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    patches = layers.Conv2D(filters=projection_dim, kernel_size=patch_size, strides=patch_size, padding=\"valid\")(inputs)\n",
        "    patches = layers.Reshape((-1, projection_dim))(patches)\n",
        "    num_patches = patches.shape[1]\n",
        "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "    position_embeddings = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n",
        "    encoded_patches = patches + position_embeddings\n",
        "    for _ in range(transformer_layers):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        mlp_output = keras.Sequential([\n",
        "            layers.Dense(mlp_dim, activation='gelu'),\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Dense(projection_dim),\n",
        "            layers.Dropout(0.1)\n",
        "        ])(x3)\n",
        "        encoded_patches = layers.Add()([x2, mlp_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(mlp_dim, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs, name=\"VisionTransformer_Custom_Improved\")\n",
        "    optimizer = optimizers.Adam(learning_rate=3e-4)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -------------------------\n",
        "# Training helper for Keras: safe compile & fit\n",
        "# -------------------------\n",
        "def compile_and_fit(model, name, epochs=EPOCHS_TF, train_ds=train_ds_tf, val_ds=val_ds_tf, outdir=OUT_ROOT):\n",
        "    cb = [\n",
        "        callbacks.ModelCheckpoint(os.path.join(outdir, f'{name}_best.h5'), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
        "        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "        callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n",
        "    ]\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cb, verbose=2)\n",
        "    model.save(os.path.join(outdir, f'{name}_final.h5'))\n",
        "    return history"
      ],
      "metadata": {
        "id": "dlwFimTimQxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Build models list (Keras)\n",
        "# -------------------------\n",
        "models_to_train = {}\n",
        "models_to_train['MobileNetV2'] = build_mobilenetv2()\n",
        "models_to_train['ResNet152'] = build_resnet152()\n",
        "models_to_train['MobileNetV3'] = build_mobilenetv3(model_type='large')\n",
        "models_to_train['ResNet50_feats'] = build_resnet50_for_feats()\n",
        "# models_to_train['VisionTransformer_Custom'] = build_vit_custom_improved()\n",
        "\n",
        "# Optional: don't instantiate Xception on small runtimes (it is heavy)\n",
        "# models_to_train['Xception'] = build_xception()"
      ],
      "metadata": {
        "id": "g5TpC0U6mX5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763828409420,
          "user_tz": -330,
          "elapsed": 10213,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "1f36bf37-553f-4860-ae63-3db1f1327354"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Training Keras classifiers (with head training + fine-tune)\n",
        "# -------------------------\n",
        "trained_classifiers = {}\n",
        "histories = {}\n",
        "reports = {}\n",
        "cms = {}\n",
        "data_dir = OUT_ROOT\n",
        "\n",
        "def plot_confusion(cm, class_names, save_path, title='Confusion Matrix'):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True'); plt.xlabel('Predicted')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path); plt.close()\n",
        "\n",
        "def save_plot_history(hist, save_path):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(hist.history.get('loss',[]), label='train_loss'); plt.plot(hist.history.get('val_loss',[]), label='val_loss'); plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(hist.history.get('accuracy',[]), label='train_acc'); plt.plot(hist.history.get('val_accuracy',[]), label='val_acc'); plt.legend()\n",
        "    plt.tight_layout(); plt.savefig(save_path); plt.close()\n",
        "\n",
        "for name, model in list(models_to_train.items()):\n",
        "    # If model is a feature extractor (ResNet50_feats), build a classifier head\n",
        "    if name == 'ResNet50_feats':\n",
        "        print(f\"Building classifier for {name}\")\n",
        "        base = model\n",
        "        inputs = base.input\n",
        "        x = base.output\n",
        "        x = layers.Dense(256, activation='relu',\n",
        "                         kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)   # \ud83d\udd35 NEW LINE: add L2\n",
        "        x = layers.Dropout(0.2)(x)  # \ud83d\udfe1 CHANGE THIS (lower dropout)\n",
        "        out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "        clf = Model(inputs, out)\n",
        "        clf.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        # Train head then fine-tune:\n",
        "        print(f\"Training head for {name} ...\")\n",
        "        hist_head = compile_and_fit(clf, name, epochs=EPOCHS_TF)\n",
        "        histories[name] = hist_head\n",
        "        trained_classifiers[name] = clf\n",
        "    else:\n",
        "        print(f\"Training {name} - head first, then fine-tune\")\n",
        "        # ---- Head training (base already frozen in builders) ----\n",
        "        #  train initial head with existing compile in builder\n",
        "        cb_head = [\n",
        "            callbacks.ModelCheckpoint(os.path.join(OUT_ROOT, f'{name}_head_best.h5'), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
        "            callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "            callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n",
        "        ]\n",
        "        hist_head = model.fit(train_ds_tf, validation_data=val_ds_tf, epochs=EPOCHS_TF, callbacks=cb_head, verbose=2)\n",
        "        histories[f\"{name}_head\"] = hist_head\n",
        "        # \ud83d\udd35 NEW LINE: Save head model\n",
        "        model.save(os.path.join(OUT_ROOT, f'{name}_head.h5'))\n",
        "\n",
        "        # ---- Fine-tune: unfreeze top layers ----\n",
        "        # unfreeze last UNFREEZE_AT layers of the base (if model has .layers and includes a pretrained base)\n",
        "        # how many layers to unfreeze (tune if needed)\n",
        "        UNFREEZE_AT = 100   #  (number of layers from end to unfreeze)\n",
        "        # set UNFREEZE_AT=None to unfreeze all layers\n",
        "        # Try to detect base model layers: often layer names include 'mobilenet'/'resnet' etc.\n",
        "        try:\n",
        "            # find a base inside the model by checking for a layer with name 'input' and then base layers\n",
        "            # We'll attempt to find the deepest pretrained layer group by inspecting layer names\n",
        "            # If model was built with include_top=False, early layers will be part of model.layers\n",
        "            if UNFREEZE_AT is None:\n",
        "                for layer in model.layers:\n",
        "                    layer.trainable = True\n",
        "            else:\n",
        "                # Unfreeze last UNFREEZE_AT trainable layers\n",
        "                # NOTE: this is conservative \u2014 you can change UNFREEZE_AT as needed\n",
        "                trainable_count = 0\n",
        "                for layer in model.layers[::-1]:\n",
        "                    if trainable_count < UNFREEZE_AT:\n",
        "                        layer.trainable = True\n",
        "                        trainable_count += 1\n",
        "                    else:\n",
        "                        layer.trainable = False\n",
        "            # recompile with a lower lr for fine-tuning\n",
        "            model.compile(optimizer=optimizers.Adam(FINE_TUNE_LR_TF),\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "            #  fine-tune\n",
        "            hist_ft = model.fit(train_ds_tf, validation_data=val_ds_tf,\n",
        "                                epochs=EPOCHS_TF + FINE_TUNE_EPOCHS_TF,\n",
        "                                initial_epoch=hist_head.epoch[-1] + 1 if hasattr(hist_head, 'epoch') and len(hist_head.epoch)>0 else 0,\n",
        "                                callbacks=[\n",
        "                                    callbacks.ModelCheckpoint(os.path.join(OUT_ROOT, f'{name}_ft_best.h5'), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
        "                                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
        "                                    callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n",
        "                                ],\n",
        "                                verbose=2)\n",
        "            histories[name] = hist_ft\n",
        "            trained_classifiers[name] = model\n",
        "            #  save final finetuned model\n",
        "            model.save(os.path.join(OUT_ROOT, f'{name}_finetuned.h5'))\n",
        "        except Exception as e:\n",
        "            print(\"Warning: fine-tuning step failed for\", name, \" \u2014 skipping fine-tune. Error:\", e)\n",
        "            trained_classifiers[name] = model\n",
        "            histories[name] = hist_head\n",
        "\n",
        "    # Evaluate on test set (create generator)\n",
        "    test_paths_local = [full_dataset.items[i][0] for i in test_idxs]\n",
        "    # use Keras predict on batched numpy arrays\n",
        "    def keras_predict_from_paths(model, paths, batch=32, target_size=IMG_SIZE):\n",
        "        arrs=[]\n",
        "        preds=[]\n",
        "        for i in range(0,len(paths),batch):\n",
        "            batch_paths = paths[i:i+batch]\n",
        "            imgs=[]\n",
        "            for p in batch_paths:\n",
        "                im = Image.open(p).resize(target_size)\n",
        "                a = np.array(im).astype('float32')/255.0\n",
        "                imgs.append(a)\n",
        "            xb = np.stack(imgs,0)\n",
        "            probs = model.predict(xb, verbose=0)\n",
        "            preds.append(np.argmax(probs,axis=1))\n",
        "        return np.concatenate(preds, axis=0)\n",
        "    preds = keras_predict_from_paths(trained_classifiers[name], test_paths_local, batch=32)\n",
        "    y_true = np.array([full_dataset.items[i][1] for i in test_idxs])\n",
        "    rep = classification_report(y_true, preds, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, preds)\n",
        "    reports[name] = rep\n",
        "    cms[name] = cm\n",
        "    pd.DataFrame(rep).transpose().to_csv(os.path.join(data_dir, f\"{name}_classification_report.csv\"))\n",
        "    np.save(os.path.join(data_dir, f\"{name}_cm.npy\"), cm)\n",
        "    plot_confusion(cm, full_dataset.classes, os.path.join(data_dir, f\"{name}_confusion.png\"), title=f\"{name} Confusion Matrix\")\n",
        "    # choose history to save: prefer finetune history if present\n",
        "    save_plot_history(histories.get(name, histories.get(f\"{name}_head\")), os.path.join(data_dir, f\"{name}_train_curve.png\"))\n",
        "\n",
        "print(\"Keras training/evaluation done. Artifacts in\", data_dir)"
      ],
      "metadata": {
        "id": "wCbtu5U4mgI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763830843827,
          "user_tz": -330,
          "elapsed": 2434403,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "f57e6d46-58b0-46e3-c267-12b42016e595"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# PyTorch section: Swin, DeiT-small, CoAtNet0 (timm)\n",
        "# -------------------------\n",
        "# Dataset wrapper for PyTorch using same items lists\n",
        "class ImageFolderDatasetFromList(Dataset):\n",
        "    def __init__(self, items_list, transform=None):\n",
        "        self.items = items_list\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self, idx):\n",
        "        p, lbl = self.items[idx]\n",
        "        img = Image.open(p).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, int(lbl), p\n",
        "\n",
        "train_transform_pt = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE[0], scale=(0.6,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform_pt = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "train_ds_pt = ImageFolderDatasetFromList(train_items, transform=train_transform_pt)\n",
        "val_ds_pt   = ImageFolderDatasetFromList(val_items, transform=val_transform_pt)\n",
        "test_ds_pt  = ImageFolderDatasetFromList(test_items, transform=val_transform_pt)\n",
        "\n",
        "train_loader_pt = DataLoader(train_ds_pt, batch_size=32, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader_pt   = DataLoader(val_ds_pt, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)\n",
        "test_loader_pt  = DataLoader(test_ds_pt, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)\n"
      ],
      "metadata": {
        "id": "IFz36cuHmlUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763830843829,
          "user_tz": -330,
          "elapsed": 18,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "690e5c78-e298-41b6-c3bb-181cd2cc5a54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# factories\n",
        "def make_swin_tiny(num_classes, pretrained=True):\n",
        "    return timm.create_model('swin_tiny_patch4_window7_224', pretrained=pretrained, num_classes=num_classes)\n",
        "def make_deit_small_distilled(num_classes, pretrained=True):\n",
        "    return timm.create_model('deit_small_distilled_patch16_224', pretrained=pretrained, num_classes=num_classes)\n",
        "def make_coatnet0(num_classes, pretrained=True):\n",
        "    return timm.create_model('coatnet_0', pretrained=pretrained, num_classes=num_classes)\n",
        "\n",
        "pytorch_factories = {'swin_tiny': make_swin_tiny, 'deit_small_distilled': make_deit_small_distilled, 'coatnet_0': make_coatnet0}\n"
      ],
      "metadata": {
        "id": "fE2fIVnsmorH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### >>> NEW: Print Model Summary (Swin / DeiT / any timm model)\n",
        "\n",
        "import torch\n",
        "from torchsummary import summary  # pip install torchsummary if needed\n",
        "\n",
        "def show_model_summary(factory, img_size=(3,224,224)):\n",
        "    # Create model WITHOUT classifier \u2014 good for transfer learning\n",
        "    model = factory(num_classes)\n",
        "    model.eval()\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    print(\"\\n================ MODEL ARCHITECTURE ================\")\n",
        "    print(model)\n",
        "\n",
        "    # Try torchsummary (may fail for some timm models \u2014 ignore errors)\n",
        "    try:\n",
        "        summary(model, img_size)\n",
        "    except Exception as e:\n",
        "        print(\"\\n(torchsummary could not parse some models \u2014 safe to ignore)\")\n",
        "\n",
        "\n",
        "# ---- FIX: Use correct timm CoAtNet model name ----\n",
        "pytorch_factories[\"coatnet_0\"] = lambda num_classes: timm.create_model(\n",
        "    \"coatnet_0_rw_224.sw_in1k\", pretrained=True, num_classes=num_classes\n",
        ")\n",
        "\n",
        "\n",
        "show_model_summary(pytorch_factories[\"swin_tiny\"])\n",
        "show_model_summary(pytorch_factories[\"deit_small_distilled\"])\n",
        "show_model_summary(pytorch_factories[\"coatnet_0\"])\n"
      ],
      "metadata": {
        "id": "jJkypvmB-ZMA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763831622062,
          "user_tz": -330,
          "elapsed": 11126,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "899f46c21b7748f5b000210f28e988a7",
            "597f2957e9c14ced8d904e92c4b910d9",
            "53c9c2deacde4ff7843f7ad143c54989",
            "ba5545e2a9964ee4841d87155cc8ded8",
            "be075969092d411c8b1fb5ffd55f930d",
            "ee95993bd2704cab956d204b5bfbebaa",
            "ffe2299229a94ef1846a874f48242389",
            "98250c8536b54f8f8832f29dd87d58cf",
            "73d5bc66f68541eab7cfe02d245e673c",
            "bcd5b71c55cc4425a1e545c9b8cf7208",
            "f8883c1f4f664982b76f6eb4f70de164"
          ]
        },
        "collapsed": true,
        "outputId": "de061ffb-bc37-4e85-e43b-ae1f8e406499"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed PyTorch training loop cell \u2014 paste & run (replaces the broken block)\n",
        "import os, math, traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "\n",
        "# assume pytorch_factories, train_loader_pt, val_loader_pt, test_loader_pt, num_classes, OUT_ROOT, DEVICE, EPOCHS_PY, LR_PY, WEIGHT_DECAY exist\n",
        "\n",
        "def freeze_model_fraction(model, fraction=0.65):\n",
        "    params = list(model.parameters())\n",
        "    total = len(params)\n",
        "    freeze_count = int(total * fraction)\n",
        "    freeze_count = min(freeze_count, max(0, total-1))\n",
        "    for i,p in enumerate(params):\n",
        "        p.requires_grad = False if i < freeze_count else True\n",
        "\n",
        "def unfreeze_last_n_params(model, n=4):\n",
        "    params = list(model.parameters())\n",
        "    total = len(params)\n",
        "    for i in range(max(0, total - n), total):\n",
        "        params[i].requires_grad = True\n",
        "\n",
        "def train_and_evaluate_pytorch(factory, name, epochs=EPOCHS_PY, lr=LR_PY, out_dir=OUT_ROOT):\n",
        "    model = factory(num_classes).to(DEVICE)\n",
        "    # safe freeze\n",
        "    freeze_model_fraction(model, fraction=0.65)\n",
        "    trainable = [p for p in model.parameters() if p.requires_grad]\n",
        "    if len(trainable) == 0:\n",
        "        unfreeze_last_n_params(model, n=8)\n",
        "        trainable = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.AdamW(trainable, lr=lr, weight_decay=WEIGHT_DECAY)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val = -1.0\n",
        "    history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "    last_ckpt = os.path.join(out_dir, f'{name}_last.pth')\n",
        "    best_ckpt = os.path.join(out_dir, f'{name}_best.pth')\n",
        "\n",
        "    try:\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            run_loss=0.0; correct=0; total=0\n",
        "            for imgs, labels, _p in train_loader_pt:\n",
        "                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                out = model(imgs)\n",
        "                loss = criterion(out, labels)\n",
        "                loss.backward(); optimizer.step()\n",
        "                run_loss += float(loss.item()) * imgs.size(0)\n",
        "                preds = out.argmax(dim=1)\n",
        "                correct += (preds==labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "            train_loss = run_loss/total if total>0 else 0.0\n",
        "            train_acc = correct/total if total>0 else 0.0\n",
        "\n",
        "            # validation\n",
        "            model.eval()\n",
        "            vloss=0.0; vcorrect=0; vtotal=0\n",
        "            with torch.no_grad():\n",
        "                for imgs, labels, _p in val_loader_pt:\n",
        "                    imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
        "                    out = model(imgs)\n",
        "                    loss = criterion(out, labels)\n",
        "                    vloss += float(loss.item()) * imgs.size(0)\n",
        "                    preds = out.argmax(dim=1)\n",
        "                    vcorrect += (preds==labels).sum().item()\n",
        "                    vtotal += labels.size(0)\n",
        "            val_loss = vloss/vtotal if vtotal>0 else 0.0\n",
        "            val_acc = vcorrect/vtotal if vtotal>0 else 0.0\n",
        "\n",
        "            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
        "            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
        "\n",
        "            print(f'{name} Ep{ep+1}/{epochs} train_acc={train_acc:.4f} val_acc={val_acc:.4f}')\n",
        "\n",
        "            # checkpoint last\n",
        "            torch.save(model.state_dict(), last_ckpt)\n",
        "            # checkpoint best\n",
        "            if val_acc > best_val:\n",
        "                best_val = val_acc\n",
        "                torch.save(model.state_dict(), best_ckpt)\n",
        "        # end epochs\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training {name}: {e}\")\n",
        "        traceback.print_exc()\n",
        "    # Ensure we have a checkpoint: prefer best, else last, else current state\n",
        "    ckpt_to_load = best_ckpt if os.path.exists(best_ckpt) else (last_ckpt if os.path.exists(last_ckpt) else None)\n",
        "    if ckpt_to_load is not None:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(ckpt_to_load, map_location=DEVICE))\n",
        "        except Exception:\n",
        "            # try non-strict load\n",
        "            model.load_state_dict(torch.load(ckpt_to_load, map_location=DEVICE), strict=False)\n",
        "    else:\n",
        "        print(f\"Warning: No checkpoint saved for {name}; using current in-memory model for testing.\")\n",
        "\n",
        "    # test\n",
        "    y_true = []; y_pred = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, _p in test_loader_pt:\n",
        "            imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n",
        "            out = model(imgs)\n",
        "            preds = out.argmax(dim=1).cpu().numpy()\n",
        "            y_pred.extend(preds.tolist()); y_true.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # save artifacts\n",
        "    pd.DataFrame(rep).transpose().to_csv(os.path.join(out_dir, f'{name}_classification_report.csv'))\n",
        "    np.save(os.path.join(out_dir, f'{name}_cm.npy'), cm)\n",
        "    # history plot (safe: may be empty)\n",
        "    try:\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(history['train_loss'], label='train_loss'); plt.plot(history['val_loss'], label='val_loss'); plt.legend()\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(history['train_acc'], label='train_acc'); plt.plot(history['val_acc'], label='val_acc'); plt.legend()\n",
        "        plt.suptitle(name)\n",
        "        plt.tight_layout(); plt.savefig(os.path.join(out_dir, f'{name}_history.png')); plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return history, rep, cm\n",
        "\n",
        "# Initialize results dict (must be outside function)\n",
        "pytorch_results = {}\n",
        "\n",
        "# Train each pytorch factory and store results safely\n",
        "for name, fac in pytorch_factories.items():\n",
        "    try:\n",
        "        print(\"Training PyTorch model:\", name)\n",
        "        hist, rep, cm = train_and_evaluate_pytorch(fac, name, epochs=EPOCHS_PY, lr=LR_PY, out_dir=OUT_ROOT)\n",
        "        pytorch_results[name] = {'history': hist, 'report': rep, 'cm': cm}\n",
        "    except Exception as e:\n",
        "        # catch-all ensures loop continues for other models\n",
        "        print(\"Error training\", name, \"->\", e)\n",
        "        traceback.print_exc()\n",
        "        pytorch_results[name] = {'error': str(e)}\n",
        "\n",
        "print(\"PyTorch training done. Artifacts in\", OUT_ROOT)\n",
        "print(\"Summary of PyTorch results keys:\", list(pytorch_results.keys()))\n"
      ],
      "metadata": {
        "id": "EtXsjdKIm4IF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763833673248,
          "user_tz": -330,
          "elapsed": 1992322,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "3f14e771-b622-407a-a56a-958cebd567a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FULL: INTEGRATED FEW-SHOT + HYBRID FUSION PIPELINE (READY-TO-PASTE)\n",
        "# - Robust _make_embedding_extractor and _embed_batch (probe backbone outputs)\n",
        "# - Handles varied backbone output shapes (spatial / token / pooled)\n",
        "# - Prints inferred feature dims for debugging\n",
        "# - Marks new/updated sections with comments\n",
        "# ============================================================\n",
        "\n",
        "### Imports\n",
        "import os, random, math, time, warnings\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# --- Configurable params (change if needed) ---\n",
        "FEWSHOT_EPISODES = 200        # increase to 1000+ for final results\n",
        "FUSION_EMBED_DIM = 256       # per-backbone embedding size\n",
        "FUSION_BACKBONES = ['swin_tiny','deit_small_distilled','coatnet_0']  # must be keys in pytorch_factories\n",
        "SAVE_OUTDIR = globals().get('OUT_ROOT', '/content/experiments')\n",
        "PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(SAVE_OUTDIR, 'plots'))\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# Required globals check\n",
        "required_globals = ['pytorch_factories','train_items','val_items','test_items','num_classes','DEVICE']\n",
        "missing = [g for g in required_globals if g not in globals()]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing required globals in notebook environment: {missing}. Run earlier cells first.\")\n",
        "\n",
        "# --------------------------\n",
        "# Robust fallback: _make_embedding_extractor (probes backbone to infer features)\n",
        "# --------------------------\n",
        "if 'make_embedding_extractor' in globals():\n",
        "    _make_embedding_extractor = make_embedding_extractor\n",
        "else:\n",
        "    import torch, torch.nn as nn, torch.nn.functional as F\n",
        "    from torchvision import transforms\n",
        "\n",
        "    def _make_embedding_extractor(factory, embed_dim=FUSION_EMBED_DIM, img_size=(224,224)):\n",
        "        \"\"\"\n",
        "        Create (backbone, proj) where proj matches the backbone's actual output feature dimension.\n",
        "        Probes the model with a dummy input to infer shape.\n",
        "        \"\"\"\n",
        "        model = factory(num_classes)\n",
        "        try:\n",
        "            if hasattr(model, \"reset_classifier\"):\n",
        "                model.reset_classifier(0)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        model = model.to(DEVICE)\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, 3, img_size[0], img_size[1], device=DEVICE)\n",
        "            try:\n",
        "                feats = model.forward_features(dummy) if hasattr(model, 'forward_features') else model(dummy)\n",
        "            except Exception:\n",
        "                # last resort: try normal forward and accept result\n",
        "                feats = model(dummy)\n",
        "\n",
        "        # Deduce feature vector dim and create projection head accordingly\n",
        "        if torch.is_tensor(feats):\n",
        "            if feats.ndim == 4:\n",
        "                feat_dim = feats.shape[1]\n",
        "                proj = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n",
        "            elif feats.ndim == 3:\n",
        "                feat_dim = feats.shape[2]\n",
        "                proj = nn.Sequential(nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n",
        "            elif feats.ndim == 2:\n",
        "                feat_dim = feats.shape[1]\n",
        "                proj = nn.Sequential(nn.Linear(feat_dim, embed_dim))\n",
        "            else:\n",
        "                feat_dim = embed_dim\n",
        "                proj = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n",
        "        else:\n",
        "            # non-tensor fallback\n",
        "            feat_dim = embed_dim\n",
        "            proj = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n",
        "\n",
        "        proj = proj.to(DEVICE)\n",
        "        for p in model.parameters(): p.requires_grad = False\n",
        "        for p in proj.parameters(): p.requires_grad = True\n",
        "\n",
        "        # attach attribute for later inspection\n",
        "        try:\n",
        "            model._inferred_feat_dim = int(feat_dim)\n",
        "        except Exception:\n",
        "            model._inferred_feat_dim = None\n",
        "        return model, proj\n",
        "\n",
        "# --------------------------\n",
        "# Robust fallback: _embed_batch (handles 4D/3D/2D features)\n",
        "# --------------------------\n",
        "if 'embed_batch' in globals():\n",
        "    _embed_batch = embed_batch\n",
        "else:\n",
        "    import torch, torch.nn.functional as F\n",
        "    from torchvision import transforms\n",
        "\n",
        "    def _embed_batch(backbone, proj, paths, img_size=(224,224), batch_size=16):\n",
        "        backbone.eval()\n",
        "        if proj is not None:\n",
        "            proj.eval()\n",
        "\n",
        "        tfm = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "        ])\n",
        "        device = DEVICE\n",
        "        embeds = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(paths), batch_size):\n",
        "                batch_paths = paths[i:i+batch_size]\n",
        "                imgs = []\n",
        "                for p in batch_paths:\n",
        "                    im = Image.open(p).convert('RGB')\n",
        "                    imgs.append(tfm(im))\n",
        "                x = torch.stack(imgs, dim=0).to(device)\n",
        "\n",
        "                # get features\n",
        "                if hasattr(backbone, 'forward_features'):\n",
        "                    feats = backbone.forward_features(x)\n",
        "                else:\n",
        "                    try:\n",
        "                        feats = backbone(x)\n",
        "                    except Exception:\n",
        "                        feats = backbone(x)\n",
        "\n",
        "                # handle shapes\n",
        "                if feats.ndim == 4:\n",
        "                    # spatial map (B, C, H, W)\n",
        "                    # If proj starts with Linear, pool first; else let proj handle pooling\n",
        "                    try:\n",
        "                        first_mod = next(iter(proj)) if isinstance(proj, torch.nn.Sequential) else None\n",
        "                    except Exception:\n",
        "                        first_mod = None\n",
        "\n",
        "                    if isinstance(first_mod, nn.Linear):\n",
        "                        vec = F.adaptive_avg_pool2d(feats, 1).view(feats.shape[0], -1)\n",
        "                        emb = proj(vec)\n",
        "                    else:\n",
        "                        emb = proj(feats)\n",
        "                elif feats.ndim == 3:\n",
        "                    # token sequence (B, N, D) -> average tokens\n",
        "                    vec = feats.mean(dim=1)\n",
        "                    # try to use proj directly; if incompatible, do a quick linear mapping\n",
        "                    try:\n",
        "                        emb = proj(vec)\n",
        "                    except Exception:\n",
        "                        tmp_lin = nn.Linear(vec.shape[1], FUSION_EMBED_DIM).to(device)\n",
        "                        emb = tmp_lin(vec)\n",
        "                elif feats.ndim == 2:\n",
        "                    vec = feats\n",
        "                    emb = proj(vec) if proj is not None else vec\n",
        "                else:\n",
        "                    vec = feats.view(feats.shape[0], -1)\n",
        "                    emb = proj(vec) if proj is not None else vec\n",
        "\n",
        "                emb = F.normalize(emb, dim=1)\n",
        "                embeds.append(emb.cpu().numpy())\n",
        "\n",
        "        return np.vstack(embeds)\n",
        "\n",
        "# --------------------------\n",
        "# compute_prototypes fallback\n",
        "# --------------------------\n",
        "if 'compute_prototypes' in globals():\n",
        "    _compute_prototypes = compute_prototypes\n",
        "else:\n",
        "    def _compute_prototypes(support_emb, support_labels, N_way):\n",
        "        D = support_emb.shape[1]\n",
        "        prot = np.zeros((N_way, D), dtype=np.float32)\n",
        "        for c in range(N_way):\n",
        "            idxs = [i for i,l in enumerate(support_labels) if l==c]\n",
        "            if len(idxs) == 0:\n",
        "                continue\n",
        "            prot[c] = support_emb[idxs].mean(axis=0)\n",
        "        prot /= (np.linalg.norm(prot, axis=1, keepdims=True) + 1e-9)\n",
        "        return prot\n",
        "\n",
        "# --------------------------\n",
        "# FewShotDataset (robust): reduces N_way when insufficient classes; samples with replacement for small classes\n",
        "# --------------------------\n",
        "if 'FewShotDataset' in globals():\n",
        "    _FewShotDataset = FewShotDataset\n",
        "else:\n",
        "    class _FewShotDataset:\n",
        "        def __init__(self, items, img_size=(224,224)):\n",
        "            self.items = items\n",
        "            self.by_class = defaultdict(list)\n",
        "            for p,l in items:\n",
        "                self.by_class[l].append(p)\n",
        "            self.classes = sorted(self.by_class.keys())\n",
        "\n",
        "        def sample_episode(self, N_way=5, K_shot=5, Q_query=5):\n",
        "            avail_classes = len(self.classes)\n",
        "            if avail_classes == 0:\n",
        "                raise ValueError(\"FewShotDataset has no classes (items list empty).\")\n",
        "            if avail_classes < N_way:\n",
        "                warnings.warn(f\"Requested N_way={N_way} but only {avail_classes} classes available. Reducing N_way -> {avail_classes}.\", UserWarning)\n",
        "                N_way = avail_classes\n",
        "            chosen = random.sample(self.classes, N_way)\n",
        "            support=[]; s_lbl=[]; query=[]; q_lbl=[]\n",
        "            for i,cls in enumerate(chosen):\n",
        "                imgs = self.by_class[cls]\n",
        "                required = K_shot + Q_query\n",
        "                if len(imgs) >= required:\n",
        "                    sel = random.sample(imgs, required)\n",
        "                else:\n",
        "                    warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n",
        "                    sel = [random.choice(imgs) for _ in range(required)]\n",
        "                support += sel[:K_shot]; query += sel[K_shot:]\n",
        "                s_lbl += [i]*K_shot; q_lbl += [i]*Q_query\n",
        "            return support, s_lbl, query, q_lbl\n",
        "\n",
        "# choose dataset object\n",
        "if 'fs_val' in globals():\n",
        "    _fs_val = fs_val\n",
        "else:\n",
        "    _fs_val = _FewShotDataset(val_items, img_size=globals().get('IMG_SIZE',(224,224)))\n",
        "\n",
        "# quick stats\n",
        "try:\n",
        "    per_class_counts = {cls: len(imgs) for cls, imgs in _fs_val.by_class.items()}\n",
        "    print(\"Validation set: total images =\", len(val_items), \"; classes =\", len(per_class_counts))\n",
        "    small = sorted(per_class_counts.items(), key=lambda x: x[1])[:6]\n",
        "    print(\"Few smallest class counts (class:count):\", small)\n",
        "except Exception as e:\n",
        "    print(\"Could not print fs_val stats:\", e)\n",
        "\n",
        "# --------------------------\n",
        "# 1) Base few-shot evaluation (prototypical)\n",
        "# --------------------------\n",
        "if 'accs_5shot' not in globals() or 'accs_1shot' not in globals():\n",
        "    print(\"Running base few-shot evaluation (prototypical) ...\")\n",
        "\n",
        "    def _evaluate_episode_base(fs_dataset, N_way=5, K_shot=5, Q_query=10):\n",
        "        support, s_lbl, query, q_lbl = fs_dataset.sample_episode(N_way,K_shot,Q_query)\n",
        "        # pick backbone\n",
        "        base_backbone_name = FUSION_BACKBONES[0] if FUSION_BACKBONES[0] in pytorch_factories else list(pytorch_factories.keys())[0]\n",
        "        bb_fac = pytorch_factories[base_backbone_name]\n",
        "        bb, ph = _make_embedding_extractor(bb_fac, embed_dim=FUSION_EMBED_DIM)\n",
        "        # print inferred feat dim\n",
        "        try:\n",
        "            print(f\"Base backbone '{base_backbone_name}' inferred feat dim:\", getattr(bb, '_inferred_feat_dim', None))\n",
        "        except Exception:\n",
        "            pass\n",
        "        s_emb = _embed_batch(bb, ph, support)\n",
        "        q_emb = _embed_batch(bb, ph, query)\n",
        "        proto = _compute_prototypes(s_emb, s_lbl, N_way)\n",
        "        logits = q_emb @ proto.T\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        return (preds == np.array(q_lbl)).mean()\n",
        "\n",
        "    accs_5shot = []\n",
        "    for _ in tqdm(range(FEWSHOT_EPISODES), desc='base 5-shot'):\n",
        "        accs_5shot.append(_evaluate_episode_base(_fs_val, N_way=5, K_shot=5, Q_query=10))\n",
        "    accs_1shot = []\n",
        "    for _ in tqdm(range(FEWSHOT_EPISODES), desc='base 1-shot'):\n",
        "        accs_1shot.append(_evaluate_episode_base(_fs_val, N_way=5, K_shot=1, Q_query=10))\n",
        "    print(f\"Base 5-shot mean: {np.mean(accs_5shot)*100:.2f}%  |  Base 1-shot mean: {np.mean(accs_1shot)*100:.2f}%\")\n",
        "else:\n",
        "    print(\"Base few-shot results found; skipping base evaluation.\")\n",
        "\n",
        "# --------------------------\n",
        "# 2) Build hybrid fusion extractors (HFPN)\n",
        "# --------------------------\n",
        "fusion_extractors = {}\n",
        "for bk in FUSION_BACKBONES:\n",
        "    if bk not in pytorch_factories:\n",
        "        raise RuntimeError(f\"Requested fusion backbone '{bk}' not found in pytorch_factories.\")\n",
        "    fac = pytorch_factories[bk]\n",
        "    bb, ph = _make_embedding_extractor(fac, embed_dim=FUSION_EMBED_DIM)\n",
        "    bb.eval(); ph.eval()\n",
        "    fusion_extractors[bk] = (bb, ph)\n",
        "    print(f\"Backbone '{bk}' inferred feat dim:\", getattr(bb, '_inferred_feat_dim', None))\n",
        "\n",
        "print(\"Built hybrid-fusion extractors for:\", list(fusion_extractors.keys()))\n",
        "\n",
        "def _embed_fusion(paths):\n",
        "    parts = []\n",
        "    for bk in FUSION_BACKBONES:\n",
        "        bb, ph = fusion_extractors[bk]\n",
        "        emb = _embed_batch(bb, ph, paths)\n",
        "        parts.append(emb)\n",
        "    fused = np.concatenate(parts, axis=1)\n",
        "    fused = fused / (np.linalg.norm(fused, axis=1, keepdims=True) + 1e-9)\n",
        "    return fused\n",
        "\n",
        "# --------------------------\n",
        "# 3) Optional proj-head finetune (short)\n",
        "# --------------------------\n",
        "DO_FINETUNE_PROJ = False\n",
        "if DO_FINETUNE_PROJ:\n",
        "    print(\"Fine-tuning projection heads (small supervised head training on train_items).\")\n",
        "    import torch, torch.nn as nn, torch.optim as optim\n",
        "    from torchvision import transforms\n",
        "    tf = transforms.Compose([transforms.Resize(globals().get('IMG_SIZE',(224,224))), transforms.ToTensor(),\n",
        "                             transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "    small_train = train_items[:min(500, len(train_items))]\n",
        "    X_paths = [p for p,_ in small_train]; Y = [lbl for _,lbl in small_train]\n",
        "    class _SmallDS(torch.utils.data.Dataset):\n",
        "        def __init__(self, paths, labels): self.paths=paths; self.labels=labels\n",
        "        def __len__(self): return len(self.paths)\n",
        "        def __getitem__(self,idx):\n",
        "            im = Image.open(self.paths[idx]).convert('RGB'); return tf(im), int(self.labels[idx])\n",
        "    ds = _SmallDS(X_paths, Y)\n",
        "    dl = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "    for bk,(bb,ph) in fusion_extractors.items():\n",
        "        ph.train()\n",
        "        opt = optim.Adam(filter(lambda p:p.requires_grad, ph.parameters()), lr=1e-3, weight_decay=1e-5)\n",
        "        ce = nn.CrossEntropyLoss()\n",
        "        for epoch in range(2):\n",
        "            for imgs, lbls in dl:\n",
        "                imgs = imgs.to(DEVICE); lbls = lbls.to(DEVICE)\n",
        "                with torch.no_grad():\n",
        "                    feats = bb.forward_features(imgs) if hasattr(bb,'forward_features') else bb(imgs)\n",
        "                out = ph(feats)\n",
        "                logits = nn.Linear(out.shape[1], num_classes).to(DEVICE)(out)\n",
        "                loss = ce(logits, lbls)\n",
        "                opt.zero_grad(); loss.backward(); opt.step()\n",
        "        ph.eval()\n",
        "    print(\"Proj-head fine-tune done.\")\n",
        "\n",
        "# --------------------------\n",
        "# 4) Hybrid Fusion few-shot evaluation\n",
        "# --------------------------\n",
        "if 'accs_fusion_5shot' not in globals() or 'accs_fusion_1shot' not in globals():\n",
        "    print(\"Running Hybrid Fusion few-shot evaluation (HFPN) ...\")\n",
        "    accs_fusion_5shot = []\n",
        "    for _ in tqdm(range(FEWSHOT_EPISODES), desc='fusion 5-shot'):\n",
        "        support, s_lbl, query, q_lbl = _fs_val.sample_episode(5,5,10)\n",
        "        s_emb = _embed_fusion(support)\n",
        "        q_emb = _embed_fusion(query)\n",
        "        prot = _compute_prototypes(s_emb, s_lbl, 5)\n",
        "        logits = q_emb @ prot.T\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        accs_fusion_5shot.append((preds == np.array(q_lbl)).mean())\n",
        "\n",
        "    accs_fusion_1shot = []\n",
        "    for _ in tqdm(range(FEWSHOT_EPISODES), desc='fusion 1-shot'):\n",
        "        support, s_lbl, query, q_lbl = _fs_val.sample_episode(5,1,10)\n",
        "        s_emb = _embed_fusion(support)\n",
        "        q_emb = _embed_fusion(query)\n",
        "        prot = _compute_prototypes(s_emb, s_lbl, 5)\n",
        "        logits = q_emb @ prot.T\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        accs_fusion_1shot.append((preds == np.array(q_lbl)).mean())\n",
        "\n",
        "    print(f\"Fusion 5-shot mean: {np.mean(accs_fusion_5shot)*100:.2f}%  |  Fusion 1-shot mean: {np.mean(accs_fusion_1shot)*100:.2f}%\")\n",
        "else:\n",
        "    print(\"Fusion few-shot results found; skipping fusion evaluation.\")\n",
        "\n",
        "# --------------------------\n",
        "# 5) Plotting & Export\n",
        "# --------------------------\n",
        "accs_grouped = {}\n",
        "accs_grouped['base_5shot'] = np.array(accs_5shot)\n",
        "accs_grouped['base_1shot'] = np.array(accs_1shot)\n",
        "if 'accs_fusion_5shot' in globals():\n",
        "    accs_grouped['fusion_5shot'] = np.array(accs_fusion_5shot)\n",
        "    accs_grouped['fusion_1shot'] = np.array(accs_fusion_1shot)\n",
        "\n",
        "rows=[]\n",
        "for k,arr in accs_grouped.items():\n",
        "    rows.append({'setting':k, 'mean_acc':float(np.mean(arr)), 'std_acc':float(np.std(arr,ddof=1) if len(arr)>1 else np.std(arr)),\n",
        "                 'median':float(np.median(arr)), 'n_episodes':int(len(arr))})\n",
        "df_summary = pd.DataFrame(rows)\n",
        "csv_out = os.path.join(SAVE_OUTDIR, 'fewshot_summary_integrated.csv')\n",
        "df_summary.to_csv(csv_out, index=False)\n",
        "print(\"Saved integrated few-shot summary CSV ->\", csv_out)\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(9,5))\n",
        "if 'fusion_5shot' in accs_grouped:\n",
        "    sns.histplot(accs_grouped['base_5shot'], label='base 5-shot', stat='density', kde=True, alpha=0.5)\n",
        "    sns.histplot(accs_grouped['fusion_5shot'], label='fusion 5-shot', stat='density', kde=True, alpha=0.5)\n",
        "    sns.histplot(accs_grouped['base_1shot'], label='base 1-shot', stat='density', kde=True, alpha=0.35)\n",
        "    sns.histplot(accs_grouped['fusion_1shot'], label='fusion 1-shot', stat='density', kde=True, alpha=0.35)\n",
        "else:\n",
        "    sns.histplot(accs_grouped['base_5shot'], label='base 5-shot', stat='density', kde=True, alpha=0.6)\n",
        "    sns.histplot(accs_grouped['base_1shot'], label='base 1-shot', stat='density', kde=True, alpha=0.6)\n",
        "plt.xlabel('Episode Accuracy'); plt.title('Few-Shot Episode Accuracy Distribution (Integrated)')\n",
        "plt.legend()\n",
        "hist_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_histogram.png')\n",
        "plt.tight_layout(); plt.savefig(hist_p); plt.close()\n",
        "print(\"Saved histogram ->\", hist_p)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "if 'fusion_5shot' in accs_grouped:\n",
        "    labels=['base 5-shot','fusion 5-shot','base 1-shot','fusion 1-shot']\n",
        "    data=[accs_grouped['base_5shot'], accs_grouped['fusion_5shot'], accs_grouped['base_1shot'], accs_grouped['fusion_1shot']]\n",
        "else:\n",
        "    labels=['base 5-shot','base 1-shot']; data=[accs_grouped['base_5shot'], accs_grouped['base_1shot']]\n",
        "sns.boxplot(data=data); plt.xticks(range(len(labels)), labels, rotation=15)\n",
        "plt.ylabel('Episode Accuracy'); plt.title('Few-Shot Accuracy Boxplot (Integrated)')\n",
        "box_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_boxplot.png')\n",
        "plt.tight_layout(); plt.savefig(box_p); plt.close()\n",
        "print(\"Saved boxplot ->\", box_p)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "if 'fusion_5shot' in accs_grouped:\n",
        "    bars=[accs_grouped['base_5shot'].mean()*100, accs_grouped['fusion_5shot'].mean()*100,\n",
        "          accs_grouped['base_1shot'].mean()*100, accs_grouped['fusion_1shot'].mean()*100]\n",
        "    errs=[accs_grouped['base_5shot'].std(ddof=1)*100 if len(accs_grouped['base_5shot'])>1 else 0,\n",
        "          accs_grouped['fusion_5shot'].std(ddof=1)*100 if len(accs_grouped['fusion_5shot'])>1 else 0,\n",
        "          accs_grouped['base_1shot'].std(ddof=1)*100 if len(accs_grouped['base_1shot'])>1 else 0,\n",
        "          accs_grouped['fusion_1shot'].std(ddof=1)*100 if len(accs_grouped['fusion_1shot'])>1 else 0]\n",
        "    bl=['base 5-shot','fusion 5-shot','base 1-shot','fusion 1-shot']\n",
        "else:\n",
        "    bars=[accs_grouped['base_5shot'].mean()*100, accs_grouped['base_1shot'].mean()*100]\n",
        "    errs=[accs_grouped['base_5shot'].std(ddof=1)*100 if len(accs_grouped['base_5shot'])>1 else 0,\n",
        "          accs_grouped['base_1shot'].std(ddof=1)*100 if len(accs_grouped['base_1shot'])>1 else 0]\n",
        "    bl=['base 5-shot','base 1-shot']\n",
        "x=np.arange(len(bars)); plt.bar(x,bars,yerr=errs,capsize=5); plt.xticks(x,bl,rotation=12)\n",
        "plt.ylabel('Mean Accuracy (%)'); plt.title('Few-Shot Mean Accuracy \u00b1 Std (Integrated)')\n",
        "for i,v in enumerate(bars): plt.text(i, v + (errs[i]+0.5), f\"{v:.2f}% \u00b1 {errs[i]:.2f}%\", ha='center', fontsize=9)\n",
        "bar_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_meanstdbar.png')\n",
        "plt.tight_layout(); plt.savefig(bar_p); plt.close()\n",
        "print(\"Saved mean\u00b1std bar ->\", bar_p)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "if 'fusion_5shot' in accs_grouped:\n",
        "    plt.plot(np.sort(accs_grouped['base_5shot'])[::-1], label='base 5-shot', alpha=0.8)\n",
        "    plt.plot(np.sort(accs_grouped['fusion_5shot'])[::-1], label='fusion 5-shot', alpha=0.8)\n",
        "    plt.plot(np.sort(accs_grouped['base_1shot'])[::-1], label='base 1-shot', alpha=0.6)\n",
        "    plt.plot(np.sort(accs_grouped['fusion_1shot'])[::-1], label='fusion 1-shot', alpha=0.6)\n",
        "else:\n",
        "    plt.plot(np.sort(accs_grouped['base_5shot'])[::-1], label='base 5-shot', alpha=0.8)\n",
        "    plt.plot(np.sort(accs_grouped['base_1shot'])[::-1], label='base 1-shot', alpha=0.8)\n",
        "plt.xlabel('Episode index (sorted)'); plt.ylabel('Accuracy'); plt.legend(); plt.title('Sorted Episode Accuracies (Integrated)')\n",
        "sorted_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_sortedepisodes.png')\n",
        "plt.tight_layout(); plt.savefig(sorted_p); plt.close()\n",
        "print(\"Saved sorted-episodes plot ->\", sorted_p)\n",
        "\n",
        "print(\"\\nIntegrated few-shot summary (printed):\")\n",
        "print(df_summary.to_string(index=False))\n",
        "\n",
        "combined_csv = os.path.join(SAVE_OUTDIR, 'combined_summary.csv')\n",
        "if os.path.exists(combined_csv):\n",
        "    try:\n",
        "        comb = pd.read_csv(combined_csv)\n",
        "        to_add = []\n",
        "        to_add.append({'model':'fewshot::base_proto','accuracy':float(df_summary.loc[df_summary['setting']=='base_5shot','mean_acc'].values[0]),\n",
        "                       'macro_precision':np.nan,'macro_recall':np.nan,'macro_f1':np.nan})\n",
        "        if 'fusion_5shot' in accs_grouped:\n",
        "            to_add.append({'model':'fewshot::hybrid_fusion','accuracy':float(df_summary.loc[df_summary['setting']=='fusion_5shot','mean_acc'].values[0]),\n",
        "                           'macro_precision':np.nan,'macro_recall':np.nan,'macro_f1':np.nan})\n",
        "        comb = pd.concat([comb, pd.DataFrame(to_add)], ignore_index=True)\n",
        "        comb.to_csv(os.path.join(SAVE_OUTDIR,'combined_summary_with_fewshot_integrated.csv'), index=False)\n",
        "        print(\"Appended few-shot rows to combined_summary -> combined_summary_with_fewshot_integrated.csv\")\n",
        "    except Exception as e:\n",
        "        print(\"Could not append to combined_summary:\", e)\n",
        "\n",
        "print(\"\\nAll integrated few-shot artifacts saved in:\", PLOTS_DIR)\n",
        "# ============================================================\n",
        "# END CELL\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jI8df2C7v5W",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763837296578,
          "user_tz": -330,
          "elapsed": 1089739,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "outputId": "256ee06d-2234-4018-a288-651676c6fd28",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Combined comparison: run both Keras and PyTorch models on the same test set and save combined metrics & plots\n",
        "# -------------------------\n",
        "# Build test_paths and y_test (we already have test_items)\n",
        "test_paths = [p for p,_ in test_items]\n",
        "y_test = np.array([lbl for _,lbl in test_items])\n",
        "print(\"Test samples:\", len(test_paths))"
      ],
      "metadata": {
        "id": "oLcIytD9m7R0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763837373802,
          "user_tz": -330,
          "elapsed": 57,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e72519-cc12-413e-a04c-1670aafceb84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras predictions (use trained_classifiers dict)\n",
        "def keras_predict_on_paths(keras_model, paths, batch=32, target_size=IMG_SIZE):\n",
        "    preds_list=[]; probs_list=[]\n",
        "    for i in range(0,len(paths),batch):\n",
        "        batch_paths = paths[i:i+batch]\n",
        "        imgs=[]\n",
        "        for p in batch_paths:\n",
        "            im = Image.open(p).resize(target_size)\n",
        "            a = np.array(im).astype('float32')/255.0\n",
        "            imgs.append(a)\n",
        "        xb = np.stack(imgs, axis=0)\n",
        "        probs = keras_model.predict(xb, verbose=0)\n",
        "        preds_list.append(np.argmax(probs, axis=1))\n",
        "        probs_list.append(probs)\n",
        "    return np.concatenate(probs_list, axis=0), np.concatenate(preds_list, axis=0)\n",
        "\n",
        "keras_preds = {}\n",
        "for name, model in trained_classifiers.items():\n",
        "    print(\"Predict (Keras):\", name)\n",
        "    probs, preds = keras_predict_on_paths(model, test_paths, batch=32, target_size=IMG_SIZE)\n",
        "    keras_preds[name] = {'probs': probs, 'preds': preds}\n"
      ],
      "metadata": {
        "id": "8pWw7KpBnID8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763837453898,
          "user_tz": -330,
          "elapsed": 24492,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa03e17-1e15-4823-9bf2-d43169140884"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch predictions (load best ckpts)\n",
        "val_tf_pt = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "def pytorch_predict_ckpt(factory, ckpt, paths, batch=32):\n",
        "    model = factory(num_classes).to(DEVICE)\n",
        "    st = torch.load(ckpt, map_location=DEVICE)\n",
        "    try:\n",
        "        model.load_state_dict(st)\n",
        "    except Exception:\n",
        "        if isinstance(st, dict) and 'state_dict' in st:\n",
        "            model.load_state_dict(st['state_dict'])\n",
        "        else:\n",
        "            model.load_state_dict(st, strict=False)\n",
        "    model.eval()\n",
        "    all_probs=[]; all_preds=[]\n",
        "    for i in range(0,len(paths),batch):\n",
        "        batch_paths = paths[i:i+batch]\n",
        "        imgs=[]\n",
        "        for p in batch_paths:\n",
        "            im = Image.open(p).convert('RGB')\n",
        "            t = val_tf_pt(im)\n",
        "            imgs.append(t)\n",
        "        xb = torch.stack(imgs).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            logits = model(xb)\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "            preds = probs.argmax(axis=1)\n",
        "        all_probs.append(probs); all_preds.append(preds)\n",
        "    return np.vstack(all_probs), np.concatenate(all_preds)\n",
        "\n",
        "pytorch_preds = {}\n",
        "for name, fac in pytorch_factories.items():\n",
        "    ckpt_path = os.path.join(OUT_ROOT, f'{name}_best.pth')\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(\"PyTorch checkpoint missing for\", name, \"-> skipping predict\")\n",
        "        continue\n",
        "    print(\"Predict (PyTorch):\", name)\n",
        "    probs, preds = pytorch_predict_ckpt(fac, ckpt_path, test_paths, batch=32)\n",
        "    pytorch_preds[name] = {'probs': probs, 'preds': preds}\n"
      ],
      "metadata": {
        "id": "ekHqL0JsnP5v",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763837472765,
          "user_tz": -330,
          "elapsed": 18834,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c02422f-e431-4a86-b054-41cb1d464250"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all preds\n",
        "all_preds = {}\n",
        "all_preds.update({f\"keras::{k}\": v for k,v in keras_preds.items()})\n",
        "all_preds.update({f\"torch::{k}\": v for k,v in pytorch_preds.items()})\n",
        "if len(all_preds)==0:\n",
        "    raise RuntimeError(\"No models produced predictions. Check earlier steps.\")\n"
      ],
      "metadata": {
        "id": "13dkwKyAnTKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics and save\n",
        "rows=[]\n",
        "for key, v in all_preds.items():\n",
        "    y_pred = v['preds']\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    mp = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    mr = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    mf1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    rows.append({'model': key, 'accuracy': acc, 'macro_precision': mp, 'macro_recall': mr, 'macro_f1': mf1})\n",
        "summary_df = pd.DataFrame(rows).sort_values('macro_f1', ascending=False).reset_index(drop=True)\n",
        "summary_df.to_csv(os.path.join(OUT_ROOT,'combined_summary.csv'), index=False)\n",
        "print(\"Saved combined_summary.csv\")\n",
        "\n",
        "# Save per-model confusion and classification reports & plots\n",
        "for key, v in all_preds.items():\n",
        "    preds = v['preds']\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    rep = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
        "    pd.DataFrame(rep).transpose().to_csv(os.path.join(OUT_ROOT, f\"{key.replace('::','_')}_classification_report.csv\"))\n",
        "    np.save(os.path.join(OUT_ROOT, f\"{key.replace('::','_')}_cm.npy\"), cm)\n",
        "    # plot\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=full_dataset.classes, yticklabels=full_dataset.classes, cmap='Blues')\n",
        "    plt.title(f'Confusion: {key}'); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(PLOTS_DIR, f\"{key.replace('::','_')}_confusion.png\")); plt.close()\n",
        "\n",
        "# Plot overall ranking bars\n",
        "plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n",
        "sns.barplot(data=summary_df, x='model', y='accuracy'); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.savefig(os.path.join(PLOTS_DIR,'combined_accuracy.png')); plt.close()\n",
        "plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n",
        "sns.barplot(data=summary_df, x='model', y='macro_f1'); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.savefig(os.path.join(PLOTS_DIR,'combined_macro_f1.png')); plt.close()\n",
        "print(\"Saved combined plots to\", PLOTS_DIR)\n"
      ],
      "metadata": {
        "id": "bI6XB2PqnZDP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763837513771,
          "user_tz": -330,
          "elapsed": 22180,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e33d1e2-b03f-4757-d3ab-4955ee90091d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Add this cell: Summary report + bar plots for all models\n",
        "# Paste & run after training + combined evaluation cells\n",
        "# ---------------------------\n",
        "import time, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "OUT_CSV = os.path.join(OUT_ROOT, 'models_comparison_summary.csv')\n",
        "PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(OUT_ROOT, 'plots'))\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Helpers to (re)compute predictions if needed ----\n",
        "need_recompute = 'all_preds' not in globals() or not isinstance(all_preds, dict) or len(all_preds)==0\n",
        "\n",
        "# Use existing all_preds if present\n",
        "preds_map = {}\n",
        "if not need_recompute:\n",
        "    preds_map = dict(all_preds)  # keys like 'keras::Model', 'torch::name'\n",
        "else:\n",
        "    # fallback: try to compute using helpers if defined\n",
        "    test_paths_local = globals().get('test_paths', [p for p,_ in test_items])\n",
        "    y_test_local = np.array([lbl for _,lbl in test_items])\n",
        "    print(\"Recomputing predictions (this may take a short while)...\")\n",
        "    # Keras\n",
        "    if 'trained_classifiers' in globals() and isinstance(trained_classifiers, dict):\n",
        "        for name, km in trained_classifiers.items():\n",
        "            try:\n",
        "                # reuse keras_predict_on_paths if available\n",
        "                if 'keras_predict_on_paths' in globals():\n",
        "                    probs, preds = keras_predict_on_paths(km, test_paths_local, batch=32, target_size=IMG_SIZE)\n",
        "                else:\n",
        "                    # simple fallback\n",
        "                    probs_list=[]; preds_list=[]\n",
        "                    for i in range(0,len(test_paths_local),32):\n",
        "                        batch = test_paths_local[i:i+32]; imgs=[]\n",
        "                        for p in batch:\n",
        "                            im = Image.open(p).resize(IMG_SIZE)\n",
        "                            a = np.array(im).astype('float32')/255.0; imgs.append(a)\n",
        "                        xb = np.stack(imgs,0)\n",
        "                        ps = km.predict(xb, verbose=0)\n",
        "                        probs_list.append(ps); preds_list.append(np.argmax(ps,axis=1))\n",
        "                    probs = np.vstack(probs_list); preds = np.concatenate(preds_list)\n",
        "                preds_map[f\"keras::{name}\"] = {'probs':probs, 'preds':preds}\n",
        "                print(\" - Keras preds:\", name)\n",
        "            except Exception as e:\n",
        "                print(\"  Failed Keras predict for\", name, e)\n",
        "    # PyTorch\n",
        "    if 'pytorch_factories' in globals():\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        val_tf_pt = globals().get('val_tf_pt', None)\n",
        "        if val_tf_pt is None:\n",
        "            from torchvision import transforms\n",
        "            val_tf_pt = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "        for name, fac in pytorch_factories.items():\n",
        "            ckpt_path = os.path.join(OUT_ROOT, f'{name}_best.pth')\n",
        "            if not os.path.exists(ckpt_path):\n",
        "                print(\" - skip torch:\", name, \" (checkpoint missing )\")\n",
        "                continue\n",
        "            try:\n",
        "                # reuse pytorch_predict_ckpt if present\n",
        "                if 'pytorch_predict_ckpt' in globals():\n",
        "                    probs, preds = pytorch_predict_ckpt(fac, ckpt_path, test_paths_local, batch=32)\n",
        "                else:\n",
        "                    # minimal local predict:\n",
        "                    model = fac(num_classes).to(device)\n",
        "                    st = torch.load(ckpt_path, map_location=device)\n",
        "                    try: model.load_state_dict(st)\n",
        "                    except:\n",
        "                        if isinstance(st, dict) and 'state_dict' in st: model.load_state_dict(st['state_dict'])\n",
        "                        else: model.load_state_dict(st, strict=False)\n",
        "                    model.eval()\n",
        "                    all_probs=[]; all_preds=[]\n",
        "                    for i in range(0,len(test_paths_local),32):\n",
        "                        batch = test_paths_local[i:i+32]; imgs=[]\n",
        "                        for p in batch:\n",
        "                            im = Image.open(p).convert('RGB'); t = val_tf_pt(im); imgs.append(t)\n",
        "                        xb = torch.stack(imgs).to(device)\n",
        "                        with torch.no_grad():\n",
        "                            logits = model(xb); probs_b = torch.softmax(logits, dim=1).cpu().numpy(); preds_b = probs_b.argmax(axis=1)\n",
        "                        all_probs.append(probs_b); all_preds.append(preds_b)\n",
        "                    probs = np.vstack(all_probs); preds = np.concatenate(all_preds)\n",
        "                preds_map[f\"torch::{name}\"] = {'probs':probs, 'preds':preds}\n",
        "                print(\" - Torch preds:\", name)\n",
        "            except Exception as e:\n",
        "                print(\"  Failed Torch predict for\", name, e)\n",
        "\n",
        "# ---- Build summary metrics (accuracy, macro_f1) ----\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "summary_rows = []\n",
        "y_true = np.array([lbl for _,lbl in test_items])\n",
        "\n",
        "for model_key, d in preds_map.items():\n",
        "    preds = np.array(d['preds'])\n",
        "    acc = float(accuracy_score(y_true, preds))\n",
        "    macro_f1 = float(f1_score(y_true, preds, average='macro', zero_division=0))\n",
        "    macro_prec = float(precision_score(y_true, preds, average='macro', zero_division=0))\n",
        "    macro_rec = float(recall_score(y_true, preds, average='macro', zero_division=0))\n",
        "    summary_rows.append({'model': model_key, 'accuracy': acc, 'macro_precision': macro_prec, 'macro_recall': macro_rec, 'macro_f1': macro_f1})\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows).sort_values('macro_f1', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# ---- Parameter counts ----\n",
        "param_rows = []\n",
        "# Keras params\n",
        "if 'trained_classifiers' in globals():\n",
        "    for name, km in trained_classifiers.items():\n",
        "        try:\n",
        "            params = km.count_params()\n",
        "        except Exception:\n",
        "            # fallback count trainable & non-trainable\n",
        "            params = sum([np.prod(w.shape) for w in km.weights])\n",
        "        param_rows.append({'model': f'keras::{name}', 'params': int(params)})\n",
        "# PyTorch params\n",
        "if 'pytorch_factories' in globals():\n",
        "    for name, fac in pytorch_factories.items():\n",
        "        try:\n",
        "            m = fac(num_classes)\n",
        "            pcount = sum([p.numel() for p in m.parameters()])\n",
        "            param_rows.append({'model': f'torch::{name}', 'params': int(pcount)})\n",
        "        except Exception as e:\n",
        "            print(\"Failed param count for\", name, e)\n",
        "\n",
        "df_params = pd.DataFrame(param_rows)\n",
        "\n",
        "# merge param info into summary\n",
        "summary_df = summary_df.merge(df_params, how='left', left_on='model', right_on='model')\n",
        "summary_df['params_M'] = (summary_df['params'] / 1e6).round(3)\n",
        "\n",
        "# ---- Inference timing (approx avg ms per image) ----\n",
        "def measure_inference_time_keras(km, paths, n_samples=50, batch=8):\n",
        "    # sample n_samples images\n",
        "    sel = paths[:n_samples]\n",
        "    # warmup\n",
        "    _ = None\n",
        "    t0 = time.time()\n",
        "    for i in range(0,len(sel), batch):\n",
        "        batch_p = sel[i:i+batch]; imgs=[]\n",
        "        for p in batch_p:\n",
        "            im = Image.open(p).resize(IMG_SIZE); a = np.array(im).astype('float32')/255.0; imgs.append(a)\n",
        "        xb = np.stack(imgs,0)\n",
        "        _ = km.predict(xb, verbose=0)\n",
        "    t1 = time.time()\n",
        "    total = t1 - t0\n",
        "    avg_ms = (total / len(sel)) * 1000.0\n",
        "    return avg_ms\n",
        "\n",
        "def measure_inference_time_torch(factory, ckpt, paths, n_samples=50, batch=8):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = factory(num_classes).to(device)\n",
        "    st = torch.load(ckpt, map_location=device)\n",
        "    try: model.load_state_dict(st)\n",
        "    except:\n",
        "        if isinstance(st, dict) and 'state_dict' in st: model.load_state_dict(st['state_dict'])\n",
        "        else: model.load_state_dict(st, strict=False)\n",
        "    model.eval()\n",
        "    val_tf = globals().get('val_tf_pt', None)\n",
        "    if val_tf is None:\n",
        "        from torchvision import transforms\n",
        "        val_tf = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "    sel = paths[:n_samples]\n",
        "    # warmup + measure\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0,len(sel), batch):\n",
        "            batch_p = sel[i:i+batch]; imgs=[]\n",
        "            for p in batch_p:\n",
        "                im = Image.open(p).convert('RGB'); t = val_tf(im); imgs.append(t)\n",
        "            xb = torch.stack(imgs).to(device)\n",
        "            _ = model(xb)\n",
        "    t1 = time.time()\n",
        "    total = t1 - t0\n",
        "    avg_ms = (total / len(sel)) * 1000.0\n",
        "    return avg_ms\n",
        "\n",
        "# Only measure for models where we can (limit to top N to save time)\n",
        "measure_limit = 6   # number of models to time (set smaller if compute-limited)\n",
        "timing_rows = []\n",
        "model_list_for_timing = summary_df['model'].tolist()[:measure_limit]\n",
        "print(\"Measuring inference time for (up to) {} models: {}\".format(len(model_list_for_timing), model_list_for_timing))\n",
        "for mk in model_list_for_timing:\n",
        "    try:\n",
        "        if mk.startswith('keras::') and 'trained_classifiers' in globals():\n",
        "            kn = mk.split('::',1)[1]\n",
        "            km = trained_classifiers.get(kn)\n",
        "            if km is None:\n",
        "                timing_rows.append({'model':mk, 'inf_ms':np.nan}); continue\n",
        "            avg_ms = measure_inference_time_keras(km, [p for p,_ in test_items], n_samples=50, batch=8)\n",
        "            timing_rows.append({'model':mk, 'inf_ms': round(avg_ms,2)})\n",
        "        elif mk.startswith('torch::') and 'pytorch_factories' in globals():\n",
        "            tn = mk.split('::',1)[1]\n",
        "            fac = pytorch_factories.get(tn)\n",
        "            ckpt = os.path.join(OUT_ROOT, f'{tn}_best.pth')\n",
        "            if fac is None or not os.path.exists(ckpt):\n",
        "                timing_rows.append({'model':mk, 'inf_ms':np.nan}); continue\n",
        "            avg_ms = measure_inference_time_torch(fac, ckpt, [p for p,_ in test_items], n_samples=50, batch=8)\n",
        "            timing_rows.append({'model':mk, 'inf_ms': round(avg_ms,2)})\n",
        "        else:\n",
        "            timing_rows.append({'model':mk, 'inf_ms':np.nan})\n",
        "    except Exception as e:\n",
        "        print(\"Timing failed for\", mk, e)\n",
        "        timing_rows.append({'model':mk, 'inf_ms':np.nan})\n",
        "\n",
        "df_time = pd.DataFrame(timing_rows)\n",
        "summary_df = summary_df.merge(df_time, on='model', how='left')\n",
        "\n",
        "# ---- Save CSV and show table ----\n",
        "summary_df = summary_df[['model','accuracy','macro_precision','macro_recall','macro_f1','params','params_M','inf_ms']]\n",
        "summary_df.to_csv(OUT_CSV, index=False)\n",
        "print(\"Saved summary CSV:\", OUT_CSV)\n",
        "display(summary_df.sort_values('macro_f1', ascending=False).reset_index(drop=True))\n",
        "\n",
        "# ---- Plots: Accuracy, Macro-F1, Params (log) ----\n",
        "plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n",
        "sns.barplot(data=summary_df.sort_values('accuracy', ascending=False), x='model', y='accuracy')\n",
        "plt.xticks(rotation=45, ha='right'); plt.title('Model test accuracy'); plt.tight_layout()\n",
        "plt.savefig(os.path.join(PLOTS_DIR, 'summary_accuracy_bar.png')); plt.close()\n",
        "\n",
        "plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n",
        "sns.barplot(data=summary_df.sort_values('macro_f1', ascending=False), x='model', y='macro_f1')\n",
        "plt.xticks(rotation=45, ha='right'); plt.title('Model macro F1'); plt.tight_layout()\n",
        "plt.savefig(os.path.join(PLOTS_DIR, 'summary_macrof1_bar.png')); plt.close()\n",
        "\n",
        "# Params plot (log scale)\n",
        "plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n",
        "sns.barplot(data=summary_df.sort_values('params_M', ascending=False), x='model', y='params_M')\n",
        "plt.yscale('log')\n",
        "plt.xticks(rotation=45, ha='right'); plt.title('Model Params (millions, log scale)'); plt.tight_layout()\n",
        "plt.savefig(os.path.join(PLOTS_DIR, 'summary_params_bar.png')); plt.close()\n",
        "\n",
        "# Inference time plot (if available)\n",
        "if 'inf_ms' in summary_df.columns and summary_df['inf_ms'].notna().any():\n",
        "    plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n",
        "    sns.barplot(data=summary_df.sort_values('inf_ms', ascending=True), x='model', y='inf_ms')\n",
        "    plt.xticks(rotation=45, ha='right'); plt.ylabel('Avg inference (ms/image)'); plt.title('Inference time (approx)'); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(PLOTS_DIR,'summary_inference_time.png')); plt.close()\n",
        "\n",
        "print(\"Saved plots to\", PLOTS_DIR)\n"
      ],
      "metadata": {
        "id": "znwYxt8Y4TM-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763837603471,
          "user_tz": -330,
          "elapsed": 50676,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "a81de7c2-8ba1-41aa-9790-6431966b8fe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pairwise McNemar tests\n",
        "names = list(all_preds.keys()); n=len(names)\n",
        "pvals = np.ones((n,n)); stats = np.zeros((n,n))\n",
        "for i in range(n):\n",
        "    for j in range(i+1, n):\n",
        "        a = all_preds[names[i]]['preds']; b = all_preds[names[j]]['preds']\n",
        "        a_corr = (a==y_test); b_corr = (b==y_test)\n",
        "        n01 = int(np.logical_and(a_corr==True, b_corr==False).sum())\n",
        "        n10 = int(np.logical_and(a_corr==False, b_corr==True).sum())\n",
        "        table = [[int(np.logical_and(a_corr==True,b_corr==True).sum()), n01],[n10, int(np.logical_and(a_corr==False,b_corr==False).sum())]]\n",
        "        res = mcnemar(table, exact=False)\n",
        "        pvals[i,j] = pvals[j,i] = float(res.pvalue)\n",
        "        stats[i,j] = stats[j,i] = float(res.statistic)\n",
        "pd.DataFrame(pvals, index=names, columns=names).to_csv(os.path.join(OUT_ROOT,'mcnemar_pvalues.csv'))\n",
        "pd.DataFrame(stats, index=names, columns=names).to_csv(os.path.join(OUT_ROOT,'mcnemar_stats.csv'))\n",
        "print(\"Saved McNemar p-values and stats to\", OUT_ROOT)\n"
      ],
      "metadata": {
        "id": "gVmko4tHndD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Robust Grad-CAM auto-run for top PyTorch models\n",
        "# Paste & run this cell (replaces previous Grad-CAM cell)\n",
        "# -------------------------\n",
        "import os, math, traceback\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# ---------- Robust Grad-CAM function (supports conv outputs and square-token ViT-like outputs) ----------\n",
        "def save_gradcam_grid_pytorch(factory, ckpt, layer_name, paths, save_path, max_images=12, img_size=(224,224), device='cuda'):\n",
        "    \"\"\"\n",
        "    Compute Grad-CAM overlays for up to max_images and save as a grid.\n",
        "    factory: callable(num_classes)->model\n",
        "    ckpt: path to checkpoint .pth\n",
        "    layer_name: exact module name from model.named_modules() to hook\n",
        "    paths: list of image file paths\n",
        "    \"\"\"\n",
        "    # load model and weights\n",
        "    model = factory(num_classes).to(device)\n",
        "    st = torch.load(ckpt, map_location=device)\n",
        "    # handle common wrappers\n",
        "    if isinstance(st, dict) and 'state_dict' in st:\n",
        "        st = st['state_dict']\n",
        "    try:\n",
        "        model.load_state_dict(st)\n",
        "    except Exception:\n",
        "        # try non-strict\n",
        "        try:\n",
        "            model.load_state_dict(st, strict=False)\n",
        "        except Exception:\n",
        "            print(\"Failed to load checkpoint strictly or loosely for\", ckpt)\n",
        "    model.eval()\n",
        "\n",
        "    modules = dict(model.named_modules())\n",
        "    if layer_name not in modules:\n",
        "        print(f\"[GradCAM] Layer '{layer_name}' not found in model modules. Available tail modules: {list(modules.keys())[-40:]}\")\n",
        "        return\n",
        "\n",
        "    target = modules[layer_name]\n",
        "\n",
        "    # holders for hooks\n",
        "    features_holder = {'feat': None}\n",
        "    grads_holder = {'grad': None}\n",
        "\n",
        "    def forward_hook(module, inp, out):\n",
        "        # store first tensor output if tuple/list\n",
        "        if isinstance(out, torch.Tensor):\n",
        "            features_holder['feat'] = out.detach()\n",
        "        elif isinstance(out, (list, tuple)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n",
        "            features_holder['feat'] = out[0].detach()\n",
        "        else:\n",
        "            features_holder['feat'] = None\n",
        "\n",
        "    # use full backward hook when available to avoid partial-grad warning\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        if isinstance(grad_output, torch.Tensor):\n",
        "            grads_holder['grad'] = grad_output.detach()\n",
        "        elif isinstance(grad_output, (list, tuple)) and len(grad_output)>0 and isinstance(grad_output[0], torch.Tensor):\n",
        "            grads_holder['grad'] = grad_output[0].detach()\n",
        "        else:\n",
        "            grads_holder['grad'] = None\n",
        "\n",
        "    # register hooks\n",
        "    try:\n",
        "        fh = target.register_forward_hook(forward_hook)\n",
        "        # prefer full backward hook API\n",
        "        bh = target.register_full_backward_hook(backward_hook)\n",
        "    except Exception:\n",
        "        fh = target.register_forward_hook(forward_hook)\n",
        "        bh = target.register_backward_hook(backward_hook)\n",
        "\n",
        "    overlays = []\n",
        "    tf_img = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "\n",
        "    def compute_cam(feat, grad, target_hw):\n",
        "        \"\"\"\n",
        "        feat, grad: tensors without batch dim (C,H,W) or (C,L) or (C,)\n",
        "        returns: 2D numpy array resized to target_hw or None\n",
        "        \"\"\"\n",
        "        if feat is None or grad is None:\n",
        "            return None\n",
        "\n",
        "        # remove batch if present\n",
        "        if feat.ndim == 4 and feat.shape[0] == 1: feat = feat[0]\n",
        "        if grad.ndim == 4 and grad.shape[0] == 1: grad = grad[0]\n",
        "\n",
        "        # case A: spatial (C,H,W)\n",
        "        if feat.ndim == 3 and grad.ndim == 3:\n",
        "            # channel weights: global avg pool of grads\n",
        "            w = grad.mean(dim=(1,2), keepdim=True)    # (C,1,1)\n",
        "            cam = (w * feat).sum(dim=0).cpu().numpy() # (H,W)\n",
        "        # case B: tokens/sequence (C,L) -> try to reshape to square\n",
        "        elif feat.ndim == 2 and grad.ndim == 2:\n",
        "            L = feat.shape[1]\n",
        "            s = int(np.round(np.sqrt(L)))\n",
        "            if s*s != L:\n",
        "                return None\n",
        "            # token importance\n",
        "            w = grad.mean(dim=1)                        # (C,)\n",
        "            token_scores = (w.unsqueeze(1) * feat).sum(dim=0).cpu().numpy()  # (L,)\n",
        "            cam = token_scores.reshape(s, s)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        cam = np.maximum(cam, 0)\n",
        "        if cam.max() <= 1e-9:\n",
        "            return None\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-9)\n",
        "        cam_resized = cv2.resize(cam, (target_hw[1], target_hw[0]))  # cv2(width,height)\n",
        "        return cam_resized\n",
        "\n",
        "    count = 0\n",
        "    for p in paths:\n",
        "        if count >= max_images: break\n",
        "        try:\n",
        "            pil = Image.open(p).convert('RGB').resize(img_size)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        x = tf_img(pil).unsqueeze(0).to(device)\n",
        "        x.requires_grad_(True)\n",
        "        features_holder['feat'] = None; grads_holder['grad'] = None\n",
        "\n",
        "        out = model(x)\n",
        "        # predicted class score\n",
        "        pred = int(out.argmax(dim=1).item())\n",
        "        score = out[0, pred]\n",
        "        model.zero_grad()\n",
        "        try:\n",
        "            score.backward(retain_graph=True)\n",
        "        except Exception:\n",
        "            # sometimes backward fails for certain models; try without retain\n",
        "            score.backward()\n",
        "\n",
        "        feat = features_holder.get('feat', None)\n",
        "        grad = grads_holder.get('grad', None)\n",
        "\n",
        "        # remove batch dim if present\n",
        "        if isinstance(feat, torch.Tensor) and feat.ndim == 4 and feat.shape[0] == 1:\n",
        "            feat_proc = feat[0]\n",
        "        else:\n",
        "            feat_proc = feat\n",
        "        if isinstance(grad, torch.Tensor) and grad.ndim == 4 and grad.shape[0] == 1:\n",
        "            grad_proc = grad[0]\n",
        "        else:\n",
        "            grad_proc = grad\n",
        "\n",
        "        cam = compute_cam(feat_proc, grad_proc, target_hw=img_size)\n",
        "        if cam is None:\n",
        "            # skip and continue\n",
        "            print(f\"[GradCAM] skipping image {p}: unsupported feat/grad shapes -> feat {None if feat is None else tuple(feat.shape)}, grad {None if grad is None else tuple(grad.shape)}\")\n",
        "            continue\n",
        "\n",
        "        # colorize and overlay\n",
        "        heat = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "        heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0\n",
        "        base = np.array(pil).astype(np.float32)/255.0\n",
        "        overlay = np.clip(0.5 * base + 0.5 * heat, 0, 1)\n",
        "        overlays.append((overlay*255).astype(np.uint8))\n",
        "        count += 1\n",
        "\n",
        "    # remove hooks\n",
        "    try:\n",
        "        fh.remove(); bh.remove()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if len(overlays) == 0:\n",
        "        print(\"[GradCAM] No overlays created for layer\", layer_name)\n",
        "        return\n",
        "\n",
        "    cols = 4; rows = math.ceil(len(overlays)/cols)\n",
        "    grid = Image.new('RGB', (cols*img_size[0], rows*img_size[1]))\n",
        "    for i, arr in enumerate(overlays):\n",
        "        r = i // cols; c = i % cols\n",
        "        grid.paste(Image.fromarray(arr), (c*img_size[0], r*img_size[1]))\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    grid.save(save_path)\n",
        "    print(\"[GradCAM] Saved grid to\", save_path)\n",
        "\n",
        "\n",
        "# ---------- Helper to automatically find a spatial module by testing forward pass ----------\n",
        "def find_spatial_module_name(factory, sample_image_path, num_classes_local=num_classes, device='cuda', img_size=(224,224)):\n",
        "    \"\"\"\n",
        "    Tries modules one-by-one: registers a forward hook, runs single forward and checks the feature shape.\n",
        "    Returns the first module name whose forward output is a 4D tensor with H>1 and W>1.\n",
        "    \"\"\"\n",
        "    model = factory(num_classes_local).to(device)\n",
        "    model.eval()\n",
        "    modules = list(model.named_modules())\n",
        "\n",
        "    tf_img = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "    pil = Image.open(sample_image_path).convert('RGB').resize(img_size)\n",
        "    x = tf_img(pil).unsqueeze(0).to(device)\n",
        "\n",
        "    found = None\n",
        "    for name, module in modules:\n",
        "        # skip trivial root module\n",
        "        if name == '':\n",
        "            continue\n",
        "        feat_holder = {'out': None}\n",
        "        def fh(m, inp, out):\n",
        "            if isinstance(out, torch.Tensor):\n",
        "                feat_holder['out'] = out\n",
        "            elif isinstance(out, (list,tuple)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n",
        "                feat_holder['out'] = out[0]\n",
        "            else:\n",
        "                feat_holder['out'] = None\n",
        "        h = module.register_forward_hook(fh)\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                _ = model(x)\n",
        "            out_t = feat_holder['out']\n",
        "            if isinstance(out_t, torch.Tensor):\n",
        "                # remove batch if present\n",
        "                t = out_t\n",
        "                if t.ndim == 4 and t.shape[0] == 1:\n",
        "                    t = t[0]\n",
        "                if t.ndim == 3:\n",
        "                    # (C,H,W) -> spatial found\n",
        "                    _, H, W = t.shape\n",
        "                    if H > 1 and W > 1:\n",
        "                        found = name\n",
        "                        h.remove()\n",
        "                        break\n",
        "        except Exception:\n",
        "            # if forward failed for this module, continue\n",
        "            pass\n",
        "        h.remove()\n",
        "    # clean up\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    return found\n",
        "\n",
        "# ---------- Run automatic Grad-CAM for top torch models ----------\n",
        "top3 = summary_df['model'].tolist()[:3]\n",
        "for m in top3:\n",
        "    if not m.startswith('torch::'):\n",
        "        continue\n",
        "    mn = m.split('::',1)[1]\n",
        "    if mn not in pytorch_factories:\n",
        "        print(\"[GradCAM] no factory for\", mn); continue\n",
        "    fac = pytorch_factories[mn]\n",
        "    ckpt = os.path.join(OUT_ROOT, f'{mn}_best.pth')\n",
        "    if not os.path.exists(ckpt):\n",
        "        print(\"[GradCAM] checkpoint missing for\", mn, ckpt); continue\n",
        "\n",
        "    print(f\"[GradCAM] Trying model {mn} with checkpoint {ckpt}\")\n",
        "    # choose a sample image to probe (first test image)\n",
        "    sample_img = test_paths[0] if 'test_paths' in globals() and len(test_paths)>0 else test_items[0][0]\n",
        "    try:\n",
        "        chosen_layer = find_spatial_module_name(fac, sample_img, num_classes_local=num_classes, device=DEVICE, img_size=IMG_SIZE)\n",
        "        if chosen_layer is None:\n",
        "            print(f\"[GradCAM] No spatial module automatically found for {mn}; you can choose a conv-like module name manually.\")\n",
        "            continue\n",
        "        print(f\"[GradCAM] Auto-chosen layer for {mn}: {chosen_layer}\")\n",
        "        save_path = os.path.join(PLOTS_DIR, f'{mn}_gradcam_grid.png')\n",
        "        save_gradcam_grid_pytorch(fac, ckpt, chosen_layer, test_paths, save_path, max_images=12, img_size=IMG_SIZE, device=DEVICE)\n",
        "    except Exception as e:\n",
        "        print(\"[GradCAM] Error for model\", mn, e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"Grad-CAM auto-run complete. Check\", PLOTS_DIR)\n"
      ],
      "metadata": {
        "id": "9HuPqXFinoO3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1763837772647,
          "user_tz": -330,
          "elapsed": 22205,
          "user": {
            "displayName": "Khushboo Kumari",
            "userId": "08877604357129947344"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08511d37-6550-4ce8-c256-fd5bc1d888bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "#   UNIVERSAL GRAD-CAM (CNN + ViT + Swin)\n",
        "#   - Saves individual images\n",
        "#   - Saves grid\n",
        "# ============================================\n",
        "import cv2, math, os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def save_gradcam_grid_pytorch(factory, ckpt, layer_name, paths, save_path,\n",
        "                              max_images=12, img_size=(224,224),\n",
        "                              device=DEVICE):\n",
        "\n",
        "    # -----------------------\n",
        "    # Load model + checkpoint\n",
        "    # -----------------------\n",
        "    model = factory(num_classes).to(device)\n",
        "    st = torch.load(ckpt, map_location=device)\n",
        "\n",
        "    if isinstance(st, dict) and 'state_dict' in st:\n",
        "        st = st['state_dict']\n",
        "\n",
        "    try: model.load_state_dict(st)\n",
        "    except: model.load_state_dict(st, strict=False)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Find chosen layer\n",
        "    # -----------------------------------\n",
        "    modules = dict(model.named_modules())\n",
        "    if layer_name not in modules:\n",
        "        print(\"Layer\", layer_name, \"not found. Available:\", list(modules.keys())[-30:])\n",
        "        return\n",
        "\n",
        "    target = modules[layer_name]\n",
        "\n",
        "    # holders\n",
        "    feat_holder = {'feat': None}\n",
        "    grad_holder = {'grad': None}\n",
        "\n",
        "    # forward hook\n",
        "    def fh(m, inp, out):\n",
        "        if isinstance(out, torch.Tensor):\n",
        "            feat_holder['feat'] = out.detach()\n",
        "        elif isinstance(out, (tuple,list)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n",
        "            feat_holder['feat'] = out[0].detach()\n",
        "\n",
        "    # backward hook\n",
        "    def bh(m, gin, gout):\n",
        "        g = gout[0] if isinstance(gout, (tuple,list)) else gout\n",
        "        if isinstance(g, torch.Tensor):\n",
        "            grad_holder['grad'] = g.detach()\n",
        "\n",
        "    # Try full backward hook (new) or fallback\n",
        "    try:\n",
        "        h1 = target.register_forward_hook(fh)\n",
        "        h2 = target.register_full_backward_hook(bh)\n",
        "    except:\n",
        "        h1 = target.register_forward_hook(fh)\n",
        "        h2 = target.register_backward_hook(bh)\n",
        "\n",
        "    # -----------------------\n",
        "    # Preprocessing\n",
        "    # -----------------------\n",
        "    tf_img = transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    overlays = []\n",
        "    individual_dir = save_path.replace(\".png\", \"_individual\")\n",
        "    os.makedirs(individual_dir, exist_ok=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # Grad-CAM utility\n",
        "    # -----------------------\n",
        "    def make_cam(feat, grad, target_hw):\n",
        "\n",
        "        if feat is None or grad is None:\n",
        "            return None\n",
        "\n",
        "        # remove batch\n",
        "        if feat.ndim == 4 and feat.shape[0] == 1: feat = feat[0]\n",
        "        if grad.ndim == 4 and grad.shape[0] == 1: grad = grad[0]\n",
        "\n",
        "        # -------------------------------\n",
        "        # Case B: ViT-like (L,C) tokens\n",
        "        # -------------------------------\n",
        "        if feat.ndim == 3 and feat.shape[0] == 1 and feat.shape[2] > feat.shape[1]:\n",
        "            # (1,L,C) \u2192 remove batch \u2192 (L,C)\n",
        "            feat = feat[0]\n",
        "            grad = grad[0] if grad is not None and grad.ndim==3 else grad\n",
        "\n",
        "        # (L,C) \u2192 transpose \u2192 (C,L)\n",
        "        if feat.ndim == 2 and feat.shape[1] > feat.shape[0]:\n",
        "            feat = feat.permute(1,0)   # (C,L)\n",
        "            if grad is not None:\n",
        "                grad = grad.permute(1,0)\n",
        "\n",
        "        # -----------------------------------\n",
        "        # Case 1: CNN-style (C,H,W)\n",
        "        # -----------------------------------\n",
        "        if feat.ndim == 3 and feat.shape[1] > 1 and feat.shape[2] > 1:\n",
        "            w = grad.mean(dim=(1,2), keepdim=True)\n",
        "            cam = (w * feat).sum(dim=0).cpu().numpy()\n",
        "\n",
        "        # -----------------------------------\n",
        "        # Case 2: Tokens \u2014 (C,L)\n",
        "        # -----------------------------------\n",
        "        elif feat.ndim == 2:\n",
        "            C,L = feat.shape\n",
        "            s = int(np.sqrt(L))\n",
        "            if s*s != L: return None\n",
        "            w = grad.mean(dim=1)\n",
        "            cam = (w.unsqueeze(1) * feat).sum(dim=0).cpu().numpy()\n",
        "            cam = cam.reshape(s,s)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        # normalize\n",
        "        cam = np.maximum(cam,0)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-9)\n",
        "        cam = cv2.resize(cam, (target_hw[1], target_hw[0]))\n",
        "\n",
        "        return cam\n",
        "\n",
        "    # -----------------------\n",
        "    # Process each test image\n",
        "    # -----------------------\n",
        "    idx = 0\n",
        "    for p in paths[:max_images]:\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\").resize(img_size)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        x = tf_img(pil).unsqueeze(0).to(device)\n",
        "        x.requires_grad_(True)\n",
        "\n",
        "        feat_holder['feat'] = None\n",
        "        grad_holder['grad'] = None\n",
        "\n",
        "        out = model(x)\n",
        "        pred = int(out.argmax(1))\n",
        "        score = out[0,pred]\n",
        "\n",
        "        model.zero_grad()\n",
        "        try: score.backward(retain_graph=True)\n",
        "        except: score.backward()\n",
        "\n",
        "        feat = feat_holder['feat']\n",
        "        grad = grad_holder['grad']\n",
        "\n",
        "        cam = make_cam(feat, grad, img_size)\n",
        "        if cam is None:\n",
        "            print(\"\u26a0\ufe0f CAM failed for:\", p)\n",
        "            continue\n",
        "\n",
        "        # make overlay\n",
        "        heat = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "        heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB) / 255.0\n",
        "        base = np.array(pil) / 255.0\n",
        "        overlay = (0.5*base + 0.5*heat)\n",
        "        overlay = np.clip(overlay*255,0,255).astype(np.uint8)\n",
        "\n",
        "        overlays.append(overlay)\n",
        "\n",
        "        # save individual\n",
        "        Image.fromarray(overlay).save(os.path.join(individual_dir, f\"img_{idx+1:02d}.png\"))\n",
        "        idx += 1\n",
        "\n",
        "    # remove hooks\n",
        "    try:\n",
        "        h1.remove(); h2.remove()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if len(overlays)==0:\n",
        "        print(\"\u274c No Grad-CAM images created for:\", layer_name)\n",
        "        return\n",
        "\n",
        "    # -----------------------\n",
        "    # Make grid\n",
        "    # -----------------------\n",
        "    cols = 4\n",
        "    rows = math.ceil(len(overlays)/cols)\n",
        "    grid = Image.new(\"RGB\", (cols*img_size[0], rows*img_size[1]))\n",
        "\n",
        "    for i,arr in enumerate(overlays):\n",
        "        r = i//cols\n",
        "        c = i%cols\n",
        "        grid.paste(Image.fromarray(arr), (c*img_size[0], r*img_size[1]))\n",
        "\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    grid.save(save_path)\n",
        "    print(\"\u2705 Saved Grad-CAM grid:\", save_path)\n",
        "    print(\"\ud83d\udcc2 Individual images saved to:\", individual_dir)\n"
      ],
      "metadata": {
        "id": "7bRVf-F0Ab5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \ud83d\udd35 NEW: Replacement Grad-CAM function that RETURNS overlays, paths, preds\n",
        "# Paste this cell into your notebook to replace the old save_gradcam_grid_pytorch OR keep it as a new function.\n",
        "\n",
        "import cv2, math, os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def save_gradcam_grid_pytorch_return(factory, ckpt, layer_name, paths, save_path,\n",
        "                                     max_images=12, img_size=(224,224),\n",
        "                                     device=DEVICE, num_classes=num_classes, idx_to_class=idx_to_class):\n",
        "    \"\"\"\n",
        "    Similar to your original save_gradcam_grid_pytorch but returns:\n",
        "       overlays -> list of numpy arrays (H,W,3) uint8\n",
        "       paths_used -> list of source image paths\n",
        "       preds -> list of predicted class indices (ints)\n",
        "    Also saves individual images and a grid as before.\n",
        "    \"\"\"\n",
        "    # -----------------------\n",
        "    # Load model + checkpoint\n",
        "    # -----------------------\n",
        "    model = factory(num_classes).to(device)\n",
        "    st = torch.load(ckpt, map_location=device)\n",
        "\n",
        "    if isinstance(st, dict) and 'state_dict' in st:\n",
        "        st = st['state_dict']\n",
        "\n",
        "    try: model.load_state_dict(st)\n",
        "    except: model.load_state_dict(st, strict=False)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Find chosen layer\n",
        "    # -----------------------------------\n",
        "    modules = dict(model.named_modules())\n",
        "    if layer_name not in modules:\n",
        "        print(\"Layer\", layer_name, \"not found. Available (last 30):\", list(modules.keys())[-30:])\n",
        "        return [], [], []\n",
        "\n",
        "    target = modules[layer_name]\n",
        "\n",
        "    # holders\n",
        "    feat_holder = {'feat': None}\n",
        "    grad_holder = {'grad': None}\n",
        "\n",
        "    # forward hook\n",
        "    def fh(m, inp, out):\n",
        "        if isinstance(out, torch.Tensor):\n",
        "            feat_holder['feat'] = out.detach()\n",
        "        elif isinstance(out, (tuple,list)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n",
        "            feat_holder['feat'] = out[0].detach()\n",
        "\n",
        "    # backward hook\n",
        "    def bh(m, gin, gout):\n",
        "        g = gout[0] if isinstance(gout, (tuple,list)) else gout\n",
        "        if isinstance(g, torch.Tensor):\n",
        "            grad_holder['grad'] = g.detach()\n",
        "\n",
        "    # Try full backward hook (new) or fallback\n",
        "    try:\n",
        "        h1 = target.register_forward_hook(fh)\n",
        "        h2 = target.register_full_backward_hook(bh)\n",
        "    except:\n",
        "        h1 = target.register_forward_hook(fh)\n",
        "        h2 = target.register_backward_hook(bh)\n",
        "\n",
        "    # -----------------------\n",
        "    # Preprocessing\n",
        "    # -----------------------\n",
        "    tf_img = transforms.Compose([\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    overlays = []\n",
        "    preds = []\n",
        "    paths_used = []\n",
        "    individual_dir = save_path.replace(\".png\", \"_individual\")\n",
        "    os.makedirs(individual_dir, exist_ok=True)\n",
        "\n",
        "    # -----------------------\n",
        "    # Grad-CAM utility\n",
        "    # -----------------------\n",
        "    def make_cam(feat, grad, target_hw):\n",
        "\n",
        "        if feat is None or grad is None:\n",
        "            return None\n",
        "\n",
        "        # remove batch\n",
        "        if feat.ndim == 4 and feat.shape[0] == 1: feat = feat[0]\n",
        "        if grad is not None and grad.ndim == 4 and grad.shape[0] == 1: grad = grad[0]\n",
        "\n",
        "        # -------------------------------\n",
        "        # Case B: ViT-like (L,C) tokens\n",
        "        # -------------------------------\n",
        "        if feat.ndim == 3 and feat.shape[0] == 1 and feat.shape[2] > feat.shape[1]:\n",
        "            # (1,L,C) \u2192 remove batch \u2192 (L,C)\n",
        "            feat = feat[0]\n",
        "            grad = grad[0] if grad is not None and grad.ndim==3 else grad\n",
        "\n",
        "        # (L,C) \u2192 transpose \u2192 (C,L)\n",
        "        if feat.ndim == 2 and feat.shape[1] > feat.shape[0]:\n",
        "            feat = feat.permute(1,0)   # (C,L)\n",
        "            if grad is not None:\n",
        "                grad = grad.permute(1,0)\n",
        "\n",
        "        # -----------------------------------\n",
        "        # Case 1: CNN-style (C,H,W)\n",
        "        # -----------------------------------\n",
        "        if feat.ndim == 3 and feat.shape[1] > 1 and feat.shape[2] > 1:\n",
        "            w = grad.mean(dim=(1,2), keepdim=True)\n",
        "            cam = (w * feat).sum(dim=0).cpu().numpy()\n",
        "\n",
        "        # -----------------------------------\n",
        "        # Case 2: Tokens \u2014 (C,L)\n",
        "        # -----------------------------------\n",
        "        elif feat.ndim == 2:\n",
        "            C,L = feat.shape\n",
        "            s = int(np.sqrt(L))\n",
        "            if s*s != L: return None\n",
        "            w = grad.mean(dim=1)\n",
        "            cam = (w.unsqueeze(1) * feat).sum(dim=0).cpu().numpy()\n",
        "            cam = cam.reshape(s,s)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        # normalize\n",
        "        cam = np.maximum(cam,0)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-9)\n",
        "        cam = cv2.resize(cam, (target_hw[1], target_hw[0]))\n",
        "\n",
        "        return cam\n",
        "\n",
        "    # -----------------------\n",
        "    # Process each test image\n",
        "    # -----------------------\n",
        "    idx = 0\n",
        "    for p in paths[:max_images]:\n",
        "        try:\n",
        "            pil = Image.open(p).convert(\"RGB\").resize(img_size)\n",
        "        except Exception as e:\n",
        "            print(\"Skipping\", p, \" \u2014 read error:\", e)\n",
        "            continue\n",
        "\n",
        "        x = tf_img(pil).unsqueeze(0).to(device)\n",
        "        x.requires_grad_(True)\n",
        "\n",
        "        feat_holder['feat'] = None\n",
        "        grad_holder['grad'] = None\n",
        "\n",
        "        out = model(x)\n",
        "        pred = int(out.argmax(1))\n",
        "        score = out[0,pred]\n",
        "\n",
        "        model.zero_grad()\n",
        "        try: score.backward(retain_graph=True)\n",
        "        except: score.backward()\n",
        "\n",
        "        feat = feat_holder['feat']\n",
        "        grad = grad_holder['grad']\n",
        "\n",
        "        cam = make_cam(feat, grad, img_size)\n",
        "        if cam is None:\n",
        "            print(\"\u26a0\ufe0f CAM failed for:\", p)\n",
        "            continue\n",
        "\n",
        "        # make overlay\n",
        "        heat = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
        "        heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB) / 255.0\n",
        "        base = np.array(pil) / 255.0\n",
        "        overlay = (0.5*base + 0.5*heat)\n",
        "        overlay = np.clip(overlay*255,0,255).astype(np.uint8)\n",
        "\n",
        "        overlays.append(overlay)\n",
        "        preds.append(pred)\n",
        "        paths_used.append(p)\n",
        "\n",
        "        # save individual\n",
        "        Image.fromarray(overlay).save(os.path.join(individual_dir, f\"img_{idx+1:02d}.png\"))\n",
        "        idx += 1\n",
        "\n",
        "    # remove hooks\n",
        "    try:\n",
        "        h1.remove(); h2.remove()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if len(overlays)==0:\n",
        "        print(\"\u274c No Grad-CAM images created for:\", layer_name)\n",
        "        return [], [], []\n",
        "\n",
        "    # -----------------------\n",
        "    # Make grid\n",
        "    # -----------------------\n",
        "    cols = 4\n",
        "    rows = math.ceil(len(overlays)/cols)\n",
        "    grid = Image.new(\"RGB\", (cols*img_size[0], rows*img_size[1]))\n",
        "\n",
        "    for i,arr in enumerate(overlays):\n",
        "        r = i//cols\n",
        "        c = i%cols\n",
        "        grid.paste(Image.fromarray(arr), (c*img_size[0], r*img_size[1]))\n",
        "\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    grid.save(save_path)\n",
        "    print(\"\u2705 Saved Grad-CAM grid:\", save_path)\n",
        "    print(\"\ud83d\udcc2 Individual images saved to:\", individual_dir)\n",
        "\n",
        "    # \ud83d\udd35 NEW: return overlays, source paths and preds\n",
        "    return overlays, paths_used, preds\n",
        "\n",
        "\n",
        "# \ud83d\udd35 NEW: Interactive slider viewer that shows filename + predicted class\n",
        "def interactive_overlay_slider_with_labels(overlays, paths, preds, idx_to_class_map=None, start=0, figsize=(6,6)):\n",
        "    \"\"\"\n",
        "    overlays: list of numpy arrays (H,W,3) uint8\n",
        "    paths: list of corresponding source file paths\n",
        "    preds: list of predicted class indices\n",
        "    idx_to_class_map: dict mapping index -> class name (optional)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from ipywidgets import widgets, interact\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "    except Exception as e:\n",
        "        print(\"ipywidgets not available. Install with: !pip install ipywidgets\")\n",
        "        print(\"Error:\", e)\n",
        "        return\n",
        "\n",
        "    if len(overlays) == 0:\n",
        "        print(\"No overlays to display.\")\n",
        "        return\n",
        "\n",
        "    max_index = len(overlays) - 1\n",
        "\n",
        "    def _show(i=0):\n",
        "        plt.figure(figsize=figsize)\n",
        "        img = overlays[int(i)]\n",
        "        if img.dtype != np.uint8:\n",
        "            img_show = (img * 255).astype('uint8')\n",
        "        else:\n",
        "            img_show = img\n",
        "        title_parts = [f\"{int(i)+1}/{len(overlays)}\"]\n",
        "        # filename\n",
        "        try:\n",
        "            fname = os.path.basename(paths[int(i)])\n",
        "            title_parts.append(fname)\n",
        "        except:\n",
        "            pass\n",
        "        # predicted label\n",
        "        try:\n",
        "            lab = idx_to_class_map[preds[int(i)]] if idx_to_class_map is not None else preds[int(i)]\n",
        "            title_parts.append(f\"pred: {lab}\")\n",
        "        except:\n",
        "            title_parts.append(f\"pred: {preds[int(i)]}\")\n",
        "        plt.imshow(img_show)\n",
        "        plt.axis('off')\n",
        "        plt.title(\"  |  \".join(title_parts))\n",
        "        plt.show()\n",
        "\n",
        "    interact(_show,\n",
        "             i=widgets.IntSlider(min=0, max=max_index, step=1, value=start, description='Index'))\n"
      ],
      "metadata": {
        "id": "0py0Df8-XHU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # FEW-SHOT PLOTTING & EXPORT (READY TO PASTE)\n",
        "# # Paste this right after the few-shot cell that produced accs_5shot & accs_1shot\n",
        "# # ============================================================\n",
        "\n",
        "# ### >>> NEW\n",
        "# import os\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import numpy as np\n",
        "\n",
        "# ### >>> UPDATED\n",
        "# # use existing PLOTS_DIR and OUT_ROOT variables from your notebook\n",
        "# PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(OUT_ROOT, 'plots'))\n",
        "# os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# ### >>> NEW\n",
        "# # Ensure accs_5shot and accs_1shot exist\n",
        "# if 'accs_5shot' not in globals() or 'accs_1shot' not in globals():\n",
        "#     raise RuntimeError(\"accs_5shot or accs_1shot not found. Run the few-shot cell first.\")\n",
        "\n",
        "# # Stats\n",
        "# accs5 = np.array(accs_5shot)\n",
        "# accs1 = np.array(accs_1shot)\n",
        "\n",
        "# summary = {\n",
        "#     'setting': ['5-way-5-shot', '5-way-1-shot'],\n",
        "#     'mean_acc': [accs5.mean(), accs1.mean()],\n",
        "#     'std_acc':  [accs5.std(ddof=1), accs1.std(ddof=1)],\n",
        "#     'median':   [np.median(accs5), np.median(accs1)],\n",
        "#     'n_episodes':[len(accs5), len(accs1)]\n",
        "# }\n",
        "# df_summary = pd.DataFrame(summary)\n",
        "# ### >>> NEW\n",
        "# # Save summary CSV\n",
        "# csv_out = os.path.join(OUT_ROOT, 'fewshot_summary.csv')\n",
        "# df_summary.to_csv(csv_out, index=False)\n",
        "# print(\"Saved few-shot summary CSV ->\", csv_out)\n",
        "\n",
        "# # ---------- Plot 1: Histogram + KDE overlay ----------\n",
        "# plt.figure(figsize=(8,5))\n",
        "# sns.histplot(accs5, label='5-shot', stat='density', kde=True, alpha=0.6)\n",
        "# sns.histplot(accs1, label='1-shot', stat='density', kde=True, alpha=0.6)\n",
        "# plt.xlabel('Episode Accuracy')\n",
        "# plt.title('Few-Shot Episode Accuracy Distribution')\n",
        "# plt.legend()\n",
        "# hist_path = os.path.join(PLOTS_DIR, 'fewshot_accuracy_histogram.png')\n",
        "# plt.tight_layout(); plt.savefig(hist_path); plt.close()\n",
        "# print(\"Saved histogram ->\", hist_path)\n",
        "\n",
        "# # ---------- Plot 2: Boxplot comparison ----------\n",
        "# plt.figure(figsize=(6,5))\n",
        "# sns.boxplot(data=[accs5, accs1])\n",
        "# plt.xticks([0,1], ['5-way-5-shot', '5-way-1-shot'])\n",
        "# plt.ylabel('Episode Accuracy')\n",
        "# plt.title('Few-Shot Accuracy Boxplot')\n",
        "# box_path = os.path.join(PLOTS_DIR, 'fewshot_accuracy_boxplot.png')\n",
        "# plt.tight_layout(); plt.savefig(box_path); plt.close()\n",
        "# print(\"Saved boxplot ->\", box_path)\n",
        "\n",
        "# # ---------- Plot 3: Mean \u00b1 STD bar plot ----------\n",
        "# plt.figure(figsize=(6,4))\n",
        "# means = [accs5.mean()*100, accs1.mean()*100]\n",
        "# stds  = [accs5.std(ddof=1)*100, accs1.std(ddof=1)*100]\n",
        "# sns.barplot(x=['5-shot','1-shot'], y=means, yerr=stds, capsize=0.15)\n",
        "# plt.ylabel('Mean Accuracy (%)')\n",
        "# plt.title('Few-Shot Mean Accuracy \u00b1 Std')\n",
        "# for i, v in enumerate(means):\n",
        "#     plt.text(i, v + stds[i] + 0.5, f\"{v:.2f}% \u00b1 {stds[i]:.2f}%\", ha='center')\n",
        "# bar_path = os.path.join(PLOTS_DIR, 'fewshot_mean_std_bar.png')\n",
        "# plt.tight_layout(); plt.savefig(bar_path); plt.close()\n",
        "# print(\"Saved mean\u00b1std bar ->\", bar_path)\n",
        "\n",
        "# # ---------- Plot 4: Episode-level scatter (optional insight) ----------\n",
        "# plt.figure(figsize=(10,4))\n",
        "# plt.plot(np.arange(len(accs5)), np.sort(accs5)[::-1], label='5-shot (sorted)', alpha=0.8)\n",
        "# plt.plot(np.arange(len(accs1)), np.sort(accs1)[::-1], label='1-shot (sorted)', alpha=0.8)\n",
        "# plt.xlabel('Episode index (sorted)')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.title('Sorted Episode Accuracies (descending)')\n",
        "# plt.legend()\n",
        "# scatter_path = os.path.join(PLOTS_DIR, 'fewshot_sorted_episodes.png')\n",
        "# plt.tight_layout(); plt.savefig(scatter_path); plt.close()\n",
        "# print(\"Saved sorted-episodes plot ->\", scatter_path)\n",
        "\n",
        "# # ---------- Print quick summary to notebook output ----------\n",
        "# print(\"\\nFew-shot summary (printed):\")\n",
        "# print(df_summary.to_string(index=False))\n",
        "\n",
        "# ### >>> NEW\n",
        "# # Optional: add results to your combined summary CSV if it exists\n",
        "# combined_csv = os.path.join(OUT_ROOT, 'combined_summary.csv')\n",
        "# if os.path.exists(combined_csv):\n",
        "#     try:\n",
        "#         combined = pd.read_csv(combined_csv)\n",
        "#         fewshot_row = {\n",
        "#             'model': 'fewshot::swin_proto' if BACKBONE_NAME.startswith('swin') else f'fewshot::{BACKBONE_NAME}_proto',\n",
        "#             'accuracy': df_summary.loc[df_summary['setting']=='5-way-5-shot','mean_acc'].values[0],\n",
        "#             'macro_precision': np.nan, 'macro_recall': np.nan, 'macro_f1': np.nan\n",
        "#         }\n",
        "#         combined = pd.concat([combined, pd.DataFrame([fewshot_row])], ignore_index=True)\n",
        "#         combined.to_csv(os.path.join(OUT_ROOT,'combined_summary_with_fewshot.csv'), index=False)\n",
        "#         print(\"Appended few-shot summary to combined_summary -> combined_summary_with_fewshot.csv\")\n",
        "#     except Exception as e:\n",
        "#         print(\"Could not append to combined_summary:\", e)\n",
        "\n",
        "# print(\"\\nAll few-shot plots saved in:\", PLOTS_DIR)\n"
      ],
      "metadata": {
        "id": "UIMQsSwIgtV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# #  FEW-SHOT PROTOTYPE NETWORK (READY TO PASTE)\n",
        "# #  - integrates with your existing PyTorch pipeline\n",
        "# #  - few-shot 5-way-5-shot & 5-way-1-shot classification\n",
        "# #  - uses your existing timm backbones (Swin/DeiT/CoAtNet)\n",
        "# # ============================================================\n",
        "\n",
        "# ### >>> NEW\n",
        "# import random, math\n",
        "# import numpy as np\n",
        "# import torch, torch.nn as nn, torch.optim as optim\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "# from collections import defaultdict\n",
        "\n",
        "# device = DEVICE  # uses your existing device\n",
        "\n",
        "# ### >>> NEW\n",
        "# # ------------------------------------------------------------\n",
        "# # 1. BUILD EMBEDDING EXTRACTOR (BACKBONE WITHOUT CLASSIFIER)\n",
        "# # ------------------------------------------------------------\n",
        "# def make_embedding_extractor(factory, embed_dim=512):\n",
        "#     \"\"\"\n",
        "#     Converts any timm model into an embedding extractor by removing the classifier.\n",
        "#     \"\"\"\n",
        "#     model = factory(num_classes)  # your factory already expects num_classes\n",
        "#     model.reset_classifier(0) if hasattr(model, \"reset_classifier\") else None\n",
        "\n",
        "#     # Projection head to get fixed-dim embeddings\n",
        "#     feat_dim = getattr(model, \"num_features\", embed_dim)\n",
        "#     proj = nn.Sequential(\n",
        "#         nn.AdaptiveAvgPool2d(1),\n",
        "#         nn.Flatten(),\n",
        "#         nn.Linear(feat_dim, embed_dim)\n",
        "#     )\n",
        "\n",
        "#     model = model.to(device)\n",
        "#     proj = proj.to(device)\n",
        "\n",
        "#     # Freeze backbone, train projection only\n",
        "#     for p in model.parameters():\n",
        "#         p.requires_grad = False\n",
        "#     for p in proj.parameters():\n",
        "#         p.requires_grad = True\n",
        "\n",
        "#     return model, proj\n",
        "\n",
        "\n",
        "# ### >>> NEW\n",
        "# # Choose backbone (you can switch between swin_tiny, deit_small, coatnet)\n",
        "# BACKBONE_NAME = \"swin_tiny\"   # CHANGE if you want deit_small_distilled / coatnet_0\n",
        "# backbone_factory = pytorch_factories[BACKBONE_NAME]\n",
        "\n",
        "# backbone, proj_head = make_embedding_extractor(backbone_factory, embed_dim=512)\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------\n",
        "# # 2. FEW-SHOT EPISODE SAMPLER\n",
        "# # ------------------------------------------------------------\n",
        "# ### >>> NEW\n",
        "# class FewShotDataset:\n",
        "#     def __init__(self, items, img_size=(224,224)):\n",
        "#         self.items = items\n",
        "#         self.img_size = img_size\n",
        "\n",
        "#         self.by_class = defaultdict(list)\n",
        "#         for p, lbl in items:\n",
        "#             self.by_class[lbl].append(p)\n",
        "\n",
        "#         self.classes = sorted(self.by_class.keys())\n",
        "\n",
        "#         self.transform = transforms.Compose([\n",
        "#             transforms.Resize(img_size),\n",
        "#             transforms.RandomResizedCrop(img_size[0], scale=(0.7,1.0)),\n",
        "#             transforms.RandomHorizontalFlip(),\n",
        "#             transforms.ColorJitter(0.3,0.3,0.2,0.05),\n",
        "#             transforms.ToTensor(),\n",
        "#             transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "#         ])\n",
        "\n",
        "#     # sample N-way/K-shot episode\n",
        "#     def sample_episode(self, N_way=5, K_shot=5, Q_query=5):\n",
        "#         chosen = random.sample(self.classes, N_way)\n",
        "#         support, support_labels = [], []\n",
        "#         query, query_labels = [], []\n",
        "\n",
        "#         for i, cls in enumerate(chosen):\n",
        "#             imgs = random.sample(self.by_class[cls], K_shot + Q_query)\n",
        "#             support.extend(imgs[:K_shot])\n",
        "#             query.extend(imgs[K_shot:])\n",
        "#             support_labels.extend([i]*K_shot)\n",
        "#             query_labels.extend([i]*Q_query)\n",
        "\n",
        "#         return support, support_labels, query, query_labels\n",
        "\n",
        "\n",
        "# fs_train = FewShotDataset(train_items, img_size=IMG_SIZE)\n",
        "# fs_val   = FewShotDataset(val_items, img_size=IMG_SIZE)\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------\n",
        "# # 3. EMBEDDING FUNCTION\n",
        "# # ------------------------------------------------------------\n",
        "# ### >>> NEW\n",
        "# def embed_batch(backbone, proj, paths):\n",
        "#     backbone.eval(); proj.eval()\n",
        "#     embeds = []\n",
        "#     tf = transforms.Compose([\n",
        "#         transforms.Resize(IMG_SIZE),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "#     ])\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for p in paths:\n",
        "#             im = Image.open(p).convert(\"RGB\")\n",
        "#             x = tf(im).unsqueeze(0).to(device)\n",
        "\n",
        "#             feats = backbone.forward_features(x) if hasattr(backbone,\"forward_features\") else backbone(x)\n",
        "#             embedding = proj(feats)\n",
        "#             embedding = nn.functional.normalize(embedding, dim=1)\n",
        "\n",
        "#             embeds.append(embedding.cpu().numpy())\n",
        "\n",
        "#     return np.vstack(embeds)\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------\n",
        "# # 4. PROTOTYPE CREATION\n",
        "# # ------------------------------------------------------------\n",
        "# ### >>> NEW\n",
        "# def compute_prototypes(support_emb, support_labels, N_way):\n",
        "#     D = support_emb.shape[1]\n",
        "#     prototypes = np.zeros((N_way, D), dtype=np.float32)\n",
        "\n",
        "#     for c in range(N_way):\n",
        "#         idxs = [i for i,l in enumerate(support_labels) if l == c]\n",
        "#         prototypes[c] = support_emb[idxs].mean(axis=0)\n",
        "\n",
        "#     # normalize\n",
        "#     prototypes /= (np.linalg.norm(prototypes, axis=1, keepdims=True) + 1e-9)\n",
        "#     return prototypes\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------\n",
        "# # 5. EVALUATE SINGLE EPISODE\n",
        "# # ------------------------------------------------------------\n",
        "# ### >>> NEW\n",
        "# def evaluate_episode(backbone, proj, fs_dataset, N_way=5, K_shot=5, Q_query=5):\n",
        "#     support, s_lbl, query, q_lbl = fs_dataset.sample_episode(N_way, K_shot, Q_query)\n",
        "\n",
        "#     s_emb = embed_batch(backbone, proj, support)\n",
        "#     q_emb = embed_batch(backbone, proj, query)\n",
        "#     prototypes = compute_prototypes(s_emb, s_lbl, N_way)\n",
        "\n",
        "#     # distances\n",
        "#     logits = q_emb @ prototypes.T\n",
        "#     preds = np.argmax(logits, axis=1)\n",
        "#     acc = (preds == np.array(q_lbl)).mean()\n",
        "\n",
        "#     return acc\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------\n",
        "# # 6. FULL FEW-SHOT EVALUATION (RUN THIS CELL)\n",
        "# # ------------------------------------------------------------\n",
        "# ### >>> NEW\n",
        "# print(\"\\n==================================================\")\n",
        "# print(\" FEW-SHOT TESTING (5-way-5-shot and 5-way-1-shot)\")\n",
        "# print(\"==================================================\\n\")\n",
        "\n",
        "# episodes = 200  # you can increase to 1000\n",
        "\n",
        "# # 5-way-5-shot\n",
        "# accs_5shot = []\n",
        "# for _ in range(episodes):\n",
        "#     accs_5shot.append(evaluate_episode(backbone, proj_head, fs_val, N_way=5, K_shot=5, Q_query=10))\n",
        "\n",
        "# # 5-way-1-shot\n",
        "# accs_1shot = []\n",
        "# for _ in range(episodes):\n",
        "#     accs_1shot.append(evaluate_episode(backbone, proj_head, fs_val, N_way=5, K_shot=1, Q_query=10))\n",
        "\n",
        "\n",
        "# print(f\"5-way 5-shot accuracy: {np.mean(accs_5shot)*100:.2f}%\")\n",
        "# print(f\"5-way 1-shot accuracy: {np.mean(accs_1shot)*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "wLQrQ5XugBsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # HYBRID FUSION FEW-SHOT PROTOTYPE NETWORK\n",
        "# # - Fuse embeddings from Swin, DeiT, CoAtNet\n",
        "# # - Evaluate 5-way 5-shot and 5-way 1-shot\n",
        "# # - Produces plots and appends to combined_summary if present\n",
        "# # Paste this cell AFTER your existing FEW-SHOT block\n",
        "# # ============================================================\n",
        "\n",
        "# import numpy as np, os\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # --- 1) Build embedding extractors (frozen backbones + small proj heads) ---\n",
        "# # use embed_dim 256 per backbone to keep fused dim manageable\n",
        "# EMBED_DIM = 256\n",
        "\n",
        "# # create three independent embedding extractors (backbone + proj)\n",
        "# backbone_swin, proj_swin = make_embedding_extractor(pytorch_factories['swin_tiny'], embed_dim=EMBED_DIM)\n",
        "# backbone_deit, proj_deit = make_embedding_extractor(pytorch_factories['deit_small_distilled'], embed_dim=EMBED_DIM)\n",
        "# backbone_coat, proj_coat = make_embedding_extractor(pytorch_factories['coatnet_0'], embed_dim=EMBED_DIM)\n",
        "\n",
        "# # Put them in eval mode (proj heads are trainable by default in your extractor but here we assume evaluation-only)\n",
        "# backbone_swin.eval(); proj_swin.eval()\n",
        "# backbone_deit.eval(); proj_deit.eval()\n",
        "# backbone_coat.eval(); proj_coat.eval()\n",
        "\n",
        "# # --- 2) fused-embedding helper that calls your existing embed_batch() ---\n",
        "# def embed_fusion(paths):\n",
        "#     \"\"\"\n",
        "#     Given list of image file paths -> returns L2-normalized fused embeddings (N x (3*EMBED_DIM))\n",
        "#     Uses embed_batch(backbone, proj, paths) already defined in notebook.\n",
        "#     \"\"\"\n",
        "#     # compute each embedding (these return numpy arrays N x EMBED_DIM)\n",
        "#     emb_s = embed_batch(backbone_swin, proj_swin, paths)\n",
        "#     emb_d = embed_batch(backbone_deit, proj_deit, paths)\n",
        "#     emb_c = embed_batch(backbone_coat, proj_coat, paths)\n",
        "\n",
        "#     # concat\n",
        "#     fused = np.concatenate([emb_s, emb_d, emb_c], axis=1)\n",
        "#     # L2 normalize per-sample\n",
        "#     fused = fused / (np.linalg.norm(fused, axis=1, keepdims=True) + 1e-9)\n",
        "#     return fused\n",
        "\n",
        "# # --- 3) reuse compute_prototypes (already in notebook) ---\n",
        "# # compute_prototypes(support_emb, support_labels, N_way) -> prototypes normalized\n",
        "\n",
        "# # --- 4) evaluate a single fusion episode ---\n",
        "# def evaluate_episode_fusion(fs_dataset, N_way=5, K_shot=5, Q_query=10):\n",
        "#     support, s_lbl, query, q_lbl = fs_dataset.sample_episode(N_way=N_way, K_shot=K_shot, Q_query=Q_query)\n",
        "#     s_emb = embed_fusion(support)\n",
        "#     q_emb = embed_fusion(query)\n",
        "\n",
        "#     prototypes = compute_prototypes(s_emb, s_lbl, N_way)\n",
        "#     logits = q_emb @ prototypes.T   # cosine-like scores because embeddings normalized\n",
        "#     preds = np.argmax(logits, axis=1)\n",
        "#     acc = (preds == np.array(q_lbl)).mean()\n",
        "#     return acc\n",
        "\n",
        "# # --- 5) Run evaluation (episodes) ---\n",
        "# episodes = 200   # increase to 1000 for final runs if you have time\n",
        "\n",
        "# print(\"\\nRunning Hybrid-Fusion few-shot evaluation (this may take a while)...\")\n",
        "# accs_fusion_5shot = []\n",
        "# for _ in tqdm(range(episodes), desc='fusion 5-shot'):\n",
        "#     accs_fusion_5shot.append(evaluate_episode_fusion(fs_val, N_way=5, K_shot=5, Q_query=10))\n",
        "\n",
        "# accs_fusion_1shot = []\n",
        "# for _ in tqdm(range(episodes), desc='fusion 1-shot'):\n",
        "#     accs_fusion_1shot.append(evaluate_episode_fusion(fs_val, N_way=5, K_shot=1, Q_query=10))\n",
        "\n",
        "# # --- 6) Summarize & save results + plots (integrates with your PLOTS_DIR + OUT_ROOT) ---\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import pandas as pd\n",
        "\n",
        "# PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(OUT_ROOT, 'plots'))\n",
        "# os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "# acc5 = np.array(accs_fusion_5shot)\n",
        "# acc1 = np.array(accs_fusion_1shot)\n",
        "\n",
        "# summary = {\n",
        "#     'setting': ['5-way-5-shot', '5-way-1-shot'],\n",
        "#     'mean_acc': [acc5.mean(), acc1.mean()],\n",
        "#     'std_acc':  [acc5.std(ddof=1), acc1.std(ddof=1)],\n",
        "#     'median':   [np.median(acc5), np.median(acc1)],\n",
        "#     'n_episodes':[len(acc5), len(acc1)]\n",
        "# }\n",
        "# df_summary = pd.DataFrame(summary)\n",
        "# csv_out = os.path.join(OUT_ROOT, 'fewshot_hybrid_fusion_summary.csv')\n",
        "# df_summary.to_csv(csv_out, index=False)\n",
        "# print(\"Saved fusion summary CSV ->\", csv_out)\n",
        "\n",
        "# # Histogram\n",
        "# plt.figure(figsize=(8,5))\n",
        "# sns.histplot(acc5, label='fusion 5-shot', stat='density', kde=True, alpha=0.6)\n",
        "# sns.histplot(acc1, label='fusion 1-shot', stat='density', kde=True, alpha=0.6)\n",
        "# plt.xlabel('Episode Accuracy'); plt.title('Fusion Few-Shot Episode Accuracy')\n",
        "# plt.legend()\n",
        "# hist_path = os.path.join(PLOTS_DIR, 'fusion_fewshot_accuracy_histogram.png')\n",
        "# plt.tight_layout(); plt.savefig(hist_path); plt.close()\n",
        "# print(\"Saved histogram ->\", hist_path)\n",
        "\n",
        "# # Boxplot\n",
        "# plt.figure(figsize=(6,5))\n",
        "# sns.boxplot(data=[acc5, acc1])\n",
        "# plt.xticks([0,1], ['5-way-5-shot', '5-way-1-shot'])\n",
        "# plt.ylabel('Episode Accuracy')\n",
        "# plt.title('Fusion Few-Shot Accuracy Boxplot')\n",
        "# box_path = os.path.join(PLOTS_DIR, 'fusion_fewshot_boxplot.png')\n",
        "# plt.tight_layout(); plt.savefig(box_path); plt.close()\n",
        "# print(\"Saved boxplot ->\", box_path)\n",
        "\n",
        "# # Mean \u00b1 std bar\n",
        "# plt.figure(figsize=(6,4))\n",
        "# means = [acc5.mean()*100, acc1.mean()*100]\n",
        "# stds  = [acc5.std(ddof=1)*100, acc1.std(ddof=1)*100]\n",
        "# sns.barplot(x=['5-shot','1-shot'], y=means, yerr=stds, capsize=0.15)\n",
        "# plt.ylabel('Mean Accuracy (%)'); plt.title('Fusion Few-Shot Mean Accuracy \u00b1 Std')\n",
        "# for i, v in enumerate(means):\n",
        "#     plt.text(i, v + stds[i] + 0.5, f\"{v:.2f}% \u00b1 {stds[i]:.2f}%\", ha='center')\n",
        "# bar_path = os.path.join(PLOTS_DIR, 'fusion_fewshot_mean_std_bar.png')\n",
        "# plt.tight_layout(); plt.savefig(bar_path); plt.close()\n",
        "# print(\"Saved mean\u00b1std bar ->\", bar_path)\n",
        "\n",
        "# # Print summary\n",
        "# print(\"\\nHybrid Fusion few-shot summary:\")\n",
        "# print(df_summary.to_string(index=False))\n",
        "\n",
        "# # --- 7) Append to combined_summary.csv if present (safe) ---\n",
        "# combined_csv = os.path.join(OUT_ROOT, 'combined_summary.csv')\n",
        "# try:\n",
        "#     if os.path.exists(combined_csv):\n",
        "#         combined = pd.read_csv(combined_csv)\n",
        "#         fewshot_row = {\n",
        "#             'model': 'fewshot::hybrid_fusion',\n",
        "#             'accuracy': float(df_summary.loc[df_summary['setting']=='5-way-5-shot','mean_acc'].values[0]),\n",
        "#             'macro_precision': np.nan, 'macro_recall': np.nan, 'macro_f1': np.nan\n",
        "#         }\n",
        "#         combined = pd.concat([combined, pd.DataFrame([fewshot_row])], ignore_index=True)\n",
        "#         combined.to_csv(os.path.join(OUT_ROOT,'combined_summary_with_fewshot_hybrid.csv'), index=False)\n",
        "#         print(\"Appended fusion summary to combined_summary -> combined_summary_with_fewshot_hybrid.csv\")\n",
        "# except Exception as e:\n",
        "#     print(\"Could not append to combined_summary:\", e)\n",
        "\n",
        "# print(\"\\nFusion few-shot artifacts saved in:\", PLOTS_DIR)\n"
      ],
      "metadata": {
        "id": "FVq-7U_q5VNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
