{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"899f46c21b7748f5b000210f28e988a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_597f2957e9c14ced8d904e92c4b910d9","IPY_MODEL_53c9c2deacde4ff7843f7ad143c54989","IPY_MODEL_ba5545e2a9964ee4841d87155cc8ded8"],"layout":"IPY_MODEL_be075969092d411c8b1fb5ffd55f930d"}},"597f2957e9c14ced8d904e92c4b910d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee95993bd2704cab956d204b5bfbebaa","placeholder":"‚Äã","style":"IPY_MODEL_ffe2299229a94ef1846a874f48242389","value":"model.safetensors:‚Äá100%"}},"53c9c2deacde4ff7843f7ad143c54989":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98250c8536b54f8f8832f29dd87d58cf","max":109813430,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73d5bc66f68541eab7cfe02d245e673c","value":109813430}},"ba5545e2a9964ee4841d87155cc8ded8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcd5b71c55cc4425a1e545c9b8cf7208","placeholder":"‚Äã","style":"IPY_MODEL_f8883c1f4f664982b76f6eb4f70de164","value":"‚Äá110M/110M‚Äá[00:02&lt;00:00,‚Äá104MB/s]"}},"be075969092d411c8b1fb5ffd55f930d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee95993bd2704cab956d204b5bfbebaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffe2299229a94ef1846a874f48242389":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98250c8536b54f8f8832f29dd87d58cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73d5bc66f68541eab7cfe02d245e673c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcd5b71c55cc4425a1e545c9b8cf7208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8883c1f4f664982b76f6eb4f70de164":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install -q timm==0.9.2 transformers sentence-transformers torchmetrics scikit-learn statsmodels\n","# If you use BLIP captioning and sentence embeddings uncomment:\n","!pip install -q transformers sentence-transformers"],"metadata":{"id":"hd_cAqcqLn-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CSlTYfwEL38L","executionInfo":{"status":"ok","timestamp":1763828350104,"user_tz":-330,"elapsed":31088,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"e09a88dc-fb2f-4fca-e772-8e9cb76e23de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","# !cp '/content/drive/MyDrive/projectResearchPaper/plant/kaggle.json' ~/.kaggle/\n","!cp '/content/drive/MyDrive/test/kaggle.json' ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"uBKQz0F5Lzxa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/bin/bash\n","!kaggle datasets download warcoder/indian-medicinal-plant-image-dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7CcTPc2SL71A","executionInfo":{"status":"ok","timestamp":1763828358062,"user_tz":-330,"elapsed":5968,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"f4804915-1af5-46da-ca6b-4e35984b4f0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/warcoder/indian-medicinal-plant-image-dataset\n","License(s): Attribution 4.0 International (CC BY 4.0)\n","Downloading indian-medicinal-plant-image-dataset.zip to /content\n"," 93% 234M/253M [00:00<00:00, 581MB/s]\n","100% 253M/253M [00:00<00:00, 616MB/s]\n"]}]},{"cell_type":"code","source":["import zipfile\n","with zipfile.ZipFile('/content/indian-medicinal-plant-image-dataset.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content')"],"metadata":{"id":"jUr-ByJNL-oo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import os, random, math, time, json, zipfile\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","# TensorFlow / Keras\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, Model, applications\n","\n","# PyTorch & timm\n","import torch, torch.nn as nn, torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import timm\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Sklearn\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","from statsmodels.stats.contingency_tables import mcnemar"],"metadata":{"id":"OWhl-6G8MLJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Repro\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n","torch.manual_seed(SEED)"],"metadata":{"id":"eUZ_1NLFlMeA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763828397070,"user_tz":-330,"elapsed":67,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"4cab8440-1a80-484d-e940-a52dd9f2487a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7c5904f5b530>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Configuration (edit as needed)\n","# -------------------------\n","DATA_DIR = '/content/Medicinal plant dataset'  # change if different\n","OUT_ROOT = '/content/experiments'                      # where results will go\n","os.makedirs(OUT_ROOT, exist_ok=True)\n","PLOTS_DIR = os.path.join(OUT_ROOT, 'plots'); os.makedirs(PLOTS_DIR, exist_ok=True)\n","DATA_DIR = DATA_DIR\n","IMG_SIZE = (224,224)            # H,W\n","BATCH_SIZE_TF = 24\n","NUM_WORKERS = 4\n","EPOCHS_TF = 12\n","EPOCHS_PY = 12\n","FINE_TUNE_EPOCHS_TF = 10\n","LR_TF = 1e-3\n","LR_PY = 2e-5\n","FINE_TUNE_LR_TF = 1e-5\n","WEIGHT_DECAY = 1e-5\n","REPEATS = 1    # reduce during debugging; set >1 if you want repeats\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device:\", DEVICE)\n","import pathlib, os, random, shutil\n","data_dir = '/content/Medicinal plant dataset'\n","data_dir_path = pathlib.Path(data_dir)\n","classes = [d.name for d in data_dir_path.iterdir() if d.is_dir()]"],"metadata":{"id":"Ub4EmuR3lidK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763828397080,"user_tz":-330,"elapsed":17,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"cbfc07a5-2679-46ec-9e5b-46938d6e4bd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# Dataset (Keras-style listing)\n","# -------------------------\n","class FolderDatasetListing:\n","    def __init__(self, root_dir):\n","        self.root = Path(root_dir)\n","        classes = sorted([p.name for p in self.root.iterdir() if p.is_dir()])\n","        self.class_to_idx = {c:i for i,c in enumerate(classes)}\n","        self.items = []\n","        for c in classes:\n","            for f in (self.root / c).glob('*'):\n","                if f.suffix.lower() in ('.jpg','.jpeg','.png','.webp'):\n","                    self.items.append((str(f), self.class_to_idx[c]))\n","        print(f\"Found {len(self.items)} images across {len(classes)} classes.\")\n","        self.classes = classes\n","        self.num_classes = len(classes)\n","\n","full_dataset = FolderDatasetListing(DATA_DIR)\n","num_classes = full_dataset.num_classes\n","idx_to_class = {v:k for k,v in full_dataset.class_to_idx.items()}\n"],"metadata":{"id":"z4Mx4AY6l2-X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763828397202,"user_tz":-330,"elapsed":63,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"6d8d5b7f-8977-4db8-ac09-67634be34a89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5945 images across 40 classes.\n"]}]},{"cell_type":"code","source":["# Create train/val/test split indices\n","n = len(full_dataset.items)\n","train_n = int(0.8 * n)\n","val_n = int(0.1 * n)\n","test_n = n - train_n - val_n\n","indices = list(range(n))\n","random.shuffle(indices)\n","train_idxs = indices[:train_n]\n","val_idxs   = indices[train_n:train_n+val_n]\n","test_idxs  = indices[train_n+val_n:]"],"metadata":{"id":"SFV80-G9l8T3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper to build tf.data from indices\n","def make_tf_dataset_from_indices(indices_list, batch_size=BATCH_SIZE_TF, transform=None, shuffle=False):\n","    paths = [full_dataset.items[i][0] for i in indices_list]\n","    labels = [full_dataset.items[i][1] for i in indices_list]\n","    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n","    def _load(path, label):\n","        image = tf.io.read_file(path)\n","        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n","        image = tf.image.convert_image_dtype(image, tf.float32)\n","        image = tf.image.resize(image, IMG_SIZE)\n","        return image, label\n","    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n","    if shuffle:\n","        ds = ds.shuffle(1024)\n","    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return ds\n","\n","train_ds_tf = make_tf_dataset_from_indices(train_idxs, batch_size=BATCH_SIZE_TF, shuffle=True)\n","val_ds_tf   = make_tf_dataset_from_indices(val_idxs, batch_size=BATCH_SIZE_TF, shuffle=False)\n","test_ds_tf  = make_tf_dataset_from_indices(test_idxs, batch_size=BATCH_SIZE_TF, shuffle=False)\n"],"metadata":{"id":"W-Wdb8IBmBdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Also keep lists for PyTorch inference\n","train_items = [full_dataset.items[i] for i in train_idxs]\n","val_items = [full_dataset.items[i] for i in val_idxs]\n","test_items = [full_dataset.items[i] for i in test_idxs]\n"],"metadata":{"id":"O3On3B-8mIc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# TensorFlow / Keras Models (from your code) - fixed and ready\n","# -------------------------\n","from tensorflow.keras import applications, optimizers, callbacks, regularizers\n","\n","def build_mobilenetv2(input_shape=(*IMG_SIZE,3)):\n","    base = applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n","    base.trainable=False\n","    x = layers.GlobalAveragePooling2D()(base.output)\n","    # üü° CHANGE THIS: add L2 and lower dropout to reduce underfitting\n","    x = layers.Dense(128, activation='relu',\n","                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n","    x = layers.Dropout(0.2)(x)\n","    out = layers.Dense(num_classes, activation='softmax')(x)\n","    model = Model(base.input, out)\n","    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def build_resnet152(input_shape=(*IMG_SIZE,3)):\n","    base = applications.ResNet152(weights='imagenet', include_top=False, input_shape=input_shape)\n","    base.trainable=False\n","    x = layers.GlobalAveragePooling2D()(base.output)\n","    x = layers.Dense(128, activation='relu',\n","                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n","    x = layers.Dropout(0.2)(x)\n","    out = layers.Dense(num_classes, activation='softmax')(x)\n","    model = Model(base.input, out)\n","    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def build_xception(input_shape=(*IMG_SIZE,3)):\n","    base = applications.Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n","    base.trainable=False\n","    x = layers.GlobalAveragePooling2D()(base.output)\n","    x = layers.Dense(256, activation='relu',\n","                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n","    x = layers.Dropout(0.3)(x)\n","    out = layers.Dense(num_classes, activation='softmax')(x)\n","    model = Model(base.input, out)\n","    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def build_mobilenetv3(input_shape=(*IMG_SIZE,3), model_type='large'):\n","    try:\n","        if model_type=='large':\n","            base = applications.MobileNetV3Large(weights='imagenet', include_top=False, input_shape=input_shape)\n","        else:\n","            base = applications.MobileNetV3Small(weights='imagenet', include_top=False, input_shape=input_shape)\n","    except Exception as e:\n","        print(\"MobileNetV3 not available; falling back to MobileNetV2. Error:\", e)\n","        return build_mobilenetv2(input_shape)\n","    base.trainable=False\n","    x = layers.GlobalAveragePooling2D()(base.output)\n","    x = layers.Dense(128, activation='relu',\n","                     kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)\n","    x = layers.Dropout(0.2)(x)\n","    out = layers.Dense(num_classes, activation='softmax')(x)\n","    model = Model(base.input, out)\n","    model.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def build_resnet50_for_feats(input_shape=(*IMG_SIZE,3)):\n","    base = applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n","    base.trainable=False\n","    x = layers.GlobalAveragePooling2D()(base.output)\n","    model = Model(base.input, x)\n","    return model\n","\n","def build_vit_custom_improved(input_shape=(*IMG_SIZE,3), num_classes=num_classes,\n","                              patch_size=16, projection_dim=128, transformer_layers=6,\n","                              num_heads=8, mlp_dim=256):\n","    inputs = layers.Input(shape=input_shape)\n","    patches = layers.Conv2D(filters=projection_dim, kernel_size=patch_size, strides=patch_size, padding=\"valid\")(inputs)\n","    patches = layers.Reshape((-1, projection_dim))(patches)\n","    num_patches = patches.shape[1]\n","    positions = tf.range(start=0, limit=num_patches, delta=1)\n","    position_embeddings = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)(positions)\n","    encoded_patches = patches + position_embeddings\n","    for _ in range(transformer_layers):\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        mlp_output = keras.Sequential([\n","            layers.Dense(mlp_dim, activation='gelu'),\n","            layers.Dropout(0.1),\n","            layers.Dense(projection_dim),\n","            layers.Dropout(0.1)\n","        ])(x3)\n","        encoded_patches = layers.Add()([x2, mlp_output])\n","    x = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dropout(0.4)(x)\n","    x = layers.Dense(mlp_dim, activation='relu')(x)\n","    outputs = layers.Dense(num_classes, activation='softmax')(x)\n","    model = Model(inputs, outputs, name=\"VisionTransformer_Custom_Improved\")\n","    optimizer = optimizers.Adam(learning_rate=3e-4)\n","    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","# -------------------------\n","# Training helper for Keras: safe compile & fit\n","# -------------------------\n","def compile_and_fit(model, name, epochs=EPOCHS_TF, train_ds=train_ds_tf, val_ds=val_ds_tf, outdir=OUT_ROOT):\n","    cb = [\n","        callbacks.ModelCheckpoint(os.path.join(outdir, f'{name}_best.h5'), save_best_only=True, monitor='val_accuracy', mode='max'),\n","        callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n","        callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n","    ]\n","    history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks=cb, verbose=2)\n","    model.save(os.path.join(outdir, f'{name}_final.h5'))\n","    return history"],"metadata":{"id":"dlwFimTimQxf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Build models list (Keras)\n","# -------------------------\n","models_to_train = {}\n","models_to_train['MobileNetV2'] = build_mobilenetv2()\n","models_to_train['ResNet152'] = build_resnet152()\n","models_to_train['MobileNetV3'] = build_mobilenetv3(model_type='large')\n","models_to_train['ResNet50_feats'] = build_resnet50_for_feats()\n","# models_to_train['VisionTransformer_Custom'] = build_vit_custom_improved()\n","\n","# Optional: don't instantiate Xception on small runtimes (it is heavy)\n","# models_to_train['Xception'] = build_xception()"],"metadata":{"id":"g5TpC0U6mX5-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763828409420,"user_tz":-330,"elapsed":10213,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"1f36bf37-553f-4860-ae63-3db1f1327354"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m234698864/234698864\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n","\u001b[1m12683000/12683000\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m94765736/94765736\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# Training Keras classifiers (with head training + fine-tune)\n","# -------------------------\n","trained_classifiers = {}\n","histories = {}\n","reports = {}\n","cms = {}\n","data_dir = OUT_ROOT\n","\n","def plot_confusion(cm, class_names, save_path, title='Confusion Matrix'):\n","    plt.figure(figsize=(8,6))\n","    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n","    plt.title(title)\n","    plt.ylabel('True'); plt.xlabel('Predicted')\n","    plt.tight_layout()\n","    plt.savefig(save_path); plt.close()\n","\n","def save_plot_history(hist, save_path):\n","    plt.figure(figsize=(10,4))\n","    plt.subplot(1,2,1)\n","    plt.plot(hist.history.get('loss',[]), label='train_loss'); plt.plot(hist.history.get('val_loss',[]), label='val_loss'); plt.legend()\n","    plt.subplot(1,2,2)\n","    plt.plot(hist.history.get('accuracy',[]), label='train_acc'); plt.plot(hist.history.get('val_accuracy',[]), label='val_acc'); plt.legend()\n","    plt.tight_layout(); plt.savefig(save_path); plt.close()\n","\n","for name, model in list(models_to_train.items()):\n","    # If model is a feature extractor (ResNet50_feats), build a classifier head\n","    if name == 'ResNet50_feats':\n","        print(f\"Building classifier for {name}\")\n","        base = model\n","        inputs = base.input\n","        x = base.output\n","        x = layers.Dense(256, activation='relu',\n","                         kernel_regularizer=regularizers.l2(WEIGHT_DECAY))(x)   # üîµ NEW LINE: add L2\n","        x = layers.Dropout(0.2)(x)  # üü° CHANGE THIS (lower dropout)\n","        out = layers.Dense(num_classes, activation='softmax')(x)\n","        clf = Model(inputs, out)\n","        clf.compile(optimizer=optimizers.Adam(LR_TF), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","        # Train head then fine-tune:\n","        print(f\"Training head for {name} ...\")\n","        hist_head = compile_and_fit(clf, name, epochs=EPOCHS_TF)\n","        histories[name] = hist_head\n","        trained_classifiers[name] = clf\n","    else:\n","        print(f\"Training {name} - head first, then fine-tune\")\n","        # ---- Head training (base already frozen in builders) ----\n","        # üîµ NEW LINE: train initial head with existing compile in builder\n","        cb_head = [\n","            callbacks.ModelCheckpoint(os.path.join(OUT_ROOT, f'{name}_head_best.h5'), save_best_only=True, monitor='val_accuracy', mode='max'),\n","            callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n","            callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights=True)\n","        ]\n","        hist_head = model.fit(train_ds_tf, validation_data=val_ds_tf, epochs=EPOCHS_TF, callbacks=cb_head, verbose=2)\n","        histories[f\"{name}_head\"] = hist_head\n","        # üîµ NEW LINE: Save head model\n","        model.save(os.path.join(OUT_ROOT, f'{name}_head.h5'))\n","\n","        # ---- Fine-tune: unfreeze top layers ----\n","        # üîµ NEW BLOCK: unfreeze last UNFREEZE_AT layers of the base (if model has .layers and includes a pretrained base)\n","        # üü° CHANGE THIS: how many layers to unfreeze (tune if needed)\n","        UNFREEZE_AT = 100   # üü° CHANGE THIS (number of layers from end to unfreeze)\n","        # set UNFREEZE_AT=None to unfreeze all layers\n","        # Try to detect base model layers: often layer names include 'mobilenet'/'resnet' etc.\n","        try:\n","            # find a base inside the model by checking for a layer with name 'input' and then base layers\n","            # We'll attempt to find the deepest pretrained layer group by inspecting layer names\n","            # If model was built with include_top=False, early layers will be part of model.layers\n","            if UNFREEZE_AT is None:\n","                for layer in model.layers:\n","                    layer.trainable = True\n","            else:\n","                # Unfreeze last UNFREEZE_AT trainable layers\n","                # NOTE: this is conservative ‚Äî you can change UNFREEZE_AT as needed\n","                trainable_count = 0\n","                for layer in model.layers[::-1]:\n","                    if trainable_count < UNFREEZE_AT:\n","                        layer.trainable = True\n","                        trainable_count += 1\n","                    else:\n","                        layer.trainable = False\n","            # üîµ NEW LINE: recompile with a lower lr for fine-tuning\n","            model.compile(optimizer=optimizers.Adam(FINE_TUNE_LR_TF),\n","                          loss='sparse_categorical_crossentropy',\n","                          metrics=['accuracy'])\n","            # üîµ NEW LINE: fine-tune\n","            hist_ft = model.fit(train_ds_tf, validation_data=val_ds_tf,\n","                                epochs=EPOCHS_TF + FINE_TUNE_EPOCHS_TF,\n","                                initial_epoch=hist_head.epoch[-1] + 1 if hasattr(hist_head, 'epoch') and len(hist_head.epoch)>0 else 0,\n","                                callbacks=[\n","                                    callbacks.ModelCheckpoint(os.path.join(OUT_ROOT, f'{name}_ft_best.h5'), save_best_only=True, monitor='val_accuracy', mode='max'),\n","                                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n","                                    callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n","                                ],\n","                                verbose=2)\n","            histories[name] = hist_ft\n","            trained_classifiers[name] = model\n","            # üîµ NEW LINE: save final finetuned model\n","            model.save(os.path.join(OUT_ROOT, f'{name}_finetuned.h5'))\n","        except Exception as e:\n","            print(\"Warning: fine-tuning step failed for\", name, \" ‚Äî skipping fine-tune. Error:\", e)\n","            trained_classifiers[name] = model\n","            histories[name] = hist_head\n","\n","    # Evaluate on test set (create generator)\n","    test_paths_local = [full_dataset.items[i][0] for i in test_idxs]\n","    # use Keras predict on batched numpy arrays\n","    def keras_predict_from_paths(model, paths, batch=32, target_size=IMG_SIZE):\n","        arrs=[]\n","        preds=[]\n","        for i in range(0,len(paths),batch):\n","            batch_paths = paths[i:i+batch]\n","            imgs=[]\n","            for p in batch_paths:\n","                im = Image.open(p).resize(target_size)\n","                a = np.array(im).astype('float32')/255.0\n","                imgs.append(a)\n","            xb = np.stack(imgs,0)\n","            probs = model.predict(xb, verbose=0)\n","            preds.append(np.argmax(probs,axis=1))\n","        return np.concatenate(preds, axis=0)\n","    preds = keras_predict_from_paths(trained_classifiers[name], test_paths_local, batch=32)\n","    y_true = np.array([full_dataset.items[i][1] for i in test_idxs])\n","    rep = classification_report(y_true, preds, output_dict=True, zero_division=0)\n","    cm = confusion_matrix(y_true, preds)\n","    reports[name] = rep\n","    cms[name] = cm\n","    pd.DataFrame(rep).transpose().to_csv(os.path.join(data_dir, f\"{name}_classification_report.csv\"))\n","    np.save(os.path.join(data_dir, f\"{name}_cm.npy\"), cm)\n","    plot_confusion(cm, full_dataset.classes, os.path.join(data_dir, f\"{name}_confusion.png\"), title=f\"{name} Confusion Matrix\")\n","    # choose history to save: prefer finetune history if present\n","    save_plot_history(histories.get(name, histories.get(f\"{name}_head\")), os.path.join(data_dir, f\"{name}_train_curve.png\"))\n","\n","print(\"Keras training/evaluation done. Artifacts in\", data_dir)"],"metadata":{"id":"wCbtu5U4mgI5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763830843827,"user_tz":-330,"elapsed":2434403,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"f57e6d46-58b0-46e3-c267-12b42016e595"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training MobileNetV2 - head first, then fine-tune\n","Epoch 1/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 56s - 279ms/step - accuracy: 0.4508 - loss: 2.0994 - val_accuracy: 0.7593 - val_loss: 0.9658 - learning_rate: 1.0000e-03\n","Epoch 2/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 12s - 61ms/step - accuracy: 0.7754 - loss: 0.8285 - val_accuracy: 0.8232 - val_loss: 0.6333 - learning_rate: 1.0000e-03\n","Epoch 3/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 15s - 73ms/step - accuracy: 0.8474 - loss: 0.5392 - val_accuracy: 0.8956 - val_loss: 0.4462 - learning_rate: 1.0000e-03\n","Epoch 4/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 17s - 85ms/step - accuracy: 0.8921 - loss: 0.3913 - val_accuracy: 0.8973 - val_loss: 0.3933 - learning_rate: 1.0000e-03\n","Epoch 5/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 12s - 58ms/step - accuracy: 0.9155 - loss: 0.3044 - val_accuracy: 0.9074 - val_loss: 0.3484 - learning_rate: 1.0000e-03\n","Epoch 6/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 14s - 71ms/step - accuracy: 0.9361 - loss: 0.2243 - val_accuracy: 0.9175 - val_loss: 0.3277 - learning_rate: 1.0000e-03\n","Epoch 7/12\n","199/199 - 12s - 59ms/step - accuracy: 0.9464 - loss: 0.1975 - val_accuracy: 0.9141 - val_loss: 0.2942 - learning_rate: 1.0000e-03\n","Epoch 8/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 12s - 60ms/step - accuracy: 0.9657 - loss: 0.1457 - val_accuracy: 0.9259 - val_loss: 0.2498 - learning_rate: 1.0000e-03\n","Epoch 9/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 12s - 61ms/step - accuracy: 0.9676 - loss: 0.1271 - val_accuracy: 0.9327 - val_loss: 0.2480 - learning_rate: 1.0000e-03\n","Epoch 10/12\n","199/199 - 12s - 60ms/step - accuracy: 0.9722 - loss: 0.1188 - val_accuracy: 0.9209 - val_loss: 0.2799 - learning_rate: 1.0000e-03\n","Epoch 11/12\n","199/199 - 12s - 61ms/step - accuracy: 0.9767 - loss: 0.1003 - val_accuracy: 0.9310 - val_loss: 0.2272 - learning_rate: 1.0000e-03\n","Epoch 12/12\n","199/199 - 12s - 59ms/step - accuracy: 0.9748 - loss: 0.0958 - val_accuracy: 0.9226 - val_loss: 0.2597 - learning_rate: 1.0000e-03\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 67s - 336ms/step - accuracy: 0.5845 - loss: 1.4729 - val_accuracy: 0.9192 - val_loss: 0.2795 - learning_rate: 1.0000e-05\n","Epoch 14/22\n","199/199 - 13s - 63ms/step - accuracy: 0.7794 - loss: 0.7416 - val_accuracy: 0.8973 - val_loss: 0.2994 - learning_rate: 1.0000e-05\n","Epoch 15/22\n","199/199 - 13s - 64ms/step - accuracy: 0.8558 - loss: 0.4980 - val_accuracy: 0.8990 - val_loss: 0.2876 - learning_rate: 1.0000e-05\n","Epoch 16/22\n","199/199 - 13s - 66ms/step - accuracy: 0.8917 - loss: 0.3818 - val_accuracy: 0.9091 - val_loss: 0.2597 - learning_rate: 1.0000e-05\n","Epoch 17/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 13s - 68ms/step - accuracy: 0.9167 - loss: 0.3008 - val_accuracy: 0.9209 - val_loss: 0.2460 - learning_rate: 1.0000e-05\n","Epoch 18/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 21s - 106ms/step - accuracy: 0.9363 - loss: 0.2428 - val_accuracy: 0.9259 - val_loss: 0.2210 - learning_rate: 1.0000e-05\n","Epoch 19/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 20s - 103ms/step - accuracy: 0.9420 - loss: 0.2109 - val_accuracy: 0.9411 - val_loss: 0.2011 - learning_rate: 1.0000e-05\n","Epoch 20/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 14s - 70ms/step - accuracy: 0.9512 - loss: 0.1858 - val_accuracy: 0.9478 - val_loss: 0.1916 - learning_rate: 1.0000e-05\n","Epoch 21/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 14s - 69ms/step - accuracy: 0.9649 - loss: 0.1464 - val_accuracy: 0.9545 - val_loss: 0.1753 - learning_rate: 1.0000e-05\n","Epoch 22/22\n","199/199 - 14s - 69ms/step - accuracy: 0.9683 - loss: 0.1293 - val_accuracy: 0.9529 - val_loss: 0.1678 - learning_rate: 1.0000e-05\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training ResNet152 - head first, then fine-tune\n","Epoch 1/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 98s - 490ms/step - accuracy: 0.0271 - loss: 3.7074 - val_accuracy: 0.0118 - val_loss: 3.6784 - learning_rate: 1.0000e-03\n","Epoch 2/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 42s - 210ms/step - accuracy: 0.0433 - loss: 3.6533 - val_accuracy: 0.0505 - val_loss: 3.6286 - learning_rate: 1.0000e-03\n","Epoch 3/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 42s - 212ms/step - accuracy: 0.0519 - loss: 3.5998 - val_accuracy: 0.0556 - val_loss: 3.5688 - learning_rate: 1.0000e-03\n","Epoch 4/12\n","199/199 - 40s - 199ms/step - accuracy: 0.0614 - loss: 3.5501 - val_accuracy: 0.0522 - val_loss: 3.5314 - learning_rate: 1.0000e-03\n","Epoch 5/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 41s - 207ms/step - accuracy: 0.0654 - loss: 3.5202 - val_accuracy: 0.0623 - val_loss: 3.4905 - learning_rate: 1.0000e-03\n","Epoch 6/12\n","199/199 - 40s - 200ms/step - accuracy: 0.0692 - loss: 3.4957 - val_accuracy: 0.0623 - val_loss: 3.4619 - learning_rate: 1.0000e-03\n","Epoch 7/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 42s - 212ms/step - accuracy: 0.0706 - loss: 3.4782 - val_accuracy: 0.0741 - val_loss: 3.4615 - learning_rate: 1.0000e-03\n","Epoch 8/12\n","199/199 - 39s - 198ms/step - accuracy: 0.0709 - loss: 3.4549 - val_accuracy: 0.0673 - val_loss: 3.4246 - learning_rate: 1.0000e-03\n","Epoch 9/12\n","199/199 - 39s - 197ms/step - accuracy: 0.0717 - loss: 3.4472 - val_accuracy: 0.0707 - val_loss: 3.4138 - learning_rate: 1.0000e-03\n","Epoch 10/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 44s - 221ms/step - accuracy: 0.0795 - loss: 3.4314 - val_accuracy: 0.0791 - val_loss: 3.3946 - learning_rate: 1.0000e-03\n","Epoch 11/12\n","199/199 - 40s - 202ms/step - accuracy: 0.0841 - loss: 3.4059 - val_accuracy: 0.0724 - val_loss: 3.3643 - learning_rate: 1.0000e-03\n","Epoch 12/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 42s - 211ms/step - accuracy: 0.0839 - loss: 3.3915 - val_accuracy: 0.1044 - val_loss: 3.3599 - learning_rate: 1.0000e-03\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 147s - 736ms/step - accuracy: 0.0648 - loss: 3.4906 - val_accuracy: 0.0539 - val_loss: 3.5263 - learning_rate: 1.0000e-05\n","Epoch 14/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 64s - 319ms/step - accuracy: 0.0778 - loss: 3.3459 - val_accuracy: 0.0623 - val_loss: 3.4642 - learning_rate: 1.0000e-05\n","Epoch 15/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 63s - 318ms/step - accuracy: 0.0950 - loss: 3.2658 - val_accuracy: 0.0842 - val_loss: 3.2008 - learning_rate: 1.0000e-05\n","Epoch 16/22\n","199/199 - 57s - 288ms/step - accuracy: 0.1152 - loss: 3.1849 - val_accuracy: 0.0842 - val_loss: 3.2132 - learning_rate: 1.0000e-05\n","Epoch 17/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 65s - 326ms/step - accuracy: 0.1207 - loss: 3.1053 - val_accuracy: 0.1044 - val_loss: 3.1336 - learning_rate: 1.0000e-05\n","Epoch 18/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 80s - 403ms/step - accuracy: 0.1459 - loss: 3.0368 - val_accuracy: 0.1263 - val_loss: 3.1277 - learning_rate: 1.0000e-05\n","Epoch 19/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 69s - 349ms/step - accuracy: 0.1550 - loss: 2.9687 - val_accuracy: 0.1380 - val_loss: 3.0510 - learning_rate: 1.0000e-05\n","Epoch 20/22\n","199/199 - 57s - 284ms/step - accuracy: 0.1598 - loss: 2.9077 - val_accuracy: 0.1347 - val_loss: 3.0926 - learning_rate: 1.0000e-05\n","Epoch 21/22\n","199/199 - 57s - 287ms/step - accuracy: 0.1739 - loss: 2.8711 - val_accuracy: 0.1263 - val_loss: 3.0945 - learning_rate: 1.0000e-05\n","Epoch 22/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 67s - 337ms/step - accuracy: 0.1850 - loss: 2.8100 - val_accuracy: 0.1734 - val_loss: 2.9676 - learning_rate: 1.0000e-05\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training MobileNetV3 - head first, then fine-tune\n","Epoch 1/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 56s - 282ms/step - accuracy: 0.0288 - loss: 3.7033 - val_accuracy: 0.0168 - val_loss: 3.6981 - learning_rate: 1.0000e-03\n","Epoch 2/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 12s - 61ms/step - accuracy: 0.0275 - loss: 3.6855 - val_accuracy: 0.0269 - val_loss: 3.6845 - learning_rate: 1.0000e-03\n","Epoch 3/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 12s - 62ms/step - accuracy: 0.0406 - loss: 3.6621 - val_accuracy: 0.0354 - val_loss: 3.6493 - learning_rate: 1.0000e-03\n","Epoch 4/12\n","199/199 - 12s - 59ms/step - accuracy: 0.0469 - loss: 3.6223 - val_accuracy: 0.0303 - val_loss: 3.6092 - learning_rate: 1.0000e-03\n","Epoch 5/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 13s - 63ms/step - accuracy: 0.0549 - loss: 3.5775 - val_accuracy: 0.0438 - val_loss: 3.5683 - learning_rate: 1.0000e-03\n","Epoch 6/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 11s - 58ms/step - accuracy: 0.0591 - loss: 3.5488 - val_accuracy: 0.0471 - val_loss: 3.5381 - learning_rate: 1.0000e-03\n","Epoch 7/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 11s - 57ms/step - accuracy: 0.0555 - loss: 3.5306 - val_accuracy: 0.0572 - val_loss: 3.5277 - learning_rate: 1.0000e-03\n","Epoch 8/12\n","199/199 - 21s - 104ms/step - accuracy: 0.0608 - loss: 3.5177 - val_accuracy: 0.0539 - val_loss: 3.5163 - learning_rate: 1.0000e-03\n","Epoch 9/12\n","199/199 - 12s - 59ms/step - accuracy: 0.0568 - loss: 3.5029 - val_accuracy: 0.0488 - val_loss: 3.5064 - learning_rate: 1.0000e-03\n","Epoch 10/12\n","199/199 - 21s - 104ms/step - accuracy: 0.0614 - loss: 3.4950 - val_accuracy: 0.0539 - val_loss: 3.4935 - learning_rate: 1.0000e-03\n","Epoch 11/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 11s - 54ms/step - accuracy: 0.0622 - loss: 3.4837 - val_accuracy: 0.0690 - val_loss: 3.4884 - learning_rate: 1.0000e-03\n","Epoch 12/12\n","199/199 - 12s - 59ms/step - accuracy: 0.0648 - loss: 3.4799 - val_accuracy: 0.0640 - val_loss: 3.4759 - learning_rate: 1.0000e-03\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/22\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 74s - 371ms/step - accuracy: 0.0465 - loss: 3.9490 - val_accuracy: 0.0455 - val_loss: 3.7354 - learning_rate: 1.0000e-05\n","Epoch 14/22\n","199/199 - 13s - 68ms/step - accuracy: 0.0694 - loss: 3.4802 - val_accuracy: 0.0303 - val_loss: 3.8405 - learning_rate: 1.0000e-05\n","Epoch 15/22\n","199/199 - 13s - 64ms/step - accuracy: 0.0900 - loss: 3.3839 - val_accuracy: 0.0286 - val_loss: 3.8962 - learning_rate: 1.0000e-05\n","Epoch 16/22\n","\n","Epoch 16: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n","199/199 - 13s - 64ms/step - accuracy: 0.1100 - loss: 3.2970 - val_accuracy: 0.0236 - val_loss: 4.1073 - learning_rate: 1.0000e-05\n","Epoch 17/22\n","199/199 - 12s - 60ms/step - accuracy: 0.1289 - loss: 3.2411 - val_accuracy: 0.0185 - val_loss: 4.2395 - learning_rate: 5.0000e-06\n","Epoch 18/22\n","199/199 - 12s - 62ms/step - accuracy: 0.1392 - loss: 3.2221 - val_accuracy: 0.0202 - val_loss: 4.5220 - learning_rate: 5.0000e-06\n","Epoch 19/22\n","\n","Epoch 19: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n","199/199 - 12s - 61ms/step - accuracy: 0.1400 - loss: 3.1872 - val_accuracy: 0.0185 - val_loss: 4.7156 - learning_rate: 5.0000e-06\n","Epoch 20/22\n","199/199 - 13s - 65ms/step - accuracy: 0.1461 - loss: 3.1616 - val_accuracy: 0.0185 - val_loss: 4.8775 - learning_rate: 2.5000e-06\n","Epoch 21/22\n","199/199 - 14s - 71ms/step - accuracy: 0.1560 - loss: 3.1222 - val_accuracy: 0.0202 - val_loss: 4.8305 - learning_rate: 2.5000e-06\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Building classifier for ResNet50_feats\n","Training head for ResNet50_feats ...\n","Epoch 1/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 39s - 198ms/step - accuracy: 0.0387 - loss: 3.7034 - val_accuracy: 0.0354 - val_loss: 3.6308 - learning_rate: 1.0000e-03\n","Epoch 2/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 19s - 95ms/step - accuracy: 0.0534 - loss: 3.5931 - val_accuracy: 0.0505 - val_loss: 3.5366 - learning_rate: 1.0000e-03\n","Epoch 3/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 19s - 94ms/step - accuracy: 0.0652 - loss: 3.5262 - val_accuracy: 0.0589 - val_loss: 3.5000 - learning_rate: 1.0000e-03\n","Epoch 4/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 18s - 91ms/step - accuracy: 0.0675 - loss: 3.4882 - val_accuracy: 0.0707 - val_loss: 3.4509 - learning_rate: 1.0000e-03\n","Epoch 5/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 17s - 88ms/step - accuracy: 0.0772 - loss: 3.4623 - val_accuracy: 0.0791 - val_loss: 3.4104 - learning_rate: 1.0000e-03\n","Epoch 6/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 19s - 94ms/step - accuracy: 0.0784 - loss: 3.4399 - val_accuracy: 0.0892 - val_loss: 3.3852 - learning_rate: 1.0000e-03\n","Epoch 7/12\n","199/199 - 17s - 83ms/step - accuracy: 0.0925 - loss: 3.3955 - val_accuracy: 0.0791 - val_loss: 3.3640 - learning_rate: 1.0000e-03\n","Epoch 8/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 17s - 86ms/step - accuracy: 0.0995 - loss: 3.3747 - val_accuracy: 0.0926 - val_loss: 3.3205 - learning_rate: 1.0000e-03\n","Epoch 9/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 17s - 87ms/step - accuracy: 0.0980 - loss: 3.3429 - val_accuracy: 0.0960 - val_loss: 3.2909 - learning_rate: 1.0000e-03\n","Epoch 10/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 21s - 106ms/step - accuracy: 0.1037 - loss: 3.3211 - val_accuracy: 0.1111 - val_loss: 3.2969 - learning_rate: 1.0000e-03\n","Epoch 11/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 17s - 86ms/step - accuracy: 0.1102 - loss: 3.3016 - val_accuracy: 0.1145 - val_loss: 3.3281 - learning_rate: 1.0000e-03\n","Epoch 12/12\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["199/199 - 18s - 91ms/step - accuracy: 0.1125 - loss: 3.2835 - val_accuracy: 0.1212 - val_loss: 3.2504 - learning_rate: 1.0000e-03\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Keras training/evaluation done. Artifacts in /content/experiments\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# PyTorch section: Swin, DeiT-small, CoAtNet0 (timm)\n","# -------------------------\n","# Dataset wrapper for PyTorch using same items lists\n","class ImageFolderDatasetFromList(Dataset):\n","    def __init__(self, items_list, transform=None):\n","        self.items = items_list\n","        self.transform = transform\n","    def __len__(self): return len(self.items)\n","    def __getitem__(self, idx):\n","        p, lbl = self.items[idx]\n","        img = Image.open(p).convert('RGB')\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, int(lbl), p\n","\n","train_transform_pt = transforms.Compose([\n","    transforms.RandomResizedCrop(IMG_SIZE[0], scale=(0.6,1.0)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","val_transform_pt = transforms.Compose([\n","    transforms.Resize(IMG_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","])\n","\n","train_ds_pt = ImageFolderDatasetFromList(train_items, transform=train_transform_pt)\n","val_ds_pt   = ImageFolderDatasetFromList(val_items, transform=val_transform_pt)\n","test_ds_pt  = ImageFolderDatasetFromList(test_items, transform=val_transform_pt)\n","\n","train_loader_pt = DataLoader(train_ds_pt, batch_size=32, shuffle=True, num_workers=NUM_WORKERS)\n","val_loader_pt   = DataLoader(val_ds_pt, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)\n","test_loader_pt  = DataLoader(test_ds_pt, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)\n"],"metadata":{"id":"IFz36cuHmlUs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763830843829,"user_tz":-330,"elapsed":18,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"690e5c78-e298-41b6-c3bb-181cd2cc5a54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# factories\n","def make_swin_tiny(num_classes, pretrained=True):\n","    return timm.create_model('swin_tiny_patch4_window7_224', pretrained=pretrained, num_classes=num_classes)\n","def make_deit_small_distilled(num_classes, pretrained=True):\n","    return timm.create_model('deit_small_distilled_patch16_224', pretrained=pretrained, num_classes=num_classes)\n","def make_coatnet0(num_classes, pretrained=True):\n","    return timm.create_model('coatnet_0', pretrained=pretrained, num_classes=num_classes)\n","\n","pytorch_factories = {'swin_tiny': make_swin_tiny, 'deit_small_distilled': make_deit_small_distilled, 'coatnet_0': make_coatnet0}\n"],"metadata":{"id":"fE2fIVnsmorH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### >>> NEW: Print Model Summary (Swin / DeiT / any timm model)\n","\n","import torch\n","from torchsummary import summary  # pip install torchsummary if needed\n","\n","def show_model_summary(factory, img_size=(3,224,224)):\n","    # Create model WITHOUT classifier ‚Äî good for transfer learning\n","    model = factory(num_classes)\n","    model.eval()\n","    model = model.to(DEVICE)\n","\n","    print(\"\\n================ MODEL ARCHITECTURE ================\")\n","    print(model)\n","\n","    # Try torchsummary (may fail for some timm models ‚Äî ignore errors)\n","    try:\n","        summary(model, img_size)\n","    except Exception as e:\n","        print(\"\\n(torchsummary could not parse some models ‚Äî safe to ignore)\")\n","\n","\n","# ---- FIX: Use correct timm CoAtNet model name ----\n","pytorch_factories[\"coatnet_0\"] = lambda num_classes: timm.create_model(\n","    \"coatnet_0_rw_224.sw_in1k\", pretrained=True, num_classes=num_classes\n",")\n","\n","\n","show_model_summary(pytorch_factories[\"swin_tiny\"])\n","show_model_summary(pytorch_factories[\"deit_small_distilled\"])\n","show_model_summary(pytorch_factories[\"coatnet_0\"])\n"],"metadata":{"id":"jJkypvmB-ZMA","executionInfo":{"status":"ok","timestamp":1763831622062,"user_tz":-330,"elapsed":11126,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["899f46c21b7748f5b000210f28e988a7","597f2957e9c14ced8d904e92c4b910d9","53c9c2deacde4ff7843f7ad143c54989","ba5545e2a9964ee4841d87155cc8ded8","be075969092d411c8b1fb5ffd55f930d","ee95993bd2704cab956d204b5bfbebaa","ffe2299229a94ef1846a874f48242389","98250c8536b54f8f8832f29dd87d58cf","73d5bc66f68541eab7cfe02d245e673c","bcd5b71c55cc4425a1e545c9b8cf7208","f8883c1f4f664982b76f6eb4f70de164"]},"collapsed":true,"outputId":"de061ffb-bc37-4e85-e43b-ae1f8e406499"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ MODEL ARCHITECTURE ================\n","SwinTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n","    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (layers): Sequential(\n","    (0): SwinTransformerStage(\n","      (downsample): Identity()\n","      (blocks): Sequential(\n","        (0): SwinTransformerBlock(\n","          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=96, out_features=288, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=96, out_features=96, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): Identity()\n","          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=96, out_features=384, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=384, out_features=96, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (1): SwinTransformerBlock(\n","          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=96, out_features=288, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=96, out_features=96, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.009)\n","          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=96, out_features=384, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=384, out_features=96, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (1): SwinTransformerStage(\n","      (downsample): PatchMerging(\n","        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","        (reduction): Linear(in_features=384, out_features=192, bias=False)\n","      )\n","      (blocks): Sequential(\n","        (0): SwinTransformerBlock(\n","          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=192, out_features=576, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=192, out_features=192, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.018)\n","          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=192, out_features=768, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=768, out_features=192, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (1): SwinTransformerBlock(\n","          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=192, out_features=576, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=192, out_features=192, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.027)\n","          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=192, out_features=768, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=768, out_features=192, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (2): SwinTransformerStage(\n","      (downsample): PatchMerging(\n","        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (reduction): Linear(in_features=768, out_features=384, bias=False)\n","      )\n","      (blocks): Sequential(\n","        (0): SwinTransformerBlock(\n","          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=384, out_features=384, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.036)\n","          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (1): SwinTransformerBlock(\n","          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=384, out_features=384, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.045)\n","          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (2): SwinTransformerBlock(\n","          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=384, out_features=384, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.055)\n","          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (3): SwinTransformerBlock(\n","          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=384, out_features=384, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.064)\n","          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (4): SwinTransformerBlock(\n","          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=384, out_features=384, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.073)\n","          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (5): SwinTransformerBlock(\n","          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=384, out_features=384, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.082)\n","          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (3): SwinTransformerStage(\n","      (downsample): PatchMerging(\n","        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n","      )\n","      (blocks): Sequential(\n","        (0): SwinTransformerBlock(\n","          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.091)\n","          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","        (1): SwinTransformerBlock(\n","          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attn): WindowAttention(\n","            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","            (softmax): Softmax(dim=-1)\n","          )\n","          (drop_path): DropPath(drop_prob=0.100)\n","          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (head): ClassifierHead(\n","    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n","    (drop): Dropout(p=0.0, inplace=False)\n","    (fc): Linear(in_features=768, out_features=40, bias=True)\n","    (flatten): Identity()\n","  )\n",")\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 56, 56]           4,704\n","         LayerNorm-2           [-1, 56, 56, 96]             192\n","        PatchEmbed-3           [-1, 56, 56, 96]               0\n","          Identity-4           [-1, 56, 56, 96]               0\n","         LayerNorm-5           [-1, 56, 56, 96]             192\n","            Linear-6              [-1, 49, 288]          27,936\n","           Softmax-7            [-1, 3, 49, 49]               0\n","           Dropout-8            [-1, 3, 49, 49]               0\n","            Linear-9               [-1, 49, 96]           9,312\n","          Dropout-10               [-1, 49, 96]               0\n","  WindowAttention-11               [-1, 49, 96]               0\n","         Identity-12           [-1, 56, 56, 96]               0\n","        LayerNorm-13             [-1, 3136, 96]             192\n","           Linear-14            [-1, 3136, 384]          37,248\n","             GELU-15            [-1, 3136, 384]               0\n","          Dropout-16            [-1, 3136, 384]               0\n","           Linear-17             [-1, 3136, 96]          36,960\n","          Dropout-18             [-1, 3136, 96]               0\n","              Mlp-19             [-1, 3136, 96]               0\n","         Identity-20             [-1, 3136, 96]               0\n","SwinTransformerBlock-21           [-1, 56, 56, 96]               0\n","        LayerNorm-22           [-1, 56, 56, 96]             192\n","           Linear-23              [-1, 49, 288]          27,936\n","          Softmax-24            [-1, 3, 49, 49]               0\n","          Dropout-25            [-1, 3, 49, 49]               0\n","           Linear-26               [-1, 49, 96]           9,312\n","          Dropout-27               [-1, 49, 96]               0\n","  WindowAttention-28               [-1, 49, 96]               0\n","         DropPath-29           [-1, 56, 56, 96]               0\n","        LayerNorm-30             [-1, 3136, 96]             192\n","           Linear-31            [-1, 3136, 384]          37,248\n","             GELU-32            [-1, 3136, 384]               0\n","          Dropout-33            [-1, 3136, 384]               0\n","           Linear-34             [-1, 3136, 96]          36,960\n","          Dropout-35             [-1, 3136, 96]               0\n","              Mlp-36             [-1, 3136, 96]               0\n","         DropPath-37             [-1, 3136, 96]               0\n","SwinTransformerBlock-38           [-1, 56, 56, 96]               0\n","SwinTransformerStage-39           [-1, 56, 56, 96]               0\n","        LayerNorm-40          [-1, 28, 28, 384]             768\n","           Linear-41          [-1, 28, 28, 192]          73,728\n","     PatchMerging-42          [-1, 28, 28, 192]               0\n","        LayerNorm-43          [-1, 28, 28, 192]             384\n","           Linear-44              [-1, 49, 576]         111,168\n","          Softmax-45            [-1, 6, 49, 49]               0\n","          Dropout-46            [-1, 6, 49, 49]               0\n","           Linear-47              [-1, 49, 192]          37,056\n","          Dropout-48              [-1, 49, 192]               0\n","  WindowAttention-49              [-1, 49, 192]               0\n","         DropPath-50          [-1, 28, 28, 192]               0\n","        LayerNorm-51             [-1, 784, 192]             384\n","           Linear-52             [-1, 784, 768]         148,224\n","             GELU-53             [-1, 784, 768]               0\n","          Dropout-54             [-1, 784, 768]               0\n","           Linear-55             [-1, 784, 192]         147,648\n","          Dropout-56             [-1, 784, 192]               0\n","              Mlp-57             [-1, 784, 192]               0\n","         DropPath-58             [-1, 784, 192]               0\n","SwinTransformerBlock-59          [-1, 28, 28, 192]               0\n","        LayerNorm-60          [-1, 28, 28, 192]             384\n","           Linear-61              [-1, 49, 576]         111,168\n","          Softmax-62            [-1, 6, 49, 49]               0\n","          Dropout-63            [-1, 6, 49, 49]               0\n","           Linear-64              [-1, 49, 192]          37,056\n","          Dropout-65              [-1, 49, 192]               0\n","  WindowAttention-66              [-1, 49, 192]               0\n","         DropPath-67          [-1, 28, 28, 192]               0\n","        LayerNorm-68             [-1, 784, 192]             384\n","           Linear-69             [-1, 784, 768]         148,224\n","             GELU-70             [-1, 784, 768]               0\n","          Dropout-71             [-1, 784, 768]               0\n","           Linear-72             [-1, 784, 192]         147,648\n","          Dropout-73             [-1, 784, 192]               0\n","              Mlp-74             [-1, 784, 192]               0\n","         DropPath-75             [-1, 784, 192]               0\n","SwinTransformerBlock-76          [-1, 28, 28, 192]               0\n","SwinTransformerStage-77          [-1, 28, 28, 192]               0\n","        LayerNorm-78          [-1, 14, 14, 768]           1,536\n","           Linear-79          [-1, 14, 14, 384]         294,912\n","     PatchMerging-80          [-1, 14, 14, 384]               0\n","        LayerNorm-81          [-1, 14, 14, 384]             768\n","           Linear-82             [-1, 49, 1152]         443,520\n","          Softmax-83           [-1, 12, 49, 49]               0\n","          Dropout-84           [-1, 12, 49, 49]               0\n","           Linear-85              [-1, 49, 384]         147,840\n","          Dropout-86              [-1, 49, 384]               0\n","  WindowAttention-87              [-1, 49, 384]               0\n","         DropPath-88          [-1, 14, 14, 384]               0\n","        LayerNorm-89             [-1, 196, 384]             768\n","           Linear-90            [-1, 196, 1536]         591,360\n","             GELU-91            [-1, 196, 1536]               0\n","          Dropout-92            [-1, 196, 1536]               0\n","           Linear-93             [-1, 196, 384]         590,208\n","          Dropout-94             [-1, 196, 384]               0\n","              Mlp-95             [-1, 196, 384]               0\n","         DropPath-96             [-1, 196, 384]               0\n","SwinTransformerBlock-97          [-1, 14, 14, 384]               0\n","        LayerNorm-98          [-1, 14, 14, 384]             768\n","           Linear-99             [-1, 49, 1152]         443,520\n","         Softmax-100           [-1, 12, 49, 49]               0\n","         Dropout-101           [-1, 12, 49, 49]               0\n","          Linear-102              [-1, 49, 384]         147,840\n","         Dropout-103              [-1, 49, 384]               0\n"," WindowAttention-104              [-1, 49, 384]               0\n","        DropPath-105          [-1, 14, 14, 384]               0\n","       LayerNorm-106             [-1, 196, 384]             768\n","          Linear-107            [-1, 196, 1536]         591,360\n","            GELU-108            [-1, 196, 1536]               0\n","         Dropout-109            [-1, 196, 1536]               0\n","          Linear-110             [-1, 196, 384]         590,208\n","         Dropout-111             [-1, 196, 384]               0\n","             Mlp-112             [-1, 196, 384]               0\n","        DropPath-113             [-1, 196, 384]               0\n","SwinTransformerBlock-114          [-1, 14, 14, 384]               0\n","       LayerNorm-115          [-1, 14, 14, 384]             768\n","          Linear-116             [-1, 49, 1152]         443,520\n","         Softmax-117           [-1, 12, 49, 49]               0\n","         Dropout-118           [-1, 12, 49, 49]               0\n","          Linear-119              [-1, 49, 384]         147,840\n","         Dropout-120              [-1, 49, 384]               0\n"," WindowAttention-121              [-1, 49, 384]               0\n","        DropPath-122          [-1, 14, 14, 384]               0\n","       LayerNorm-123             [-1, 196, 384]             768\n","          Linear-124            [-1, 196, 1536]         591,360\n","            GELU-125            [-1, 196, 1536]               0\n","         Dropout-126            [-1, 196, 1536]               0\n","          Linear-127             [-1, 196, 384]         590,208\n","         Dropout-128             [-1, 196, 384]               0\n","             Mlp-129             [-1, 196, 384]               0\n","        DropPath-130             [-1, 196, 384]               0\n","SwinTransformerBlock-131          [-1, 14, 14, 384]               0\n","       LayerNorm-132          [-1, 14, 14, 384]             768\n","          Linear-133             [-1, 49, 1152]         443,520\n","         Softmax-134           [-1, 12, 49, 49]               0\n","         Dropout-135           [-1, 12, 49, 49]               0\n","          Linear-136              [-1, 49, 384]         147,840\n","         Dropout-137              [-1, 49, 384]               0\n"," WindowAttention-138              [-1, 49, 384]               0\n","        DropPath-139          [-1, 14, 14, 384]               0\n","       LayerNorm-140             [-1, 196, 384]             768\n","          Linear-141            [-1, 196, 1536]         591,360\n","            GELU-142            [-1, 196, 1536]               0\n","         Dropout-143            [-1, 196, 1536]               0\n","          Linear-144             [-1, 196, 384]         590,208\n","         Dropout-145             [-1, 196, 384]               0\n","             Mlp-146             [-1, 196, 384]               0\n","        DropPath-147             [-1, 196, 384]               0\n","SwinTransformerBlock-148          [-1, 14, 14, 384]               0\n","       LayerNorm-149          [-1, 14, 14, 384]             768\n","          Linear-150             [-1, 49, 1152]         443,520\n","         Softmax-151           [-1, 12, 49, 49]               0\n","         Dropout-152           [-1, 12, 49, 49]               0\n","          Linear-153              [-1, 49, 384]         147,840\n","         Dropout-154              [-1, 49, 384]               0\n"," WindowAttention-155              [-1, 49, 384]               0\n","        DropPath-156          [-1, 14, 14, 384]               0\n","       LayerNorm-157             [-1, 196, 384]             768\n","          Linear-158            [-1, 196, 1536]         591,360\n","            GELU-159            [-1, 196, 1536]               0\n","         Dropout-160            [-1, 196, 1536]               0\n","          Linear-161             [-1, 196, 384]         590,208\n","         Dropout-162             [-1, 196, 384]               0\n","             Mlp-163             [-1, 196, 384]               0\n","        DropPath-164             [-1, 196, 384]               0\n","SwinTransformerBlock-165          [-1, 14, 14, 384]               0\n","       LayerNorm-166          [-1, 14, 14, 384]             768\n","          Linear-167             [-1, 49, 1152]         443,520\n","         Softmax-168           [-1, 12, 49, 49]               0\n","         Dropout-169           [-1, 12, 49, 49]               0\n","          Linear-170              [-1, 49, 384]         147,840\n","         Dropout-171              [-1, 49, 384]               0\n"," WindowAttention-172              [-1, 49, 384]               0\n","        DropPath-173          [-1, 14, 14, 384]               0\n","       LayerNorm-174             [-1, 196, 384]             768\n","          Linear-175            [-1, 196, 1536]         591,360\n","            GELU-176            [-1, 196, 1536]               0\n","         Dropout-177            [-1, 196, 1536]               0\n","          Linear-178             [-1, 196, 384]         590,208\n","         Dropout-179             [-1, 196, 384]               0\n","             Mlp-180             [-1, 196, 384]               0\n","        DropPath-181             [-1, 196, 384]               0\n","SwinTransformerBlock-182          [-1, 14, 14, 384]               0\n","SwinTransformerStage-183          [-1, 14, 14, 384]               0\n","       LayerNorm-184           [-1, 7, 7, 1536]           3,072\n","          Linear-185            [-1, 7, 7, 768]       1,179,648\n","    PatchMerging-186            [-1, 7, 7, 768]               0\n","       LayerNorm-187            [-1, 7, 7, 768]           1,536\n","          Linear-188             [-1, 49, 2304]       1,771,776\n","         Softmax-189           [-1, 24, 49, 49]               0\n","         Dropout-190           [-1, 24, 49, 49]               0\n","          Linear-191              [-1, 49, 768]         590,592\n","         Dropout-192              [-1, 49, 768]               0\n"," WindowAttention-193              [-1, 49, 768]               0\n","        DropPath-194            [-1, 7, 7, 768]               0\n","       LayerNorm-195              [-1, 49, 768]           1,536\n","          Linear-196             [-1, 49, 3072]       2,362,368\n","            GELU-197             [-1, 49, 3072]               0\n","         Dropout-198             [-1, 49, 3072]               0\n","          Linear-199              [-1, 49, 768]       2,360,064\n","         Dropout-200              [-1, 49, 768]               0\n","             Mlp-201              [-1, 49, 768]               0\n","        DropPath-202              [-1, 49, 768]               0\n","SwinTransformerBlock-203            [-1, 7, 7, 768]               0\n","       LayerNorm-204            [-1, 7, 7, 768]           1,536\n","          Linear-205             [-1, 49, 2304]       1,771,776\n","         Softmax-206           [-1, 24, 49, 49]               0\n","         Dropout-207           [-1, 24, 49, 49]               0\n","          Linear-208              [-1, 49, 768]         590,592\n","         Dropout-209              [-1, 49, 768]               0\n"," WindowAttention-210              [-1, 49, 768]               0\n","        DropPath-211            [-1, 7, 7, 768]               0\n","       LayerNorm-212              [-1, 49, 768]           1,536\n","          Linear-213             [-1, 49, 3072]       2,362,368\n","            GELU-214             [-1, 49, 3072]               0\n","         Dropout-215             [-1, 49, 3072]               0\n","          Linear-216              [-1, 49, 768]       2,360,064\n","         Dropout-217              [-1, 49, 768]               0\n","             Mlp-218              [-1, 49, 768]               0\n","        DropPath-219              [-1, 49, 768]               0\n","SwinTransformerBlock-220            [-1, 7, 7, 768]               0\n","SwinTransformerStage-221            [-1, 7, 7, 768]               0\n","       LayerNorm-222            [-1, 7, 7, 768]           1,536\n","FastAdaptiveAvgPool-223                  [-1, 768]               0\n","        Identity-224                  [-1, 768]               0\n","SelectAdaptivePool2d-225                  [-1, 768]               0\n","         Dropout-226                  [-1, 768]               0\n","          Linear-227                   [-1, 40]          30,760\n","        Identity-228                   [-1, 40]               0\n","  ClassifierHead-229                   [-1, 40]               0\n","================================================================\n","Total params: 27,526,792\n","Trainable params: 27,526,792\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 255.01\n","Params size (MB): 105.01\n","Estimated Total Size (MB): 360.59\n","----------------------------------------------------------------\n","\n","================ MODEL ARCHITECTURE ================\n","VisionTransformerDistilled(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (patch_drop): Identity()\n","  (norm_pre): Identity()\n","  (blocks): Sequential(\n","    (0): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (1): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (2): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (3): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (4): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (5): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (6): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (7): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (8): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (9): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (10): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (11): Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","  )\n","  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","  (fc_norm): Identity()\n","  (head_drop): Dropout(p=0.0, inplace=False)\n","  (head): Linear(in_features=384, out_features=40, bias=True)\n","  (head_dist): Linear(in_features=384, out_features=40, bias=True)\n",")\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 384, 14, 14]         295,296\n","          Identity-2             [-1, 196, 384]               0\n","        PatchEmbed-3             [-1, 196, 384]               0\n","           Dropout-4             [-1, 198, 384]               0\n","         LayerNorm-5             [-1, 198, 384]             768\n","            Linear-6            [-1, 198, 1152]         443,520\n","          Identity-7           [-1, 6, 198, 64]               0\n","          Identity-8           [-1, 6, 198, 64]               0\n","            Linear-9             [-1, 198, 384]         147,840\n","          Dropout-10             [-1, 198, 384]               0\n","        Attention-11             [-1, 198, 384]               0\n","         Identity-12             [-1, 198, 384]               0\n","         Identity-13             [-1, 198, 384]               0\n","        LayerNorm-14             [-1, 198, 384]             768\n","           Linear-15            [-1, 198, 1536]         591,360\n","             GELU-16            [-1, 198, 1536]               0\n","          Dropout-17            [-1, 198, 1536]               0\n","           Linear-18             [-1, 198, 384]         590,208\n","          Dropout-19             [-1, 198, 384]               0\n","              Mlp-20             [-1, 198, 384]               0\n","         Identity-21             [-1, 198, 384]               0\n","         Identity-22             [-1, 198, 384]               0\n","            Block-23             [-1, 198, 384]               0\n","        LayerNorm-24             [-1, 198, 384]             768\n","           Linear-25            [-1, 198, 1152]         443,520\n","         Identity-26           [-1, 6, 198, 64]               0\n","         Identity-27           [-1, 6, 198, 64]               0\n","           Linear-28             [-1, 198, 384]         147,840\n","          Dropout-29             [-1, 198, 384]               0\n","        Attention-30             [-1, 198, 384]               0\n","         Identity-31             [-1, 198, 384]               0\n","         Identity-32             [-1, 198, 384]               0\n","        LayerNorm-33             [-1, 198, 384]             768\n","           Linear-34            [-1, 198, 1536]         591,360\n","             GELU-35            [-1, 198, 1536]               0\n","          Dropout-36            [-1, 198, 1536]               0\n","           Linear-37             [-1, 198, 384]         590,208\n","          Dropout-38             [-1, 198, 384]               0\n","              Mlp-39             [-1, 198, 384]               0\n","         Identity-40             [-1, 198, 384]               0\n","         Identity-41             [-1, 198, 384]               0\n","            Block-42             [-1, 198, 384]               0\n","        LayerNorm-43             [-1, 198, 384]             768\n","           Linear-44            [-1, 198, 1152]         443,520\n","         Identity-45           [-1, 6, 198, 64]               0\n","         Identity-46           [-1, 6, 198, 64]               0\n","           Linear-47             [-1, 198, 384]         147,840\n","          Dropout-48             [-1, 198, 384]               0\n","        Attention-49             [-1, 198, 384]               0\n","         Identity-50             [-1, 198, 384]               0\n","         Identity-51             [-1, 198, 384]               0\n","        LayerNorm-52             [-1, 198, 384]             768\n","           Linear-53            [-1, 198, 1536]         591,360\n","             GELU-54            [-1, 198, 1536]               0\n","          Dropout-55            [-1, 198, 1536]               0\n","           Linear-56             [-1, 198, 384]         590,208\n","          Dropout-57             [-1, 198, 384]               0\n","              Mlp-58             [-1, 198, 384]               0\n","         Identity-59             [-1, 198, 384]               0\n","         Identity-60             [-1, 198, 384]               0\n","            Block-61             [-1, 198, 384]               0\n","        LayerNorm-62             [-1, 198, 384]             768\n","           Linear-63            [-1, 198, 1152]         443,520\n","         Identity-64           [-1, 6, 198, 64]               0\n","         Identity-65           [-1, 6, 198, 64]               0\n","           Linear-66             [-1, 198, 384]         147,840\n","          Dropout-67             [-1, 198, 384]               0\n","        Attention-68             [-1, 198, 384]               0\n","         Identity-69             [-1, 198, 384]               0\n","         Identity-70             [-1, 198, 384]               0\n","        LayerNorm-71             [-1, 198, 384]             768\n","           Linear-72            [-1, 198, 1536]         591,360\n","             GELU-73            [-1, 198, 1536]               0\n","          Dropout-74            [-1, 198, 1536]               0\n","           Linear-75             [-1, 198, 384]         590,208\n","          Dropout-76             [-1, 198, 384]               0\n","              Mlp-77             [-1, 198, 384]               0\n","         Identity-78             [-1, 198, 384]               0\n","         Identity-79             [-1, 198, 384]               0\n","            Block-80             [-1, 198, 384]               0\n","        LayerNorm-81             [-1, 198, 384]             768\n","           Linear-82            [-1, 198, 1152]         443,520\n","         Identity-83           [-1, 6, 198, 64]               0\n","         Identity-84           [-1, 6, 198, 64]               0\n","           Linear-85             [-1, 198, 384]         147,840\n","          Dropout-86             [-1, 198, 384]               0\n","        Attention-87             [-1, 198, 384]               0\n","         Identity-88             [-1, 198, 384]               0\n","         Identity-89             [-1, 198, 384]               0\n","        LayerNorm-90             [-1, 198, 384]             768\n","           Linear-91            [-1, 198, 1536]         591,360\n","             GELU-92            [-1, 198, 1536]               0\n","          Dropout-93            [-1, 198, 1536]               0\n","           Linear-94             [-1, 198, 384]         590,208\n","          Dropout-95             [-1, 198, 384]               0\n","              Mlp-96             [-1, 198, 384]               0\n","         Identity-97             [-1, 198, 384]               0\n","         Identity-98             [-1, 198, 384]               0\n","            Block-99             [-1, 198, 384]               0\n","       LayerNorm-100             [-1, 198, 384]             768\n","          Linear-101            [-1, 198, 1152]         443,520\n","        Identity-102           [-1, 6, 198, 64]               0\n","        Identity-103           [-1, 6, 198, 64]               0\n","          Linear-104             [-1, 198, 384]         147,840\n","         Dropout-105             [-1, 198, 384]               0\n","       Attention-106             [-1, 198, 384]               0\n","        Identity-107             [-1, 198, 384]               0\n","        Identity-108             [-1, 198, 384]               0\n","       LayerNorm-109             [-1, 198, 384]             768\n","          Linear-110            [-1, 198, 1536]         591,360\n","            GELU-111            [-1, 198, 1536]               0\n","         Dropout-112            [-1, 198, 1536]               0\n","          Linear-113             [-1, 198, 384]         590,208\n","         Dropout-114             [-1, 198, 384]               0\n","             Mlp-115             [-1, 198, 384]               0\n","        Identity-116             [-1, 198, 384]               0\n","        Identity-117             [-1, 198, 384]               0\n","           Block-118             [-1, 198, 384]               0\n","       LayerNorm-119             [-1, 198, 384]             768\n","          Linear-120            [-1, 198, 1152]         443,520\n","        Identity-121           [-1, 6, 198, 64]               0\n","        Identity-122           [-1, 6, 198, 64]               0\n","          Linear-123             [-1, 198, 384]         147,840\n","         Dropout-124             [-1, 198, 384]               0\n","       Attention-125             [-1, 198, 384]               0\n","        Identity-126             [-1, 198, 384]               0\n","        Identity-127             [-1, 198, 384]               0\n","       LayerNorm-128             [-1, 198, 384]             768\n","          Linear-129            [-1, 198, 1536]         591,360\n","            GELU-130            [-1, 198, 1536]               0\n","         Dropout-131            [-1, 198, 1536]               0\n","          Linear-132             [-1, 198, 384]         590,208\n","         Dropout-133             [-1, 198, 384]               0\n","             Mlp-134             [-1, 198, 384]               0\n","        Identity-135             [-1, 198, 384]               0\n","        Identity-136             [-1, 198, 384]               0\n","           Block-137             [-1, 198, 384]               0\n","       LayerNorm-138             [-1, 198, 384]             768\n","          Linear-139            [-1, 198, 1152]         443,520\n","        Identity-140           [-1, 6, 198, 64]               0\n","        Identity-141           [-1, 6, 198, 64]               0\n","          Linear-142             [-1, 198, 384]         147,840\n","         Dropout-143             [-1, 198, 384]               0\n","       Attention-144             [-1, 198, 384]               0\n","        Identity-145             [-1, 198, 384]               0\n","        Identity-146             [-1, 198, 384]               0\n","       LayerNorm-147             [-1, 198, 384]             768\n","          Linear-148            [-1, 198, 1536]         591,360\n","            GELU-149            [-1, 198, 1536]               0\n","         Dropout-150            [-1, 198, 1536]               0\n","          Linear-151             [-1, 198, 384]         590,208\n","         Dropout-152             [-1, 198, 384]               0\n","             Mlp-153             [-1, 198, 384]               0\n","        Identity-154             [-1, 198, 384]               0\n","        Identity-155             [-1, 198, 384]               0\n","           Block-156             [-1, 198, 384]               0\n","       LayerNorm-157             [-1, 198, 384]             768\n","          Linear-158            [-1, 198, 1152]         443,520\n","        Identity-159           [-1, 6, 198, 64]               0\n","        Identity-160           [-1, 6, 198, 64]               0\n","          Linear-161             [-1, 198, 384]         147,840\n","         Dropout-162             [-1, 198, 384]               0\n","       Attention-163             [-1, 198, 384]               0\n","        Identity-164             [-1, 198, 384]               0\n","        Identity-165             [-1, 198, 384]               0\n","       LayerNorm-166             [-1, 198, 384]             768\n","          Linear-167            [-1, 198, 1536]         591,360\n","            GELU-168            [-1, 198, 1536]               0\n","         Dropout-169            [-1, 198, 1536]               0\n","          Linear-170             [-1, 198, 384]         590,208\n","         Dropout-171             [-1, 198, 384]               0\n","             Mlp-172             [-1, 198, 384]               0\n","        Identity-173             [-1, 198, 384]               0\n","        Identity-174             [-1, 198, 384]               0\n","           Block-175             [-1, 198, 384]               0\n","       LayerNorm-176             [-1, 198, 384]             768\n","          Linear-177            [-1, 198, 1152]         443,520\n","        Identity-178           [-1, 6, 198, 64]               0\n","        Identity-179           [-1, 6, 198, 64]               0\n","          Linear-180             [-1, 198, 384]         147,840\n","         Dropout-181             [-1, 198, 384]               0\n","       Attention-182             [-1, 198, 384]               0\n","        Identity-183             [-1, 198, 384]               0\n","        Identity-184             [-1, 198, 384]               0\n","       LayerNorm-185             [-1, 198, 384]             768\n","          Linear-186            [-1, 198, 1536]         591,360\n","            GELU-187            [-1, 198, 1536]               0\n","         Dropout-188            [-1, 198, 1536]               0\n","          Linear-189             [-1, 198, 384]         590,208\n","         Dropout-190             [-1, 198, 384]               0\n","             Mlp-191             [-1, 198, 384]               0\n","        Identity-192             [-1, 198, 384]               0\n","        Identity-193             [-1, 198, 384]               0\n","           Block-194             [-1, 198, 384]               0\n","       LayerNorm-195             [-1, 198, 384]             768\n","          Linear-196            [-1, 198, 1152]         443,520\n","        Identity-197           [-1, 6, 198, 64]               0\n","        Identity-198           [-1, 6, 198, 64]               0\n","          Linear-199             [-1, 198, 384]         147,840\n","         Dropout-200             [-1, 198, 384]               0\n","       Attention-201             [-1, 198, 384]               0\n","        Identity-202             [-1, 198, 384]               0\n","        Identity-203             [-1, 198, 384]               0\n","       LayerNorm-204             [-1, 198, 384]             768\n","          Linear-205            [-1, 198, 1536]         591,360\n","            GELU-206            [-1, 198, 1536]               0\n","         Dropout-207            [-1, 198, 1536]               0\n","          Linear-208             [-1, 198, 384]         590,208\n","         Dropout-209             [-1, 198, 384]               0\n","             Mlp-210             [-1, 198, 384]               0\n","        Identity-211             [-1, 198, 384]               0\n","        Identity-212             [-1, 198, 384]               0\n","           Block-213             [-1, 198, 384]               0\n","       LayerNorm-214             [-1, 198, 384]             768\n","          Linear-215            [-1, 198, 1152]         443,520\n","        Identity-216           [-1, 6, 198, 64]               0\n","        Identity-217           [-1, 6, 198, 64]               0\n","          Linear-218             [-1, 198, 384]         147,840\n","         Dropout-219             [-1, 198, 384]               0\n","       Attention-220             [-1, 198, 384]               0\n","        Identity-221             [-1, 198, 384]               0\n","        Identity-222             [-1, 198, 384]               0\n","       LayerNorm-223             [-1, 198, 384]             768\n","          Linear-224            [-1, 198, 1536]         591,360\n","            GELU-225            [-1, 198, 1536]               0\n","         Dropout-226            [-1, 198, 1536]               0\n","          Linear-227             [-1, 198, 384]         590,208\n","         Dropout-228             [-1, 198, 384]               0\n","             Mlp-229             [-1, 198, 384]               0\n","        Identity-230             [-1, 198, 384]               0\n","        Identity-231             [-1, 198, 384]               0\n","           Block-232             [-1, 198, 384]               0\n","       LayerNorm-233             [-1, 198, 384]             768\n","          Linear-234                   [-1, 40]          15,400\n","          Linear-235                   [-1, 40]          15,400\n","================================================================\n","Total params: 21,620,432\n","Trainable params: 21,620,432\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 211.71\n","Params size (MB): 82.48\n","Estimated Total Size (MB): 294.76\n","----------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/110M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"899f46c21b7748f5b000210f28e988a7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","================ MODEL ARCHITECTURE ================\n","MaxxVit(\n","  (stem): Stem(\n","    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","    (norm1): BatchNormAct2d(\n","      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","      (drop): Identity()\n","      (act): SiLU(inplace=True)\n","    )\n","    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (stages): Sequential(\n","    (0): MaxxVitStage(\n","      (blocks): Sequential(\n","        (0): MbConvBlock(\n","          (shortcut): Downsample2d(\n","            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","            (expand): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          )\n","          (pre_norm): BatchNormAct2d(\n","            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (down): Downsample2d(\n","            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","            (expand): Identity()\n","          )\n","          (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm1): BatchNormAct2d(\n","            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","          (se_early): SEModule(\n","            (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (bn): Identity()\n","            (act): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (gate): Sigmoid()\n","          )\n","          (norm2): BatchNormAct2d(\n","            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv3_1x1): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (drop_path): Identity()\n","        )\n","        (1): MbConvBlock(\n","          (shortcut): Identity()\n","          (pre_norm): BatchNormAct2d(\n","            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (down): Identity()\n","          (conv1_1x1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm1): BatchNormAct2d(\n","            384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv2_kxk): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (se_early): SEModule(\n","            (fc1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n","            (bn): Identity()\n","            (act): ReLU(inplace=True)\n","            (fc2): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (gate): Sigmoid()\n","          )\n","          (norm2): BatchNormAct2d(\n","            384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv3_1x1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (drop_path): Identity()\n","        )\n","      )\n","    )\n","    (1): MaxxVitStage(\n","      (blocks): Sequential(\n","        (0): MbConvBlock(\n","          (shortcut): Downsample2d(\n","            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","            (expand): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          )\n","          (pre_norm): BatchNormAct2d(\n","            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (down): Downsample2d(\n","            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","            (expand): Identity()\n","          )\n","          (conv1_1x1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm1): BatchNormAct2d(\n","            384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv2_kxk): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","          (se_early): SEModule(\n","            (fc1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n","            (bn): Identity()\n","            (act): ReLU(inplace=True)\n","            (fc2): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (gate): Sigmoid()\n","          )\n","          (norm2): BatchNormAct2d(\n","            384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv3_1x1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (drop_path): Identity()\n","        )\n","        (1): MbConvBlock(\n","          (shortcut): Identity()\n","          (pre_norm): BatchNormAct2d(\n","            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (down): Identity()\n","          (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm1): BatchNormAct2d(\n","            768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n","          (se_early): SEModule(\n","            (fc1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n","            (bn): Identity()\n","            (act): ReLU(inplace=True)\n","            (fc2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n","            (gate): Sigmoid()\n","          )\n","          (norm2): BatchNormAct2d(\n","            768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (drop_path): Identity()\n","        )\n","        (2): MbConvBlock(\n","          (shortcut): Identity()\n","          (pre_norm): BatchNormAct2d(\n","            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (down): Identity()\n","          (conv1_1x1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm1): BatchNormAct2d(\n","            768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv2_kxk): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n","          (se_early): SEModule(\n","            (fc1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n","            (bn): Identity()\n","            (act): ReLU(inplace=True)\n","            (fc2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n","            (gate): Sigmoid()\n","          )\n","          (norm2): BatchNormAct2d(\n","            768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n","            (drop): Identity()\n","            (act): SiLU(inplace=True)\n","          )\n","          (conv3_1x1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (drop_path): Identity()\n","        )\n","      )\n","    )\n","    (2): MaxxVitStage(\n","      (blocks): Sequential(\n","        (0): TransformerBlock2d(\n","          (shortcut): Downsample2d(\n","            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","            (expand): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          )\n","          (norm1): Sequential(\n","            (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n","            (down): Downsample2d(\n","              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              (expand): Identity()\n","            )\n","          )\n","          (attn): Attention2d(\n","            (qkv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (1): TransformerBlock2d(\n","          (shortcut): Identity()\n","          (norm1): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention2d(\n","            (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (2): TransformerBlock2d(\n","          (shortcut): Identity()\n","          (norm1): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention2d(\n","            (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (3): TransformerBlock2d(\n","          (shortcut): Identity()\n","          (norm1): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention2d(\n","            (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (4): TransformerBlock2d(\n","          (shortcut): Identity()\n","          (norm1): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention2d(\n","            (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (5): TransformerBlock2d(\n","          (shortcut): Identity()\n","          (norm1): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention2d(\n","            (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (6): TransformerBlock2d(\n","          (shortcut): Identity()\n","          (norm1): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention2d(\n","            (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","      )\n","    )\n","    (3): MaxxVitStage(\n","      (blocks): Sequential(\n","        (0): TransformerBlock2d(\n","          (shortcut): Downsample2d(\n","            (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","            (expand): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          )\n","          (norm1): Sequential(\n","            (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n","            (down): Downsample2d(\n","              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","              (expand): Identity()\n","            )\n","          )\n","          (attn): Attention2d(\n","            (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (1): TransformerBlock2d(\n","          (shortcut): Identity()\n","          (norm1): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention2d(\n","            (qkv): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))\n","            (rel_pos): RelPosBias()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): ConvMlp(\n","            (fc1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))\n","            (norm): Identity()\n","            (act): GELU()\n","            (drop): Dropout(p=0.0, inplace=False)\n","            (fc2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","      )\n","    )\n","  )\n","  (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n","  (head): ClassifierHead(\n","    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n","    (drop): Dropout(p=0.0, inplace=False)\n","    (fc): Linear(in_features=768, out_features=40, bias=True)\n","    (flatten): Identity()\n","  )\n",")\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 112, 112]             864\n","          Identity-2         [-1, 32, 112, 112]               0\n","              SiLU-3         [-1, 32, 112, 112]               0\n","    BatchNormAct2d-4         [-1, 32, 112, 112]              64\n","            Conv2d-5         [-1, 64, 112, 112]          18,432\n","              Stem-6         [-1, 64, 112, 112]               0\n","         AvgPool2d-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 96, 56, 56]           6,144\n","      Downsample2d-9           [-1, 96, 56, 56]               0\n","         Identity-10         [-1, 64, 112, 112]               0\n","             SiLU-11         [-1, 64, 112, 112]               0\n","   BatchNormAct2d-12         [-1, 64, 112, 112]             128\n","        AvgPool2d-13           [-1, 64, 56, 56]               0\n","         Identity-14           [-1, 64, 56, 56]               0\n","     Downsample2d-15           [-1, 64, 56, 56]               0\n","           Conv2d-16          [-1, 256, 56, 56]          16,384\n","         Identity-17          [-1, 256, 56, 56]               0\n","             SiLU-18          [-1, 256, 56, 56]               0\n","   BatchNormAct2d-19          [-1, 256, 56, 56]             512\n","           Conv2d-20          [-1, 256, 56, 56]           2,304\n","           Conv2d-21             [-1, 64, 1, 1]          16,448\n","         Identity-22             [-1, 64, 1, 1]               0\n","             ReLU-23             [-1, 64, 1, 1]               0\n","           Conv2d-24            [-1, 256, 1, 1]          16,640\n","          Sigmoid-25            [-1, 256, 1, 1]               0\n","         SEModule-26          [-1, 256, 56, 56]               0\n","         Identity-27          [-1, 256, 56, 56]               0\n","             SiLU-28          [-1, 256, 56, 56]               0\n","   BatchNormAct2d-29          [-1, 256, 56, 56]             512\n","           Conv2d-30           [-1, 96, 56, 56]          24,576\n","         Identity-31           [-1, 96, 56, 56]               0\n","      MbConvBlock-32           [-1, 96, 56, 56]               0\n","         Identity-33           [-1, 96, 56, 56]               0\n","         Identity-34           [-1, 96, 56, 56]               0\n","             SiLU-35           [-1, 96, 56, 56]               0\n","   BatchNormAct2d-36           [-1, 96, 56, 56]             192\n","         Identity-37           [-1, 96, 56, 56]               0\n","           Conv2d-38          [-1, 384, 56, 56]          36,864\n","         Identity-39          [-1, 384, 56, 56]               0\n","             SiLU-40          [-1, 384, 56, 56]               0\n","   BatchNormAct2d-41          [-1, 384, 56, 56]             768\n","           Conv2d-42          [-1, 384, 56, 56]           3,456\n","           Conv2d-43             [-1, 96, 1, 1]          36,960\n","         Identity-44             [-1, 96, 1, 1]               0\n","             ReLU-45             [-1, 96, 1, 1]               0\n","           Conv2d-46            [-1, 384, 1, 1]          37,248\n","          Sigmoid-47            [-1, 384, 1, 1]               0\n","         SEModule-48          [-1, 384, 56, 56]               0\n","         Identity-49          [-1, 384, 56, 56]               0\n","             SiLU-50          [-1, 384, 56, 56]               0\n","   BatchNormAct2d-51          [-1, 384, 56, 56]             768\n","           Conv2d-52           [-1, 96, 56, 56]          36,864\n","         Identity-53           [-1, 96, 56, 56]               0\n","      MbConvBlock-54           [-1, 96, 56, 56]               0\n","     MaxxVitStage-55           [-1, 96, 56, 56]               0\n","        AvgPool2d-56           [-1, 96, 28, 28]               0\n","           Conv2d-57          [-1, 192, 28, 28]          18,432\n","     Downsample2d-58          [-1, 192, 28, 28]               0\n","         Identity-59           [-1, 96, 56, 56]               0\n","             SiLU-60           [-1, 96, 56, 56]               0\n","   BatchNormAct2d-61           [-1, 96, 56, 56]             192\n","        AvgPool2d-62           [-1, 96, 28, 28]               0\n","         Identity-63           [-1, 96, 28, 28]               0\n","     Downsample2d-64           [-1, 96, 28, 28]               0\n","           Conv2d-65          [-1, 384, 28, 28]          36,864\n","         Identity-66          [-1, 384, 28, 28]               0\n","             SiLU-67          [-1, 384, 28, 28]               0\n","   BatchNormAct2d-68          [-1, 384, 28, 28]             768\n","           Conv2d-69          [-1, 384, 28, 28]           3,456\n","           Conv2d-70             [-1, 96, 1, 1]          36,960\n","         Identity-71             [-1, 96, 1, 1]               0\n","             ReLU-72             [-1, 96, 1, 1]               0\n","           Conv2d-73            [-1, 384, 1, 1]          37,248\n","          Sigmoid-74            [-1, 384, 1, 1]               0\n","         SEModule-75          [-1, 384, 28, 28]               0\n","         Identity-76          [-1, 384, 28, 28]               0\n","             SiLU-77          [-1, 384, 28, 28]               0\n","   BatchNormAct2d-78          [-1, 384, 28, 28]             768\n","           Conv2d-79          [-1, 192, 28, 28]          73,728\n","         Identity-80          [-1, 192, 28, 28]               0\n","      MbConvBlock-81          [-1, 192, 28, 28]               0\n","         Identity-82          [-1, 192, 28, 28]               0\n","         Identity-83          [-1, 192, 28, 28]               0\n","             SiLU-84          [-1, 192, 28, 28]               0\n","   BatchNormAct2d-85          [-1, 192, 28, 28]             384\n","         Identity-86          [-1, 192, 28, 28]               0\n","           Conv2d-87          [-1, 768, 28, 28]         147,456\n","         Identity-88          [-1, 768, 28, 28]               0\n","             SiLU-89          [-1, 768, 28, 28]               0\n","   BatchNormAct2d-90          [-1, 768, 28, 28]           1,536\n","           Conv2d-91          [-1, 768, 28, 28]           6,912\n","           Conv2d-92            [-1, 192, 1, 1]         147,648\n","         Identity-93            [-1, 192, 1, 1]               0\n","             ReLU-94            [-1, 192, 1, 1]               0\n","           Conv2d-95            [-1, 768, 1, 1]         148,224\n","          Sigmoid-96            [-1, 768, 1, 1]               0\n","         SEModule-97          [-1, 768, 28, 28]               0\n","         Identity-98          [-1, 768, 28, 28]               0\n","             SiLU-99          [-1, 768, 28, 28]               0\n","  BatchNormAct2d-100          [-1, 768, 28, 28]           1,536\n","          Conv2d-101          [-1, 192, 28, 28]         147,456\n","        Identity-102          [-1, 192, 28, 28]               0\n","     MbConvBlock-103          [-1, 192, 28, 28]               0\n","        Identity-104          [-1, 192, 28, 28]               0\n","        Identity-105          [-1, 192, 28, 28]               0\n","            SiLU-106          [-1, 192, 28, 28]               0\n","  BatchNormAct2d-107          [-1, 192, 28, 28]             384\n","        Identity-108          [-1, 192, 28, 28]               0\n","          Conv2d-109          [-1, 768, 28, 28]         147,456\n","        Identity-110          [-1, 768, 28, 28]               0\n","            SiLU-111          [-1, 768, 28, 28]               0\n","  BatchNormAct2d-112          [-1, 768, 28, 28]           1,536\n","          Conv2d-113          [-1, 768, 28, 28]           6,912\n","          Conv2d-114            [-1, 192, 1, 1]         147,648\n","        Identity-115            [-1, 192, 1, 1]               0\n","            ReLU-116            [-1, 192, 1, 1]               0\n","          Conv2d-117            [-1, 768, 1, 1]         148,224\n","         Sigmoid-118            [-1, 768, 1, 1]               0\n","        SEModule-119          [-1, 768, 28, 28]               0\n","        Identity-120          [-1, 768, 28, 28]               0\n","            SiLU-121          [-1, 768, 28, 28]               0\n","  BatchNormAct2d-122          [-1, 768, 28, 28]           1,536\n","          Conv2d-123          [-1, 192, 28, 28]         147,456\n","        Identity-124          [-1, 192, 28, 28]               0\n","     MbConvBlock-125          [-1, 192, 28, 28]               0\n","    MaxxVitStage-126          [-1, 192, 28, 28]               0\n","       AvgPool2d-127          [-1, 192, 14, 14]               0\n","          Conv2d-128          [-1, 384, 14, 14]          73,728\n","    Downsample2d-129          [-1, 384, 14, 14]               0\n","     LayerNorm2d-130          [-1, 192, 28, 28]             384\n","       AvgPool2d-131          [-1, 192, 14, 14]               0\n","        Identity-132          [-1, 192, 14, 14]               0\n","    Downsample2d-133          [-1, 192, 14, 14]               0\n","          Conv2d-134          [-1, 576, 14, 14]         111,168\n","          Conv2d-135          [-1, 384, 14, 14]          74,112\n","         Dropout-136          [-1, 384, 14, 14]               0\n","     Attention2d-137          [-1, 384, 14, 14]               0\n","        Identity-138          [-1, 384, 14, 14]               0\n","        Identity-139          [-1, 384, 14, 14]               0\n","     LayerNorm2d-140          [-1, 384, 14, 14]             768\n","          Conv2d-141         [-1, 1536, 14, 14]         591,360\n","        Identity-142         [-1, 1536, 14, 14]               0\n","            GELU-143         [-1, 1536, 14, 14]               0\n","         Dropout-144         [-1, 1536, 14, 14]               0\n","          Conv2d-145          [-1, 384, 14, 14]         590,208\n","         ConvMlp-146          [-1, 384, 14, 14]               0\n","        Identity-147          [-1, 384, 14, 14]               0\n","        Identity-148          [-1, 384, 14, 14]               0\n","TransformerBlock2d-149          [-1, 384, 14, 14]               0\n","        Identity-150          [-1, 384, 14, 14]               0\n","     LayerNorm2d-151          [-1, 384, 14, 14]             768\n","          Conv2d-152         [-1, 1152, 14, 14]         443,520\n","          Conv2d-153          [-1, 384, 14, 14]         147,840\n","         Dropout-154          [-1, 384, 14, 14]               0\n","     Attention2d-155          [-1, 384, 14, 14]               0\n","        Identity-156          [-1, 384, 14, 14]               0\n","        Identity-157          [-1, 384, 14, 14]               0\n","     LayerNorm2d-158          [-1, 384, 14, 14]             768\n","          Conv2d-159         [-1, 1536, 14, 14]         591,360\n","        Identity-160         [-1, 1536, 14, 14]               0\n","            GELU-161         [-1, 1536, 14, 14]               0\n","         Dropout-162         [-1, 1536, 14, 14]               0\n","          Conv2d-163          [-1, 384, 14, 14]         590,208\n","         ConvMlp-164          [-1, 384, 14, 14]               0\n","        Identity-165          [-1, 384, 14, 14]               0\n","        Identity-166          [-1, 384, 14, 14]               0\n","TransformerBlock2d-167          [-1, 384, 14, 14]               0\n","        Identity-168          [-1, 384, 14, 14]               0\n","     LayerNorm2d-169          [-1, 384, 14, 14]             768\n","          Conv2d-170         [-1, 1152, 14, 14]         443,520\n","          Conv2d-171          [-1, 384, 14, 14]         147,840\n","         Dropout-172          [-1, 384, 14, 14]               0\n","     Attention2d-173          [-1, 384, 14, 14]               0\n","        Identity-174          [-1, 384, 14, 14]               0\n","        Identity-175          [-1, 384, 14, 14]               0\n","     LayerNorm2d-176          [-1, 384, 14, 14]             768\n","          Conv2d-177         [-1, 1536, 14, 14]         591,360\n","        Identity-178         [-1, 1536, 14, 14]               0\n","            GELU-179         [-1, 1536, 14, 14]               0\n","         Dropout-180         [-1, 1536, 14, 14]               0\n","          Conv2d-181          [-1, 384, 14, 14]         590,208\n","         ConvMlp-182          [-1, 384, 14, 14]               0\n","        Identity-183          [-1, 384, 14, 14]               0\n","        Identity-184          [-1, 384, 14, 14]               0\n","TransformerBlock2d-185          [-1, 384, 14, 14]               0\n","        Identity-186          [-1, 384, 14, 14]               0\n","     LayerNorm2d-187          [-1, 384, 14, 14]             768\n","          Conv2d-188         [-1, 1152, 14, 14]         443,520\n","          Conv2d-189          [-1, 384, 14, 14]         147,840\n","         Dropout-190          [-1, 384, 14, 14]               0\n","     Attention2d-191          [-1, 384, 14, 14]               0\n","        Identity-192          [-1, 384, 14, 14]               0\n","        Identity-193          [-1, 384, 14, 14]               0\n","     LayerNorm2d-194          [-1, 384, 14, 14]             768\n","          Conv2d-195         [-1, 1536, 14, 14]         591,360\n","        Identity-196         [-1, 1536, 14, 14]               0\n","            GELU-197         [-1, 1536, 14, 14]               0\n","         Dropout-198         [-1, 1536, 14, 14]               0\n","          Conv2d-199          [-1, 384, 14, 14]         590,208\n","         ConvMlp-200          [-1, 384, 14, 14]               0\n","        Identity-201          [-1, 384, 14, 14]               0\n","        Identity-202          [-1, 384, 14, 14]               0\n","TransformerBlock2d-203          [-1, 384, 14, 14]               0\n","        Identity-204          [-1, 384, 14, 14]               0\n","     LayerNorm2d-205          [-1, 384, 14, 14]             768\n","          Conv2d-206         [-1, 1152, 14, 14]         443,520\n","          Conv2d-207          [-1, 384, 14, 14]         147,840\n","         Dropout-208          [-1, 384, 14, 14]               0\n","     Attention2d-209          [-1, 384, 14, 14]               0\n","        Identity-210          [-1, 384, 14, 14]               0\n","        Identity-211          [-1, 384, 14, 14]               0\n","     LayerNorm2d-212          [-1, 384, 14, 14]             768\n","          Conv2d-213         [-1, 1536, 14, 14]         591,360\n","        Identity-214         [-1, 1536, 14, 14]               0\n","            GELU-215         [-1, 1536, 14, 14]               0\n","         Dropout-216         [-1, 1536, 14, 14]               0\n","          Conv2d-217          [-1, 384, 14, 14]         590,208\n","         ConvMlp-218          [-1, 384, 14, 14]               0\n","        Identity-219          [-1, 384, 14, 14]               0\n","        Identity-220          [-1, 384, 14, 14]               0\n","TransformerBlock2d-221          [-1, 384, 14, 14]               0\n","        Identity-222          [-1, 384, 14, 14]               0\n","     LayerNorm2d-223          [-1, 384, 14, 14]             768\n","          Conv2d-224         [-1, 1152, 14, 14]         443,520\n","          Conv2d-225          [-1, 384, 14, 14]         147,840\n","         Dropout-226          [-1, 384, 14, 14]               0\n","     Attention2d-227          [-1, 384, 14, 14]               0\n","        Identity-228          [-1, 384, 14, 14]               0\n","        Identity-229          [-1, 384, 14, 14]               0\n","     LayerNorm2d-230          [-1, 384, 14, 14]             768\n","          Conv2d-231         [-1, 1536, 14, 14]         591,360\n","        Identity-232         [-1, 1536, 14, 14]               0\n","            GELU-233         [-1, 1536, 14, 14]               0\n","         Dropout-234         [-1, 1536, 14, 14]               0\n","          Conv2d-235          [-1, 384, 14, 14]         590,208\n","         ConvMlp-236          [-1, 384, 14, 14]               0\n","        Identity-237          [-1, 384, 14, 14]               0\n","        Identity-238          [-1, 384, 14, 14]               0\n","TransformerBlock2d-239          [-1, 384, 14, 14]               0\n","        Identity-240          [-1, 384, 14, 14]               0\n","     LayerNorm2d-241          [-1, 384, 14, 14]             768\n","          Conv2d-242         [-1, 1152, 14, 14]         443,520\n","          Conv2d-243          [-1, 384, 14, 14]         147,840\n","         Dropout-244          [-1, 384, 14, 14]               0\n","     Attention2d-245          [-1, 384, 14, 14]               0\n","        Identity-246          [-1, 384, 14, 14]               0\n","        Identity-247          [-1, 384, 14, 14]               0\n","     LayerNorm2d-248          [-1, 384, 14, 14]             768\n","          Conv2d-249         [-1, 1536, 14, 14]         591,360\n","        Identity-250         [-1, 1536, 14, 14]               0\n","            GELU-251         [-1, 1536, 14, 14]               0\n","         Dropout-252         [-1, 1536, 14, 14]               0\n","          Conv2d-253          [-1, 384, 14, 14]         590,208\n","         ConvMlp-254          [-1, 384, 14, 14]               0\n","        Identity-255          [-1, 384, 14, 14]               0\n","        Identity-256          [-1, 384, 14, 14]               0\n","TransformerBlock2d-257          [-1, 384, 14, 14]               0\n","    MaxxVitStage-258          [-1, 384, 14, 14]               0\n","       AvgPool2d-259            [-1, 384, 7, 7]               0\n","          Conv2d-260            [-1, 768, 7, 7]         294,912\n","    Downsample2d-261            [-1, 768, 7, 7]               0\n","     LayerNorm2d-262          [-1, 384, 14, 14]             768\n","       AvgPool2d-263            [-1, 384, 7, 7]               0\n","        Identity-264            [-1, 384, 7, 7]               0\n","    Downsample2d-265            [-1, 384, 7, 7]               0\n","          Conv2d-266           [-1, 1152, 7, 7]         443,520\n","          Conv2d-267            [-1, 768, 7, 7]         295,680\n","         Dropout-268            [-1, 768, 7, 7]               0\n","     Attention2d-269            [-1, 768, 7, 7]               0\n","        Identity-270            [-1, 768, 7, 7]               0\n","        Identity-271            [-1, 768, 7, 7]               0\n","     LayerNorm2d-272            [-1, 768, 7, 7]           1,536\n","          Conv2d-273           [-1, 3072, 7, 7]       2,362,368\n","        Identity-274           [-1, 3072, 7, 7]               0\n","            GELU-275           [-1, 3072, 7, 7]               0\n","         Dropout-276           [-1, 3072, 7, 7]               0\n","          Conv2d-277            [-1, 768, 7, 7]       2,360,064\n","         ConvMlp-278            [-1, 768, 7, 7]               0\n","        Identity-279            [-1, 768, 7, 7]               0\n","        Identity-280            [-1, 768, 7, 7]               0\n","TransformerBlock2d-281            [-1, 768, 7, 7]               0\n","        Identity-282            [-1, 768, 7, 7]               0\n","     LayerNorm2d-283            [-1, 768, 7, 7]           1,536\n","          Conv2d-284           [-1, 2304, 7, 7]       1,771,776\n","          Conv2d-285            [-1, 768, 7, 7]         590,592\n","         Dropout-286            [-1, 768, 7, 7]               0\n","     Attention2d-287            [-1, 768, 7, 7]               0\n","        Identity-288            [-1, 768, 7, 7]               0\n","        Identity-289            [-1, 768, 7, 7]               0\n","     LayerNorm2d-290            [-1, 768, 7, 7]           1,536\n","          Conv2d-291           [-1, 3072, 7, 7]       2,362,368\n","        Identity-292           [-1, 3072, 7, 7]               0\n","            GELU-293           [-1, 3072, 7, 7]               0\n","         Dropout-294           [-1, 3072, 7, 7]               0\n","          Conv2d-295            [-1, 768, 7, 7]       2,360,064\n","         ConvMlp-296            [-1, 768, 7, 7]               0\n","        Identity-297            [-1, 768, 7, 7]               0\n","        Identity-298            [-1, 768, 7, 7]               0\n","TransformerBlock2d-299            [-1, 768, 7, 7]               0\n","    MaxxVitStage-300            [-1, 768, 7, 7]               0\n","     LayerNorm2d-301            [-1, 768, 7, 7]           1,536\n","AdaptiveAvgPool2d-302            [-1, 768, 1, 1]               0\n","         Flatten-303                  [-1, 768]               0\n","SelectAdaptivePool2d-304                  [-1, 768]               0\n","         Dropout-305                  [-1, 768]               0\n","          Linear-306                   [-1, 40]          30,760\n","        Identity-307                   [-1, 40]               0\n","  ClassifierHead-308                   [-1, 40]               0\n","================================================================\n","Total params: 26,634,376\n","Trainable params: 26,634,376\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 507.16\n","Params size (MB): 101.60\n","Estimated Total Size (MB): 609.34\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Fixed PyTorch training loop cell ‚Äî paste & run (replaces the broken block)\n","import os, math, traceback\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch, torch.nn as nn, torch.optim as optim\n","\n","# assume pytorch_factories, train_loader_pt, val_loader_pt, test_loader_pt, num_classes, OUT_ROOT, DEVICE, EPOCHS_PY, LR_PY, WEIGHT_DECAY exist\n","\n","def freeze_model_fraction(model, fraction=0.65):\n","    params = list(model.parameters())\n","    total = len(params)\n","    freeze_count = int(total * fraction)\n","    freeze_count = min(freeze_count, max(0, total-1))\n","    for i,p in enumerate(params):\n","        p.requires_grad = False if i < freeze_count else True\n","\n","def unfreeze_last_n_params(model, n=4):\n","    params = list(model.parameters())\n","    total = len(params)\n","    for i in range(max(0, total - n), total):\n","        params[i].requires_grad = True\n","\n","def train_and_evaluate_pytorch(factory, name, epochs=EPOCHS_PY, lr=LR_PY, out_dir=OUT_ROOT):\n","    model = factory(num_classes).to(DEVICE)\n","    # safe freeze\n","    freeze_model_fraction(model, fraction=0.65)\n","    trainable = [p for p in model.parameters() if p.requires_grad]\n","    if len(trainable) == 0:\n","        unfreeze_last_n_params(model, n=8)\n","        trainable = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = optim.AdamW(trainable, lr=lr, weight_decay=WEIGHT_DECAY)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    best_val = -1.0\n","    history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n","    last_ckpt = os.path.join(out_dir, f'{name}_last.pth')\n","    best_ckpt = os.path.join(out_dir, f'{name}_best.pth')\n","\n","    try:\n","        for ep in range(epochs):\n","            model.train()\n","            run_loss=0.0; correct=0; total=0\n","            for imgs, labels, _p in train_loader_pt:\n","                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n","                optimizer.zero_grad()\n","                out = model(imgs)\n","                loss = criterion(out, labels)\n","                loss.backward(); optimizer.step()\n","                run_loss += float(loss.item()) * imgs.size(0)\n","                preds = out.argmax(dim=1)\n","                correct += (preds==labels).sum().item()\n","                total += labels.size(0)\n","            train_loss = run_loss/total if total>0 else 0.0\n","            train_acc = correct/total if total>0 else 0.0\n","\n","            # validation\n","            model.eval()\n","            vloss=0.0; vcorrect=0; vtotal=0\n","            with torch.no_grad():\n","                for imgs, labels, _p in val_loader_pt:\n","                    imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n","                    out = model(imgs)\n","                    loss = criterion(out, labels)\n","                    vloss += float(loss.item()) * imgs.size(0)\n","                    preds = out.argmax(dim=1)\n","                    vcorrect += (preds==labels).sum().item()\n","                    vtotal += labels.size(0)\n","            val_loss = vloss/vtotal if vtotal>0 else 0.0\n","            val_acc = vcorrect/vtotal if vtotal>0 else 0.0\n","\n","            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n","            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n","\n","            print(f'{name} Ep{ep+1}/{epochs} train_acc={train_acc:.4f} val_acc={val_acc:.4f}')\n","\n","            # checkpoint last\n","            torch.save(model.state_dict(), last_ckpt)\n","            # checkpoint best\n","            if val_acc > best_val:\n","                best_val = val_acc\n","                torch.save(model.state_dict(), best_ckpt)\n","        # end epochs\n","    except Exception as e:\n","        print(f\"Error during training {name}: {e}\")\n","        traceback.print_exc()\n","    # Ensure we have a checkpoint: prefer best, else last, else current state\n","    ckpt_to_load = best_ckpt if os.path.exists(best_ckpt) else (last_ckpt if os.path.exists(last_ckpt) else None)\n","    if ckpt_to_load is not None:\n","        try:\n","            model.load_state_dict(torch.load(ckpt_to_load, map_location=DEVICE))\n","        except Exception:\n","            # try non-strict load\n","            model.load_state_dict(torch.load(ckpt_to_load, map_location=DEVICE), strict=False)\n","    else:\n","        print(f\"Warning: No checkpoint saved for {name}; using current in-memory model for testing.\")\n","\n","    # test\n","    y_true = []; y_pred = []\n","    model.eval()\n","    with torch.no_grad():\n","        for imgs, labels, _p in test_loader_pt:\n","            imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)\n","            out = model(imgs)\n","            preds = out.argmax(dim=1).cpu().numpy()\n","            y_pred.extend(preds.tolist()); y_true.extend(labels.cpu().numpy().tolist())\n","\n","    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n","    cm = confusion_matrix(y_true, y_pred)\n","    # save artifacts\n","    pd.DataFrame(rep).transpose().to_csv(os.path.join(out_dir, f'{name}_classification_report.csv'))\n","    np.save(os.path.join(out_dir, f'{name}_cm.npy'), cm)\n","    # history plot (safe: may be empty)\n","    try:\n","        plt.figure(figsize=(10,4))\n","        plt.subplot(1,2,1)\n","        plt.plot(history['train_loss'], label='train_loss'); plt.plot(history['val_loss'], label='val_loss'); plt.legend()\n","        plt.subplot(1,2,2)\n","        plt.plot(history['train_acc'], label='train_acc'); plt.plot(history['val_acc'], label='val_acc'); plt.legend()\n","        plt.suptitle(name)\n","        plt.tight_layout(); plt.savefig(os.path.join(out_dir, f'{name}_history.png')); plt.close()\n","    except Exception:\n","        pass\n","\n","    return history, rep, cm\n","\n","# Initialize results dict (must be outside function)\n","pytorch_results = {}\n","\n","# Train each pytorch factory and store results safely\n","for name, fac in pytorch_factories.items():\n","    try:\n","        print(\"Training PyTorch model:\", name)\n","        hist, rep, cm = train_and_evaluate_pytorch(fac, name, epochs=EPOCHS_PY, lr=LR_PY, out_dir=OUT_ROOT)\n","        pytorch_results[name] = {'history': hist, 'report': rep, 'cm': cm}\n","    except Exception as e:\n","        # catch-all ensures loop continues for other models\n","        print(\"Error training\", name, \"->\", e)\n","        traceback.print_exc()\n","        pytorch_results[name] = {'error': str(e)}\n","\n","print(\"PyTorch training done. Artifacts in\", OUT_ROOT)\n","print(\"Summary of PyTorch results keys:\", list(pytorch_results.keys()))\n"],"metadata":{"id":"EtXsjdKIm4IF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763833673248,"user_tz":-330,"elapsed":1992322,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"3f14e771-b622-407a-a56a-958cebd567a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training PyTorch model: swin_tiny\n","swin_tiny Ep1/12 train_acc=0.3944 val_acc=0.7542\n","swin_tiny Ep2/12 train_acc=0.8665 val_acc=0.9411\n","swin_tiny Ep3/12 train_acc=0.9363 val_acc=0.9596\n","swin_tiny Ep4/12 train_acc=0.9607 val_acc=0.9579\n","swin_tiny Ep5/12 train_acc=0.9722 val_acc=0.9764\n","swin_tiny Ep6/12 train_acc=0.9838 val_acc=0.9815\n","swin_tiny Ep7/12 train_acc=0.9886 val_acc=0.9798\n","swin_tiny Ep8/12 train_acc=0.9851 val_acc=0.9798\n","swin_tiny Ep9/12 train_acc=0.9893 val_acc=0.9865\n","swin_tiny Ep10/12 train_acc=0.9895 val_acc=0.9848\n","swin_tiny Ep11/12 train_acc=0.9914 val_acc=0.9781\n","swin_tiny Ep12/12 train_acc=0.9950 val_acc=0.9832\n","Training PyTorch model: deit_small_distilled\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["deit_small_distilled Ep1/12 train_acc=0.2727 val_acc=0.5842\n","deit_small_distilled Ep2/12 train_acc=0.7498 val_acc=0.8249\n","deit_small_distilled Ep3/12 train_acc=0.8865 val_acc=0.9007\n","deit_small_distilled Ep4/12 train_acc=0.9361 val_acc=0.9327\n","deit_small_distilled Ep5/12 train_acc=0.9651 val_acc=0.9461\n","deit_small_distilled Ep6/12 train_acc=0.9765 val_acc=0.9529\n","deit_small_distilled Ep7/12 train_acc=0.9821 val_acc=0.9596\n","deit_small_distilled Ep8/12 train_acc=0.9857 val_acc=0.9613\n","deit_small_distilled Ep9/12 train_acc=0.9907 val_acc=0.9613\n","deit_small_distilled Ep10/12 train_acc=0.9910 val_acc=0.9630\n","deit_small_distilled Ep11/12 train_acc=0.9939 val_acc=0.9663\n","deit_small_distilled Ep12/12 train_acc=0.9947 val_acc=0.9613\n","Training PyTorch model: coatnet_0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["coatnet_0 Ep1/12 train_acc=0.5118 val_acc=0.8805\n","coatnet_0 Ep2/12 train_acc=0.9327 val_acc=0.9343\n","coatnet_0 Ep3/12 train_acc=0.9725 val_acc=0.9495\n","coatnet_0 Ep4/12 train_acc=0.9819 val_acc=0.9512\n","coatnet_0 Ep5/12 train_acc=0.9897 val_acc=0.9714\n","coatnet_0 Ep6/12 train_acc=0.9910 val_acc=0.9798\n","coatnet_0 Ep7/12 train_acc=0.9931 val_acc=0.9815\n","coatnet_0 Ep8/12 train_acc=0.9929 val_acc=0.9697\n","coatnet_0 Ep9/12 train_acc=0.9933 val_acc=0.9747\n","coatnet_0 Ep10/12 train_acc=0.9945 val_acc=0.9798\n","coatnet_0 Ep11/12 train_acc=0.9947 val_acc=0.9781\n","coatnet_0 Ep12/12 train_acc=0.9947 val_acc=0.9815\n","PyTorch training done. Artifacts in /content/experiments\n","Summary of PyTorch results keys: ['swin_tiny', 'deit_small_distilled', 'coatnet_0']\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# FULL: INTEGRATED FEW-SHOT + HYBRID FUSION PIPELINE (READY-TO-PASTE)\n","# - Robust _make_embedding_extractor and _embed_batch (probe backbone outputs)\n","# - Handles varied backbone output shapes (spatial / token / pooled)\n","# - Prints inferred feature dims for debugging\n","# - Marks new/updated sections with comments\n","# ============================================================\n","\n","### Imports\n","import os, random, math, time, warnings\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt, seaborn as sns\n","from collections import defaultdict, Counter\n","from tqdm import tqdm\n","from PIL import Image\n","\n","# --- Configurable params (change if needed) ---\n","FEWSHOT_EPISODES = 200        # increase to 1000+ for final results\n","FUSION_EMBED_DIM = 256       # per-backbone embedding size\n","FUSION_BACKBONES = ['swin_tiny','deit_small_distilled','coatnet_0']  # must be keys in pytorch_factories\n","SAVE_OUTDIR = globals().get('OUT_ROOT', '/content/experiments')\n","PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(SAVE_OUTDIR, 'plots'))\n","os.makedirs(PLOTS_DIR, exist_ok=True)\n","\n","# Required globals check\n","required_globals = ['pytorch_factories','train_items','val_items','test_items','num_classes','DEVICE']\n","missing = [g for g in required_globals if g not in globals()]\n","if missing:\n","    raise RuntimeError(f\"Missing required globals in notebook environment: {missing}. Run earlier cells first.\")\n","\n","# --------------------------\n","# Robust fallback: _make_embedding_extractor (probes backbone to infer features)\n","# --------------------------\n","if 'make_embedding_extractor' in globals():\n","    _make_embedding_extractor = make_embedding_extractor\n","else:\n","    import torch, torch.nn as nn, torch.nn.functional as F\n","    from torchvision import transforms\n","\n","    def _make_embedding_extractor(factory, embed_dim=FUSION_EMBED_DIM, img_size=(224,224)):\n","        \"\"\"\n","        Create (backbone, proj) where proj matches the backbone's actual output feature dimension.\n","        Probes the model with a dummy input to infer shape.\n","        \"\"\"\n","        model = factory(num_classes)\n","        try:\n","            if hasattr(model, \"reset_classifier\"):\n","                model.reset_classifier(0)\n","        except Exception:\n","            pass\n","\n","        model = model.to(DEVICE)\n","        model.eval()\n","\n","        with torch.no_grad():\n","            dummy = torch.zeros(1, 3, img_size[0], img_size[1], device=DEVICE)\n","            try:\n","                feats = model.forward_features(dummy) if hasattr(model, 'forward_features') else model(dummy)\n","            except Exception:\n","                # last resort: try normal forward and accept result\n","                feats = model(dummy)\n","\n","        # Deduce feature vector dim and create projection head accordingly\n","        if torch.is_tensor(feats):\n","            if feats.ndim == 4:\n","                feat_dim = feats.shape[1]\n","                proj = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n","            elif feats.ndim == 3:\n","                feat_dim = feats.shape[2]\n","                proj = nn.Sequential(nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n","            elif feats.ndim == 2:\n","                feat_dim = feats.shape[1]\n","                proj = nn.Sequential(nn.Linear(feat_dim, embed_dim))\n","            else:\n","                feat_dim = embed_dim\n","                proj = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n","        else:\n","            # non-tensor fallback\n","            feat_dim = embed_dim\n","            proj = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(feat_dim, embed_dim))\n","\n","        proj = proj.to(DEVICE)\n","        for p in model.parameters(): p.requires_grad = False\n","        for p in proj.parameters(): p.requires_grad = True\n","\n","        # attach attribute for later inspection\n","        try:\n","            model._inferred_feat_dim = int(feat_dim)\n","        except Exception:\n","            model._inferred_feat_dim = None\n","        return model, proj\n","\n","# --------------------------\n","# Robust fallback: _embed_batch (handles 4D/3D/2D features)\n","# --------------------------\n","if 'embed_batch' in globals():\n","    _embed_batch = embed_batch\n","else:\n","    import torch, torch.nn.functional as F\n","    from torchvision import transforms\n","\n","    def _embed_batch(backbone, proj, paths, img_size=(224,224), batch_size=16):\n","        backbone.eval()\n","        if proj is not None:\n","            proj.eval()\n","\n","        tfm = transforms.Compose([\n","            transforms.Resize(img_size),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","        ])\n","        device = DEVICE\n","        embeds = []\n","        with torch.no_grad():\n","            for i in range(0, len(paths), batch_size):\n","                batch_paths = paths[i:i+batch_size]\n","                imgs = []\n","                for p in batch_paths:\n","                    im = Image.open(p).convert('RGB')\n","                    imgs.append(tfm(im))\n","                x = torch.stack(imgs, dim=0).to(device)\n","\n","                # get features\n","                if hasattr(backbone, 'forward_features'):\n","                    feats = backbone.forward_features(x)\n","                else:\n","                    try:\n","                        feats = backbone(x)\n","                    except Exception:\n","                        feats = backbone(x)\n","\n","                # handle shapes\n","                if feats.ndim == 4:\n","                    # spatial map (B, C, H, W)\n","                    # If proj starts with Linear, pool first; else let proj handle pooling\n","                    try:\n","                        first_mod = next(iter(proj)) if isinstance(proj, torch.nn.Sequential) else None\n","                    except Exception:\n","                        first_mod = None\n","\n","                    if isinstance(first_mod, nn.Linear):\n","                        vec = F.adaptive_avg_pool2d(feats, 1).view(feats.shape[0], -1)\n","                        emb = proj(vec)\n","                    else:\n","                        emb = proj(feats)\n","                elif feats.ndim == 3:\n","                    # token sequence (B, N, D) -> average tokens\n","                    vec = feats.mean(dim=1)\n","                    # try to use proj directly; if incompatible, do a quick linear mapping\n","                    try:\n","                        emb = proj(vec)\n","                    except Exception:\n","                        tmp_lin = nn.Linear(vec.shape[1], FUSION_EMBED_DIM).to(device)\n","                        emb = tmp_lin(vec)\n","                elif feats.ndim == 2:\n","                    vec = feats\n","                    emb = proj(vec) if proj is not None else vec\n","                else:\n","                    vec = feats.view(feats.shape[0], -1)\n","                    emb = proj(vec) if proj is not None else vec\n","\n","                emb = F.normalize(emb, dim=1)\n","                embeds.append(emb.cpu().numpy())\n","\n","        return np.vstack(embeds)\n","\n","# --------------------------\n","# compute_prototypes fallback\n","# --------------------------\n","if 'compute_prototypes' in globals():\n","    _compute_prototypes = compute_prototypes\n","else:\n","    def _compute_prototypes(support_emb, support_labels, N_way):\n","        D = support_emb.shape[1]\n","        prot = np.zeros((N_way, D), dtype=np.float32)\n","        for c in range(N_way):\n","            idxs = [i for i,l in enumerate(support_labels) if l==c]\n","            if len(idxs) == 0:\n","                continue\n","            prot[c] = support_emb[idxs].mean(axis=0)\n","        prot /= (np.linalg.norm(prot, axis=1, keepdims=True) + 1e-9)\n","        return prot\n","\n","# --------------------------\n","# FewShotDataset (robust): reduces N_way when insufficient classes; samples with replacement for small classes\n","# --------------------------\n","if 'FewShotDataset' in globals():\n","    _FewShotDataset = FewShotDataset\n","else:\n","    class _FewShotDataset:\n","        def __init__(self, items, img_size=(224,224)):\n","            self.items = items\n","            self.by_class = defaultdict(list)\n","            for p,l in items:\n","                self.by_class[l].append(p)\n","            self.classes = sorted(self.by_class.keys())\n","\n","        def sample_episode(self, N_way=5, K_shot=5, Q_query=5):\n","            avail_classes = len(self.classes)\n","            if avail_classes == 0:\n","                raise ValueError(\"FewShotDataset has no classes (items list empty).\")\n","            if avail_classes < N_way:\n","                warnings.warn(f\"Requested N_way={N_way} but only {avail_classes} classes available. Reducing N_way -> {avail_classes}.\", UserWarning)\n","                N_way = avail_classes\n","            chosen = random.sample(self.classes, N_way)\n","            support=[]; s_lbl=[]; query=[]; q_lbl=[]\n","            for i,cls in enumerate(chosen):\n","                imgs = self.by_class[cls]\n","                required = K_shot + Q_query\n","                if len(imgs) >= required:\n","                    sel = random.sample(imgs, required)\n","                else:\n","                    warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","                    sel = [random.choice(imgs) for _ in range(required)]\n","                support += sel[:K_shot]; query += sel[K_shot:]\n","                s_lbl += [i]*K_shot; q_lbl += [i]*Q_query\n","            return support, s_lbl, query, q_lbl\n","\n","# choose dataset object\n","if 'fs_val' in globals():\n","    _fs_val = fs_val\n","else:\n","    _fs_val = _FewShotDataset(val_items, img_size=globals().get('IMG_SIZE',(224,224)))\n","\n","# quick stats\n","try:\n","    per_class_counts = {cls: len(imgs) for cls, imgs in _fs_val.by_class.items()}\n","    print(\"Validation set: total images =\", len(val_items), \"; classes =\", len(per_class_counts))\n","    small = sorted(per_class_counts.items(), key=lambda x: x[1])[:6]\n","    print(\"Few smallest class counts (class:count):\", small)\n","except Exception as e:\n","    print(\"Could not print fs_val stats:\", e)\n","\n","# --------------------------\n","# 1) Base few-shot evaluation (prototypical)\n","# --------------------------\n","if 'accs_5shot' not in globals() or 'accs_1shot' not in globals():\n","    print(\"Running base few-shot evaluation (prototypical) ...\")\n","\n","    def _evaluate_episode_base(fs_dataset, N_way=5, K_shot=5, Q_query=10):\n","        support, s_lbl, query, q_lbl = fs_dataset.sample_episode(N_way,K_shot,Q_query)\n","        # pick backbone\n","        base_backbone_name = FUSION_BACKBONES[0] if FUSION_BACKBONES[0] in pytorch_factories else list(pytorch_factories.keys())[0]\n","        bb_fac = pytorch_factories[base_backbone_name]\n","        bb, ph = _make_embedding_extractor(bb_fac, embed_dim=FUSION_EMBED_DIM)\n","        # print inferred feat dim\n","        try:\n","            print(f\"Base backbone '{base_backbone_name}' inferred feat dim:\", getattr(bb, '_inferred_feat_dim', None))\n","        except Exception:\n","            pass\n","        s_emb = _embed_batch(bb, ph, support)\n","        q_emb = _embed_batch(bb, ph, query)\n","        proto = _compute_prototypes(s_emb, s_lbl, N_way)\n","        logits = q_emb @ proto.T\n","        preds = np.argmax(logits, axis=1)\n","        return (preds == np.array(q_lbl)).mean()\n","\n","    accs_5shot = []\n","    for _ in tqdm(range(FEWSHOT_EPISODES), desc='base 5-shot'):\n","        accs_5shot.append(_evaluate_episode_base(_fs_val, N_way=5, K_shot=5, Q_query=10))\n","    accs_1shot = []\n","    for _ in tqdm(range(FEWSHOT_EPISODES), desc='base 1-shot'):\n","        accs_1shot.append(_evaluate_episode_base(_fs_val, N_way=5, K_shot=1, Q_query=10))\n","    print(f\"Base 5-shot mean: {np.mean(accs_5shot)*100:.2f}%  |  Base 1-shot mean: {np.mean(accs_1shot)*100:.2f}%\")\n","else:\n","    print(\"Base few-shot results found; skipping base evaluation.\")\n","\n","# --------------------------\n","# 2) Build hybrid fusion extractors (HFPN)\n","# --------------------------\n","fusion_extractors = {}\n","for bk in FUSION_BACKBONES:\n","    if bk not in pytorch_factories:\n","        raise RuntimeError(f\"Requested fusion backbone '{bk}' not found in pytorch_factories.\")\n","    fac = pytorch_factories[bk]\n","    bb, ph = _make_embedding_extractor(fac, embed_dim=FUSION_EMBED_DIM)\n","    bb.eval(); ph.eval()\n","    fusion_extractors[bk] = (bb, ph)\n","    print(f\"Backbone '{bk}' inferred feat dim:\", getattr(bb, '_inferred_feat_dim', None))\n","\n","print(\"Built hybrid-fusion extractors for:\", list(fusion_extractors.keys()))\n","\n","def _embed_fusion(paths):\n","    parts = []\n","    for bk in FUSION_BACKBONES:\n","        bb, ph = fusion_extractors[bk]\n","        emb = _embed_batch(bb, ph, paths)\n","        parts.append(emb)\n","    fused = np.concatenate(parts, axis=1)\n","    fused = fused / (np.linalg.norm(fused, axis=1, keepdims=True) + 1e-9)\n","    return fused\n","\n","# --------------------------\n","# 3) Optional proj-head finetune (short)\n","# --------------------------\n","DO_FINETUNE_PROJ = False\n","if DO_FINETUNE_PROJ:\n","    print(\"Fine-tuning projection heads (small supervised head training on train_items).\")\n","    import torch, torch.nn as nn, torch.optim as optim\n","    from torchvision import transforms\n","    tf = transforms.Compose([transforms.Resize(globals().get('IMG_SIZE',(224,224))), transforms.ToTensor(),\n","                             transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","    small_train = train_items[:min(500, len(train_items))]\n","    X_paths = [p for p,_ in small_train]; Y = [lbl for _,lbl in small_train]\n","    class _SmallDS(torch.utils.data.Dataset):\n","        def __init__(self, paths, labels): self.paths=paths; self.labels=labels\n","        def __len__(self): return len(self.paths)\n","        def __getitem__(self,idx):\n","            im = Image.open(self.paths[idx]).convert('RGB'); return tf(im), int(self.labels[idx])\n","    ds = _SmallDS(X_paths, Y)\n","    dl = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True, num_workers=2)\n","    for bk,(bb,ph) in fusion_extractors.items():\n","        ph.train()\n","        opt = optim.Adam(filter(lambda p:p.requires_grad, ph.parameters()), lr=1e-3, weight_decay=1e-5)\n","        ce = nn.CrossEntropyLoss()\n","        for epoch in range(2):\n","            for imgs, lbls in dl:\n","                imgs = imgs.to(DEVICE); lbls = lbls.to(DEVICE)\n","                with torch.no_grad():\n","                    feats = bb.forward_features(imgs) if hasattr(bb,'forward_features') else bb(imgs)\n","                out = ph(feats)\n","                logits = nn.Linear(out.shape[1], num_classes).to(DEVICE)(out)\n","                loss = ce(logits, lbls)\n","                opt.zero_grad(); loss.backward(); opt.step()\n","        ph.eval()\n","    print(\"Proj-head fine-tune done.\")\n","\n","# --------------------------\n","# 4) Hybrid Fusion few-shot evaluation\n","# --------------------------\n","if 'accs_fusion_5shot' not in globals() or 'accs_fusion_1shot' not in globals():\n","    print(\"Running Hybrid Fusion few-shot evaluation (HFPN) ...\")\n","    accs_fusion_5shot = []\n","    for _ in tqdm(range(FEWSHOT_EPISODES), desc='fusion 5-shot'):\n","        support, s_lbl, query, q_lbl = _fs_val.sample_episode(5,5,10)\n","        s_emb = _embed_fusion(support)\n","        q_emb = _embed_fusion(query)\n","        prot = _compute_prototypes(s_emb, s_lbl, 5)\n","        logits = q_emb @ prot.T\n","        preds = np.argmax(logits, axis=1)\n","        accs_fusion_5shot.append((preds == np.array(q_lbl)).mean())\n","\n","    accs_fusion_1shot = []\n","    for _ in tqdm(range(FEWSHOT_EPISODES), desc='fusion 1-shot'):\n","        support, s_lbl, query, q_lbl = _fs_val.sample_episode(5,1,10)\n","        s_emb = _embed_fusion(support)\n","        q_emb = _embed_fusion(query)\n","        prot = _compute_prototypes(s_emb, s_lbl, 5)\n","        logits = q_emb @ prot.T\n","        preds = np.argmax(logits, axis=1)\n","        accs_fusion_1shot.append((preds == np.array(q_lbl)).mean())\n","\n","    print(f\"Fusion 5-shot mean: {np.mean(accs_fusion_5shot)*100:.2f}%  |  Fusion 1-shot mean: {np.mean(accs_fusion_1shot)*100:.2f}%\")\n","else:\n","    print(\"Fusion few-shot results found; skipping fusion evaluation.\")\n","\n","# --------------------------\n","# 5) Plotting & Export\n","# --------------------------\n","accs_grouped = {}\n","accs_grouped['base_5shot'] = np.array(accs_5shot)\n","accs_grouped['base_1shot'] = np.array(accs_1shot)\n","if 'accs_fusion_5shot' in globals():\n","    accs_grouped['fusion_5shot'] = np.array(accs_fusion_5shot)\n","    accs_grouped['fusion_1shot'] = np.array(accs_fusion_1shot)\n","\n","rows=[]\n","for k,arr in accs_grouped.items():\n","    rows.append({'setting':k, 'mean_acc':float(np.mean(arr)), 'std_acc':float(np.std(arr,ddof=1) if len(arr)>1 else np.std(arr)),\n","                 'median':float(np.median(arr)), 'n_episodes':int(len(arr))})\n","df_summary = pd.DataFrame(rows)\n","csv_out = os.path.join(SAVE_OUTDIR, 'fewshot_summary_integrated.csv')\n","df_summary.to_csv(csv_out, index=False)\n","print(\"Saved integrated few-shot summary CSV ->\", csv_out)\n","\n","sns.set(style=\"whitegrid\")\n","plt.figure(figsize=(9,5))\n","if 'fusion_5shot' in accs_grouped:\n","    sns.histplot(accs_grouped['base_5shot'], label='base 5-shot', stat='density', kde=True, alpha=0.5)\n","    sns.histplot(accs_grouped['fusion_5shot'], label='fusion 5-shot', stat='density', kde=True, alpha=0.5)\n","    sns.histplot(accs_grouped['base_1shot'], label='base 1-shot', stat='density', kde=True, alpha=0.35)\n","    sns.histplot(accs_grouped['fusion_1shot'], label='fusion 1-shot', stat='density', kde=True, alpha=0.35)\n","else:\n","    sns.histplot(accs_grouped['base_5shot'], label='base 5-shot', stat='density', kde=True, alpha=0.6)\n","    sns.histplot(accs_grouped['base_1shot'], label='base 1-shot', stat='density', kde=True, alpha=0.6)\n","plt.xlabel('Episode Accuracy'); plt.title('Few-Shot Episode Accuracy Distribution (Integrated)')\n","plt.legend()\n","hist_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_histogram.png')\n","plt.tight_layout(); plt.savefig(hist_p); plt.close()\n","print(\"Saved histogram ->\", hist_p)\n","\n","plt.figure(figsize=(7,5))\n","if 'fusion_5shot' in accs_grouped:\n","    labels=['base 5-shot','fusion 5-shot','base 1-shot','fusion 1-shot']\n","    data=[accs_grouped['base_5shot'], accs_grouped['fusion_5shot'], accs_grouped['base_1shot'], accs_grouped['fusion_1shot']]\n","else:\n","    labels=['base 5-shot','base 1-shot']; data=[accs_grouped['base_5shot'], accs_grouped['base_1shot']]\n","sns.boxplot(data=data); plt.xticks(range(len(labels)), labels, rotation=15)\n","plt.ylabel('Episode Accuracy'); plt.title('Few-Shot Accuracy Boxplot (Integrated)')\n","box_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_boxplot.png')\n","plt.tight_layout(); plt.savefig(box_p); plt.close()\n","print(\"Saved boxplot ->\", box_p)\n","\n","plt.figure(figsize=(6,4))\n","if 'fusion_5shot' in accs_grouped:\n","    bars=[accs_grouped['base_5shot'].mean()*100, accs_grouped['fusion_5shot'].mean()*100,\n","          accs_grouped['base_1shot'].mean()*100, accs_grouped['fusion_1shot'].mean()*100]\n","    errs=[accs_grouped['base_5shot'].std(ddof=1)*100 if len(accs_grouped['base_5shot'])>1 else 0,\n","          accs_grouped['fusion_5shot'].std(ddof=1)*100 if len(accs_grouped['fusion_5shot'])>1 else 0,\n","          accs_grouped['base_1shot'].std(ddof=1)*100 if len(accs_grouped['base_1shot'])>1 else 0,\n","          accs_grouped['fusion_1shot'].std(ddof=1)*100 if len(accs_grouped['fusion_1shot'])>1 else 0]\n","    bl=['base 5-shot','fusion 5-shot','base 1-shot','fusion 1-shot']\n","else:\n","    bars=[accs_grouped['base_5shot'].mean()*100, accs_grouped['base_1shot'].mean()*100]\n","    errs=[accs_grouped['base_5shot'].std(ddof=1)*100 if len(accs_grouped['base_5shot'])>1 else 0,\n","          accs_grouped['base_1shot'].std(ddof=1)*100 if len(accs_grouped['base_1shot'])>1 else 0]\n","    bl=['base 5-shot','base 1-shot']\n","x=np.arange(len(bars)); plt.bar(x,bars,yerr=errs,capsize=5); plt.xticks(x,bl,rotation=12)\n","plt.ylabel('Mean Accuracy (%)'); plt.title('Few-Shot Mean Accuracy ¬± Std (Integrated)')\n","for i,v in enumerate(bars): plt.text(i, v + (errs[i]+0.5), f\"{v:.2f}% ¬± {errs[i]:.2f}%\", ha='center', fontsize=9)\n","bar_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_meanstdbar.png')\n","plt.tight_layout(); plt.savefig(bar_p); plt.close()\n","print(\"Saved mean¬±std bar ->\", bar_p)\n","\n","plt.figure(figsize=(10,4))\n","if 'fusion_5shot' in accs_grouped:\n","    plt.plot(np.sort(accs_grouped['base_5shot'])[::-1], label='base 5-shot', alpha=0.8)\n","    plt.plot(np.sort(accs_grouped['fusion_5shot'])[::-1], label='fusion 5-shot', alpha=0.8)\n","    plt.plot(np.sort(accs_grouped['base_1shot'])[::-1], label='base 1-shot', alpha=0.6)\n","    plt.plot(np.sort(accs_grouped['fusion_1shot'])[::-1], label='fusion 1-shot', alpha=0.6)\n","else:\n","    plt.plot(np.sort(accs_grouped['base_5shot'])[::-1], label='base 5-shot', alpha=0.8)\n","    plt.plot(np.sort(accs_grouped['base_1shot'])[::-1], label='base 1-shot', alpha=0.8)\n","plt.xlabel('Episode index (sorted)'); plt.ylabel('Accuracy'); plt.legend(); plt.title('Sorted Episode Accuracies (Integrated)')\n","sorted_p = os.path.join(PLOTS_DIR, 'fewshot_integrated_sortedepisodes.png')\n","plt.tight_layout(); plt.savefig(sorted_p); plt.close()\n","print(\"Saved sorted-episodes plot ->\", sorted_p)\n","\n","print(\"\\nIntegrated few-shot summary (printed):\")\n","print(df_summary.to_string(index=False))\n","\n","combined_csv = os.path.join(SAVE_OUTDIR, 'combined_summary.csv')\n","if os.path.exists(combined_csv):\n","    try:\n","        comb = pd.read_csv(combined_csv)\n","        to_add = []\n","        to_add.append({'model':'fewshot::base_proto','accuracy':float(df_summary.loc[df_summary['setting']=='base_5shot','mean_acc'].values[0]),\n","                       'macro_precision':np.nan,'macro_recall':np.nan,'macro_f1':np.nan})\n","        if 'fusion_5shot' in accs_grouped:\n","            to_add.append({'model':'fewshot::hybrid_fusion','accuracy':float(df_summary.loc[df_summary['setting']=='fusion_5shot','mean_acc'].values[0]),\n","                           'macro_precision':np.nan,'macro_recall':np.nan,'macro_f1':np.nan})\n","        comb = pd.concat([comb, pd.DataFrame(to_add)], ignore_index=True)\n","        comb.to_csv(os.path.join(SAVE_OUTDIR,'combined_summary_with_fewshot_integrated.csv'), index=False)\n","        print(\"Appended few-shot rows to combined_summary -> combined_summary_with_fewshot_integrated.csv\")\n","    except Exception as e:\n","        print(\"Could not append to combined_summary:\", e)\n","\n","print(\"\\nAll integrated few-shot artifacts saved in:\", PLOTS_DIR)\n","# ============================================================\n","# END CELL\n","# ============================================================\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jI8df2C7v5W","executionInfo":{"status":"ok","timestamp":1763837296578,"user_tz":-330,"elapsed":1089739,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"outputId":"256ee06d-2234-4018-a288-651676c6fd28","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation set: total images = 594 ; classes = 40\n","Few smallest class counts (class:count): [(11, 7), (37, 9), (9, 9), (7, 10), (20, 10), (28, 10)]\n","Running base few-shot evaluation (prototypical) ...\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 5 has 12 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 3 has 11 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 21 has 11 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   0%|          | 1/200 [00:02<08:05,  2.44s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 22 has 14 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 19 has 14 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 10 has 13 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   1%|          | 2/200 [00:05<08:46,  2.66s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 6 has 12 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 20 has 10 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   2%|‚ñè         | 3/200 [00:10<12:56,  3.94s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 11 has 7 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 9 has 9 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   2%|‚ñè         | 4/200 [00:13<11:46,  3.60s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 2 has 13 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 33 has 11 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 34 has 12 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   2%|‚ñé         | 5/200 [00:15<09:40,  2.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   3%|‚ñé         | 6/200 [00:16<07:35,  2.35s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 30 has 14 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 7 has 10 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 17 has 13 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   4%|‚ñé         | 7/200 [00:17<06:15,  1.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   4%|‚ñç         | 8/200 [00:19<05:24,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   4%|‚ñç         | 9/200 [00:20<04:50,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   5%|‚ñå         | 10/200 [00:21<04:27,  1.41s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 37 has 9 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   6%|‚ñå         | 11/200 [00:22<04:34,  1.45s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 0 has 13 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   6%|‚ñå         | 12/200 [00:24<04:41,  1.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   6%|‚ñã         | 13/200 [00:25<04:19,  1.39s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 28 has 10 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   7%|‚ñã         | 14/200 [00:26<04:03,  1.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   8%|‚ñä         | 15/200 [00:27<03:52,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   8%|‚ñä         | 16/200 [00:29<03:43,  1.22s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 39 has 12 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n","/tmp/ipython-input-2489329178.py:213: UserWarning: Class 14 has 13 images but required 15. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   8%|‚ñä         | 17/200 [00:30<03:40,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:   9%|‚ñâ         | 18/200 [00:31<03:37,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  10%|‚ñâ         | 19/200 [00:32<03:36,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  10%|‚ñà         | 20/200 [00:33<03:31,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  10%|‚ñà         | 21/200 [00:34<03:34,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  11%|‚ñà         | 22/200 [00:36<03:56,  1.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  12%|‚ñà‚ñè        | 23/200 [00:38<04:04,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  12%|‚ñà‚ñè        | 24/200 [00:39<03:49,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  12%|‚ñà‚ñé        | 25/200 [00:40<03:39,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  13%|‚ñà‚ñé        | 26/200 [00:41<03:32,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  14%|‚ñà‚ñé        | 27/200 [00:42<03:27,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  14%|‚ñà‚ñç        | 28/200 [00:43<03:23,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  14%|‚ñà‚ñç        | 29/200 [00:44<03:20,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  15%|‚ñà‚ñå        | 30/200 [00:46<03:18,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  16%|‚ñà‚ñå        | 31/200 [00:47<03:14,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  16%|‚ñà‚ñå        | 32/200 [00:48<03:20,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  16%|‚ñà‚ñã        | 33/200 [00:50<03:40,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  17%|‚ñà‚ñã        | 34/200 [00:51<03:43,  1.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  18%|‚ñà‚ñä        | 35/200 [00:52<03:31,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  18%|‚ñà‚ñä        | 36/200 [00:53<03:23,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  18%|‚ñà‚ñä        | 37/200 [00:54<03:18,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  19%|‚ñà‚ñâ        | 38/200 [00:56<03:15,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  20%|‚ñà‚ñâ        | 39/200 [00:57<03:11,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  20%|‚ñà‚ñà        | 40/200 [00:58<03:09,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  20%|‚ñà‚ñà        | 41/200 [00:59<03:07,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  21%|‚ñà‚ñà        | 42/200 [01:00<03:03,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  22%|‚ñà‚ñà‚ñè       | 43/200 [01:02<03:21,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  22%|‚ñà‚ñà‚ñè       | 44/200 [01:03<03:37,  1.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  22%|‚ñà‚ñà‚ñé       | 45/200 [01:05<03:27,  1.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  23%|‚ñà‚ñà‚ñé       | 46/200 [01:06<03:16,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  24%|‚ñà‚ñà‚ñé       | 47/200 [01:07<03:09,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  24%|‚ñà‚ñà‚ñç       | 48/200 [01:08<03:02,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  24%|‚ñà‚ñà‚ñç       | 49/200 [01:09<02:57,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  25%|‚ñà‚ñà‚ñå       | 50/200 [01:10<02:55,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  26%|‚ñà‚ñà‚ñå       | 51/200 [01:12<02:54,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  26%|‚ñà‚ñà‚ñå       | 52/200 [01:13<02:52,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  26%|‚ñà‚ñà‚ñã       | 53/200 [01:14<02:52,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  27%|‚ñà‚ñà‚ñã       | 54/200 [01:16<03:11,  1.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  28%|‚ñà‚ñà‚ñä       | 55/200 [01:17<03:21,  1.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  28%|‚ñà‚ñà‚ñä       | 56/200 [01:18<03:09,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  28%|‚ñà‚ñà‚ñä       | 57/200 [01:19<03:00,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  29%|‚ñà‚ñà‚ñâ       | 58/200 [01:21<02:53,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  30%|‚ñà‚ñà‚ñâ       | 59/200 [01:22<02:48,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  30%|‚ñà‚ñà‚ñà       | 60/200 [01:23<02:42,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  30%|‚ñà‚ñà‚ñà       | 61/200 [01:24<02:40,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  31%|‚ñà‚ñà‚ñà       | 62/200 [01:25<02:38,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  32%|‚ñà‚ñà‚ñà‚ñè      | 63/200 [01:26<02:38,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [01:27<02:42,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  32%|‚ñà‚ñà‚ñà‚ñé      | 65/200 [01:29<02:57,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  33%|‚ñà‚ñà‚ñà‚ñé      | 66/200 [01:31<03:04,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  34%|‚ñà‚ñà‚ñà‚ñé      | 67/200 [01:32<02:53,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  34%|‚ñà‚ñà‚ñà‚ñç      | 68/200 [01:33<02:45,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  34%|‚ñà‚ñà‚ñà‚ñç      | 69/200 [01:34<02:38,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [01:35<02:35,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  36%|‚ñà‚ñà‚ñà‚ñå      | 71/200 [01:36<02:32,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [01:37<02:30,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  36%|‚ñà‚ñà‚ñà‚ñã      | 73/200 [01:39<02:28,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [01:40<02:27,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [01:41<02:33,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  38%|‚ñà‚ñà‚ñà‚ñä      | 76/200 [01:43<03:04,  1.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  38%|‚ñà‚ñà‚ñà‚ñä      | 77/200 [01:45<03:22,  1.64s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  39%|‚ñà‚ñà‚ñà‚ñâ      | 78/200 [01:46<03:06,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  40%|‚ñà‚ñà‚ñà‚ñâ      | 79/200 [01:48<02:50,  1.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [01:49<02:39,  1.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  40%|‚ñà‚ñà‚ñà‚ñà      | 81/200 [01:50<02:31,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  41%|‚ñà‚ñà‚ñà‚ñà      | 82/200 [01:51<02:26,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 83/200 [01:52<02:21,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/200 [01:53<02:18,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 85/200 [01:55<02:25,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 86/200 [01:56<02:36,  1.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/200 [01:58<02:34,  1.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [01:59<02:25,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [02:00<02:18,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [02:01<02:14,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [02:02<02:11,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 92/200 [02:04<02:08,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 93/200 [02:05<02:05,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/200 [02:06<02:02,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 95/200 [02:07<02:01,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [02:08<02:13,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 97/200 [02:10<02:21,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 98/200 [02:11<02:15,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [02:12<02:08,  1.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [02:14<02:03,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 101/200 [02:15<01:59,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 102/200 [02:16<01:57,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 103/200 [02:17<01:54,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [02:18<01:52,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 105/200 [02:19<01:50,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 106/200 [02:21<01:50,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/200 [02:22<02:01,  1.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [02:24<02:07,  1.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 109/200 [02:25<02:00,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [02:26<01:53,  1.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 111/200 [02:27<01:49,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/200 [02:28<01:46,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 113/200 [02:30<01:44,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 114/200 [02:31<01:41,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [02:32<01:38,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 116/200 [02:33<01:37,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/200 [02:34<01:40,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [02:36<01:50,  1.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [02:37<01:51,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [02:39<01:45,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [02:40<01:40,  1.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 122/200 [02:41<01:35,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 123/200 [02:42<01:32,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [02:43<01:30,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [02:44<01:28,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 126/200 [02:45<01:26,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 127/200 [02:47<01:25,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 128/200 [02:48<01:33,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 129/200 [02:50<01:39,  1.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [02:51<01:32,  1.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 131/200 [02:52<01:28,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 132/200 [02:53<01:24,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 133/200 [02:54<01:20,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 134/200 [02:56<01:18,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 135/200 [02:57<01:17,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 136/200 [02:58<01:15,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 137/200 [02:59<01:13,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 138/200 [03:00<01:13,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [03:02<01:18,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [03:03<01:22,  1.38s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 141/200 [03:05<01:16,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 142/200 [03:06<01:12,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 143/200 [03:07<01:09,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [03:08<01:07,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 145/200 [03:09<01:05,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 146/200 [03:10<01:03,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [03:11<01:01,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [03:13<01:00,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [03:14<01:00,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [03:15<01:05,  1.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 151/200 [03:17<01:06,  1.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [03:18<01:01,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 153/200 [03:19<00:58,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 154/200 [03:20<00:55,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 155/200 [03:21<00:53,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 156/200 [03:23<00:52,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 157/200 [03:24<00:50,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 158/200 [03:25<00:48,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 159/200 [03:26<00:47,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [03:28<00:49,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 161/200 [03:29<00:52,  1.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 162/200 [03:30<00:50,  1.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 163/200 [03:32<00:47,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 164/200 [03:33<00:44,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 165/200 [03:34<00:42,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 166/200 [03:35<00:40,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 167/200 [03:36<00:38,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 168/200 [03:37<00:37,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 169/200 [03:38<00:35,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [03:40<00:34,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 171/200 [03:41<00:37,  1.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 172/200 [03:43<00:39,  1.40s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 173/200 [03:44<00:35,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [03:45<00:32,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [03:46<00:30,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [03:47<00:28,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 177/200 [03:49<00:27,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [03:50<00:25,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [03:51<00:24,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [03:52<00:23,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 181/200 [03:53<00:22,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 182/200 [03:55<00:23,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [03:56<00:23,  1.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [03:58<00:21,  1.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 185/200 [03:59<00:19,  1.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 186/200 [04:00<00:17,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 187/200 [04:01<00:15,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 188/200 [04:02<00:14,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 189/200 [04:03<00:12,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [04:04<00:11,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 191/200 [04:06<00:10,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 192/200 [04:07<00:09,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 193/200 [04:08<00:09,  1.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 194/200 [04:10<00:08,  1.35s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [04:11<00:06,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 196/200 [04:12<00:04,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 197/200 [04:13<00:03,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 198/200 [04:14<00:02,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 5-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [04:16<00:01,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["base 5-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [04:17<00:00,  1.29s/it]\n","base 1-shot:   0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 20 has 10 images but required 11. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   0%|          | 1/200 [00:01<03:22,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   1%|          | 2/200 [00:02<03:33,  1.08s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 7 has 10 images but required 11. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   2%|‚ñè         | 3/200 [00:03<03:40,  1.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   2%|‚ñè         | 4/200 [00:04<04:07,  1.26s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 9 has 9 images but required 11. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   2%|‚ñé         | 5/200 [00:06<04:11,  1.29s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 11 has 7 images but required 11. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   3%|‚ñé         | 6/200 [00:07<03:49,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   4%|‚ñé         | 7/200 [00:08<03:36,  1.12s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 37 has 9 images but required 11. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   4%|‚ñç         | 8/200 [00:09<03:27,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   4%|‚ñç         | 9/200 [00:10<03:22,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   5%|‚ñå         | 10/200 [00:11<03:20,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   6%|‚ñå         | 11/200 [00:12<03:16,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   6%|‚ñå         | 12/200 [00:13<03:13,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   6%|‚ñã         | 13/200 [00:14<03:11,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   7%|‚ñã         | 14/200 [00:15<03:08,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   8%|‚ñä         | 15/200 [00:16<03:09,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   8%|‚ñä         | 16/200 [00:17<03:32,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   8%|‚ñä         | 17/200 [00:19<03:46,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:   9%|‚ñâ         | 18/200 [00:20<03:32,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  10%|‚ñâ         | 19/200 [00:21<03:21,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  10%|‚ñà         | 20/200 [00:22<03:13,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  10%|‚ñà         | 21/200 [00:23<03:07,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  11%|‚ñà         | 22/200 [00:24<03:03,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  12%|‚ñà‚ñè        | 23/200 [00:25<03:00,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  12%|‚ñà‚ñè        | 24/200 [00:26<02:59,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  12%|‚ñà‚ñé        | 25/200 [00:27<02:57,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  13%|‚ñà‚ñé        | 26/200 [00:28<02:54,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  14%|‚ñà‚ñé        | 27/200 [00:29<02:51,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  14%|‚ñà‚ñç        | 28/200 [00:30<03:09,  1.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  14%|‚ñà‚ñç        | 29/200 [00:31<03:24,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  15%|‚ñà‚ñå        | 30/200 [00:33<03:26,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  16%|‚ñà‚ñå        | 31/200 [00:34<03:14,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  16%|‚ñà‚ñå        | 32/200 [00:35<03:05,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  16%|‚ñà‚ñã        | 33/200 [00:36<02:59,  1.07s/it]/tmp/ipython-input-2489329178.py:213: UserWarning: Class 28 has 10 images but required 11. Sampling with replacement.\n","  warnings.warn(f\"Class {cls} has {len(imgs)} images but required {required}. Sampling with replacement.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  17%|‚ñà‚ñã        | 34/200 [00:37<02:54,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  18%|‚ñà‚ñä        | 35/200 [00:38<02:52,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  18%|‚ñà‚ñä        | 36/200 [00:39<02:48,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  18%|‚ñà‚ñä        | 37/200 [00:40<02:44,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  19%|‚ñà‚ñâ        | 38/200 [00:41<02:43,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  20%|‚ñà‚ñâ        | 39/200 [00:42<02:42,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  20%|‚ñà‚ñà        | 40/200 [00:43<02:48,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  20%|‚ñà‚ñà        | 41/200 [00:44<03:06,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  21%|‚ñà‚ñà        | 42/200 [00:45<03:11,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  22%|‚ñà‚ñà‚ñè       | 43/200 [00:46<03:02,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  22%|‚ñà‚ñà‚ñè       | 44/200 [00:47<02:53,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  22%|‚ñà‚ñà‚ñé       | 45/200 [00:49<02:48,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  23%|‚ñà‚ñà‚ñé       | 46/200 [00:49<02:41,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  24%|‚ñà‚ñà‚ñé       | 47/200 [00:50<02:37,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  24%|‚ñà‚ñà‚ñç       | 48/200 [00:51<02:34,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  24%|‚ñà‚ñà‚ñç       | 49/200 [00:52<02:32,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  25%|‚ñà‚ñà‚ñå       | 50/200 [00:53<02:30,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  26%|‚ñà‚ñà‚ñå       | 51/200 [00:54<02:28,  1.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  26%|‚ñà‚ñà‚ñå       | 52/200 [00:56<02:33,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  26%|‚ñà‚ñà‚ñã       | 53/200 [00:57<02:51,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  27%|‚ñà‚ñà‚ñã       | 54/200 [00:58<03:02,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  28%|‚ñà‚ñà‚ñä       | 55/200 [00:59<02:49,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  28%|‚ñà‚ñà‚ñä       | 56/200 [01:00<02:39,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  28%|‚ñà‚ñà‚ñä       | 57/200 [01:01<02:32,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  29%|‚ñà‚ñà‚ñâ       | 58/200 [01:02<02:28,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  30%|‚ñà‚ñà‚ñâ       | 59/200 [01:03<02:24,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  30%|‚ñà‚ñà‚ñà       | 60/200 [01:04<02:21,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  30%|‚ñà‚ñà‚ñà       | 61/200 [01:05<02:20,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  31%|‚ñà‚ñà‚ñà       | 62/200 [01:06<02:19,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  32%|‚ñà‚ñà‚ñà‚ñè      | 63/200 [01:07<02:17,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  32%|‚ñà‚ñà‚ñà‚ñè      | 64/200 [01:08<02:16,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  32%|‚ñà‚ñà‚ñà‚ñé      | 65/200 [01:10<02:31,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  33%|‚ñà‚ñà‚ñà‚ñé      | 66/200 [01:11<02:43,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  34%|‚ñà‚ñà‚ñà‚ñé      | 67/200 [01:13<02:46,  1.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  34%|‚ñà‚ñà‚ñà‚ñç      | 68/200 [01:13<02:34,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  34%|‚ñà‚ñà‚ñà‚ñç      | 69/200 [01:14<02:26,  1.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  35%|‚ñà‚ñà‚ñà‚ñå      | 70/200 [01:15<02:20,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  36%|‚ñà‚ñà‚ñà‚ñå      | 71/200 [01:17<02:18,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  36%|‚ñà‚ñà‚ñà‚ñå      | 72/200 [01:18<02:13,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  36%|‚ñà‚ñà‚ñà‚ñã      | 73/200 [01:19<02:10,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [01:19<02:08,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [01:20<02:04,  1.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  38%|‚ñà‚ñà‚ñà‚ñä      | 76/200 [01:21<02:02,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  38%|‚ñà‚ñà‚ñà‚ñä      | 77/200 [01:23<02:12,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  39%|‚ñà‚ñà‚ñà‚ñâ      | 78/200 [01:24<02:24,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  40%|‚ñà‚ñà‚ñà‚ñâ      | 79/200 [01:25<02:27,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  40%|‚ñà‚ñà‚ñà‚ñà      | 80/200 [01:26<02:19,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  40%|‚ñà‚ñà‚ñà‚ñà      | 81/200 [01:27<02:12,  1.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  41%|‚ñà‚ñà‚ñà‚ñà      | 82/200 [01:28<02:07,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 83/200 [01:29<02:03,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 84/200 [01:30<02:00,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 85/200 [01:31<01:57,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 86/200 [01:32<01:55,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/200 [01:33<01:52,  1.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [01:34<01:51,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [01:35<01:53,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [01:37<02:07,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [01:38<02:13,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 92/200 [01:39<02:04,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 93/200 [01:40<01:57,  1.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 94/200 [01:41<01:53,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 95/200 [01:42<01:50,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [01:43<01:46,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 97/200 [01:44<01:44,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 98/200 [01:45<01:42,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [01:46<01:40,  1.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [01:47<01:39,  1.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 101/200 [01:48<01:39,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 102/200 [01:50<01:52,  1.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 103/200 [01:51<02:00,  1.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 104/200 [01:52<01:53,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 105/200 [01:53<01:46,  1.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 106/200 [01:54<01:40,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 107/200 [01:55<01:37,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 108/200 [01:56<01:35,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 109/200 [01:57<01:32,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [01:58<01:31,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 111/200 [01:59<01:29,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 112/200 [02:00<01:28,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 113/200 [02:01<01:27,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 114/200 [02:02<01:35,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [02:04<01:42,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 116/200 [02:05<01:43,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/200 [02:06<01:36,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [02:07<01:31,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [02:08<01:27,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [02:09<01:24,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [02:10<01:22,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 122/200 [02:11<01:20,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 123/200 [02:12<01:18,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [02:13<01:16,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [02:14<01:15,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 126/200 [02:15<01:20,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 127/200 [02:17<01:27,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 128/200 [02:18<01:28,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 129/200 [02:19<01:22,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 130/200 [02:20<01:17,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 131/200 [02:21<01:14,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 132/200 [02:22<01:11,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 133/200 [02:23<01:10,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 134/200 [02:24<01:08,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 135/200 [02:25<01:06,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 136/200 [02:26<01:05,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 137/200 [02:27<01:03,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 138/200 [02:28<01:05,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [02:30<01:12,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [02:31<01:13,  1.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 141/200 [02:32<01:08,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 142/200 [02:33<01:05,  1.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 143/200 [02:34<01:02,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [02:35<00:59,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 145/200 [02:36<00:57,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 146/200 [02:37<00:56,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [02:38<00:54,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [02:39<00:53,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [02:40<00:51,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [02:42<00:52,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 151/200 [02:43<00:57,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [02:44<01:00,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 153/200 [02:45<00:55,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 154/200 [02:46<00:51,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 155/200 [02:47<00:49,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 156/200 [02:48<00:46,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 157/200 [02:49<00:44,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 158/200 [02:50<00:43,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 159/200 [02:51<00:42,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 160/200 [02:52<00:40,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 161/200 [02:53<00:39,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 162/200 [02:55<00:39,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 163/200 [02:56<00:43,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 164/200 [02:57<00:45,  1.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 165/200 [02:59<00:41,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 166/200 [03:00<00:38,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 167/200 [03:01<00:35,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 168/200 [03:01<00:33,  1.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 169/200 [03:02<00:31,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [03:03<00:30,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 171/200 [03:04<00:29,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 172/200 [03:05<00:28,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 173/200 [03:06<00:26,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [03:07<00:25,  1.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [03:09<00:27,  1.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [03:10<00:28,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 177/200 [03:11<00:28,  1.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [03:12<00:25,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [03:13<00:23,  1.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [03:14<00:21,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 181/200 [03:15<00:19,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 182/200 [03:16<00:18,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [03:17<00:17,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 184/200 [03:18<00:16,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 185/200 [03:19<00:14,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 186/200 [03:20<00:13,  1.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 187/200 [03:21<00:13,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 188/200 [03:23<00:13,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 189/200 [03:24<00:13,  1.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [03:25<00:11,  1.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 191/200 [03:26<00:09,  1.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 192/200 [03:27<00:08,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 193/200 [03:28<00:07,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 194/200 [03:29<00:06,  1.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [03:30<00:05,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 196/200 [03:31<00:04,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 197/200 [03:32<00:03,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 198/200 [03:33<00:02,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["\rbase 1-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [03:34<00:01,  1.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Base backbone 'swin_tiny' inferred feat dim: 7\n"]},{"output_type":"stream","name":"stderr","text":["base 1-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [03:36<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Base 5-shot mean: 37.47%  |  Base 1-shot mean: 27.78%\n","Backbone 'swin_tiny' inferred feat dim: 7\n","Backbone 'deit_small_distilled' inferred feat dim: 384\n","Backbone 'coatnet_0' inferred feat dim: 768\n","Built hybrid-fusion extractors for: ['swin_tiny', 'deit_small_distilled', 'coatnet_0']\n","Running Hybrid Fusion few-shot evaluation (HFPN) ...\n"]},{"output_type":"stream","name":"stderr","text":["fusion 5-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [05:52<00:00,  1.76s/it]\n","fusion 1-shot: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [04:18<00:00,  1.29s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Fusion 5-shot mean: 87.03%  |  Fusion 1-shot mean: 67.07%\n","Saved integrated few-shot summary CSV -> /content/experiments/fewshot_summary_integrated.csv\n","Saved histogram -> /content/experiments/plots/fewshot_integrated_histogram.png\n","Saved boxplot -> /content/experiments/plots/fewshot_integrated_boxplot.png\n","Saved mean¬±std bar -> /content/experiments/plots/fewshot_integrated_meanstdbar.png\n","Saved sorted-episodes plot -> /content/experiments/plots/fewshot_integrated_sortedepisodes.png\n","\n","Integrated few-shot summary (printed):\n","     setting  mean_acc  std_acc  median  n_episodes\n","  base_5shot    0.3747 0.092328    0.38         200\n","  base_1shot    0.2778 0.083703    0.28         200\n","fusion_5shot    0.8703 0.069397    0.88         200\n","fusion_1shot    0.6707 0.115700    0.68         200\n","\n","All integrated few-shot artifacts saved in: /content/experiments/plots\n"]}]},{"cell_type":"code","source":["# -------------------------\n","# Combined comparison: run both Keras and PyTorch models on the same test set and save combined metrics & plots\n","# -------------------------\n","# Build test_paths and y_test (we already have test_items)\n","test_paths = [p for p,_ in test_items]\n","y_test = np.array([lbl for _,lbl in test_items])\n","print(\"Test samples:\", len(test_paths))"],"metadata":{"id":"oLcIytD9m7R0","executionInfo":{"status":"ok","timestamp":1763837373802,"user_tz":-330,"elapsed":57,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"21e72519-cc12-413e-a04c-1670aafceb84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test samples: 595\n"]}]},{"cell_type":"code","source":["# Keras predictions (use trained_classifiers dict)\n","def keras_predict_on_paths(keras_model, paths, batch=32, target_size=IMG_SIZE):\n","    preds_list=[]; probs_list=[]\n","    for i in range(0,len(paths),batch):\n","        batch_paths = paths[i:i+batch]\n","        imgs=[]\n","        for p in batch_paths:\n","            im = Image.open(p).resize(target_size)\n","            a = np.array(im).astype('float32')/255.0\n","            imgs.append(a)\n","        xb = np.stack(imgs, axis=0)\n","        probs = keras_model.predict(xb, verbose=0)\n","        preds_list.append(np.argmax(probs, axis=1))\n","        probs_list.append(probs)\n","    return np.concatenate(probs_list, axis=0), np.concatenate(preds_list, axis=0)\n","\n","keras_preds = {}\n","for name, model in trained_classifiers.items():\n","    print(\"Predict (Keras):\", name)\n","    probs, preds = keras_predict_on_paths(model, test_paths, batch=32, target_size=IMG_SIZE)\n","    keras_preds[name] = {'probs': probs, 'preds': preds}\n"],"metadata":{"id":"8pWw7KpBnID8","executionInfo":{"status":"ok","timestamp":1763837453898,"user_tz":-330,"elapsed":24492,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0aa03e17-1e15-4823-9bf2-d43169140884"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predict (Keras): MobileNetV2\n","Predict (Keras): ResNet152\n","Predict (Keras): MobileNetV3\n","Predict (Keras): ResNet50_feats\n"]}]},{"cell_type":"code","source":["# PyTorch predictions (load best ckpts)\n","val_tf_pt = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","def pytorch_predict_ckpt(factory, ckpt, paths, batch=32):\n","    model = factory(num_classes).to(DEVICE)\n","    st = torch.load(ckpt, map_location=DEVICE)\n","    try:\n","        model.load_state_dict(st)\n","    except Exception:\n","        if isinstance(st, dict) and 'state_dict' in st:\n","            model.load_state_dict(st['state_dict'])\n","        else:\n","            model.load_state_dict(st, strict=False)\n","    model.eval()\n","    all_probs=[]; all_preds=[]\n","    for i in range(0,len(paths),batch):\n","        batch_paths = paths[i:i+batch]\n","        imgs=[]\n","        for p in batch_paths:\n","            im = Image.open(p).convert('RGB')\n","            t = val_tf_pt(im)\n","            imgs.append(t)\n","        xb = torch.stack(imgs).to(DEVICE)\n","        with torch.no_grad():\n","            logits = model(xb)\n","            probs = torch.softmax(logits, dim=1).cpu().numpy()\n","            preds = probs.argmax(axis=1)\n","        all_probs.append(probs); all_preds.append(preds)\n","    return np.vstack(all_probs), np.concatenate(all_preds)\n","\n","pytorch_preds = {}\n","for name, fac in pytorch_factories.items():\n","    ckpt_path = os.path.join(OUT_ROOT, f'{name}_best.pth')\n","    if not os.path.exists(ckpt_path):\n","        print(\"PyTorch checkpoint missing for\", name, \"-> skipping predict\")\n","        continue\n","    print(\"Predict (PyTorch):\", name)\n","    probs, preds = pytorch_predict_ckpt(fac, ckpt_path, test_paths, batch=32)\n","    pytorch_preds[name] = {'probs': probs, 'preds': preds}\n"],"metadata":{"id":"ekHqL0JsnP5v","executionInfo":{"status":"ok","timestamp":1763837472765,"user_tz":-330,"elapsed":18834,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c02422f-e431-4a86-b054-41cb1d464250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predict (PyTorch): swin_tiny\n","Predict (PyTorch): deit_small_distilled\n","Predict (PyTorch): coatnet_0\n"]}]},{"cell_type":"code","source":["# Combine all preds\n","all_preds = {}\n","all_preds.update({f\"keras::{k}\": v for k,v in keras_preds.items()})\n","all_preds.update({f\"torch::{k}\": v for k,v in pytorch_preds.items()})\n","if len(all_preds)==0:\n","    raise RuntimeError(\"No models produced predictions. Check earlier steps.\")\n"],"metadata":{"id":"13dkwKyAnTKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute metrics and save\n","rows=[]\n","for key, v in all_preds.items():\n","    y_pred = v['preds']\n","    acc = accuracy_score(y_test, y_pred)\n","    mp = precision_score(y_test, y_pred, average='macro', zero_division=0)\n","    mr = recall_score(y_test, y_pred, average='macro', zero_division=0)\n","    mf1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n","    rows.append({'model': key, 'accuracy': acc, 'macro_precision': mp, 'macro_recall': mr, 'macro_f1': mf1})\n","summary_df = pd.DataFrame(rows).sort_values('macro_f1', ascending=False).reset_index(drop=True)\n","summary_df.to_csv(os.path.join(OUT_ROOT,'combined_summary.csv'), index=False)\n","print(\"Saved combined_summary.csv\")\n","\n","# Save per-model confusion and classification reports & plots\n","for key, v in all_preds.items():\n","    preds = v['preds']\n","    cm = confusion_matrix(y_test, preds)\n","    rep = classification_report(y_test, preds, output_dict=True, zero_division=0)\n","    pd.DataFrame(rep).transpose().to_csv(os.path.join(OUT_ROOT, f\"{key.replace('::','_')}_classification_report.csv\"))\n","    np.save(os.path.join(OUT_ROOT, f\"{key.replace('::','_')}_cm.npy\"), cm)\n","    # plot\n","    plt.figure(figsize=(8,6))\n","    sns.heatmap(cm, annot=True, fmt='d', xticklabels=full_dataset.classes, yticklabels=full_dataset.classes, cmap='Blues')\n","    plt.title(f'Confusion: {key}'); plt.tight_layout()\n","    plt.savefig(os.path.join(PLOTS_DIR, f\"{key.replace('::','_')}_confusion.png\")); plt.close()\n","\n","# Plot overall ranking bars\n","plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n","sns.barplot(data=summary_df, x='model', y='accuracy'); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.savefig(os.path.join(PLOTS_DIR,'combined_accuracy.png')); plt.close()\n","plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n","sns.barplot(data=summary_df, x='model', y='macro_f1'); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.savefig(os.path.join(PLOTS_DIR,'combined_macro_f1.png')); plt.close()\n","print(\"Saved combined plots to\", PLOTS_DIR)\n"],"metadata":{"id":"bI6XB2PqnZDP","executionInfo":{"status":"ok","timestamp":1763837513771,"user_tz":-330,"elapsed":22180,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e33d1e2-b03f-4757-d3ab-4955ee90091d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved combined_summary.csv\n","Saved combined plots to /content/experiments/plots\n"]}]},{"cell_type":"code","source":["# ---------------------------\n","# Add this cell: Summary report + bar plots for all models\n","# Paste & run after training + combined evaluation cells\n","# ---------------------------\n","import time, math\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","import torch\n","\n","sns.set(style=\"whitegrid\")\n","\n","OUT_CSV = os.path.join(OUT_ROOT, 'models_comparison_summary.csv')\n","PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(OUT_ROOT, 'plots'))\n","os.makedirs(PLOTS_DIR, exist_ok=True)\n","\n","# ---- Helpers to (re)compute predictions if needed ----\n","need_recompute = 'all_preds' not in globals() or not isinstance(all_preds, dict) or len(all_preds)==0\n","\n","# Use existing all_preds if present\n","preds_map = {}\n","if not need_recompute:\n","    preds_map = dict(all_preds)  # keys like 'keras::Model', 'torch::name'\n","else:\n","    # fallback: try to compute using helpers if defined\n","    test_paths_local = globals().get('test_paths', [p for p,_ in test_items])\n","    y_test_local = np.array([lbl for _,lbl in test_items])\n","    print(\"Recomputing predictions (this may take a short while)...\")\n","    # Keras\n","    if 'trained_classifiers' in globals() and isinstance(trained_classifiers, dict):\n","        for name, km in trained_classifiers.items():\n","            try:\n","                # reuse keras_predict_on_paths if available\n","                if 'keras_predict_on_paths' in globals():\n","                    probs, preds = keras_predict_on_paths(km, test_paths_local, batch=32, target_size=IMG_SIZE)\n","                else:\n","                    # simple fallback\n","                    probs_list=[]; preds_list=[]\n","                    for i in range(0,len(test_paths_local),32):\n","                        batch = test_paths_local[i:i+32]; imgs=[]\n","                        for p in batch:\n","                            im = Image.open(p).resize(IMG_SIZE)\n","                            a = np.array(im).astype('float32')/255.0; imgs.append(a)\n","                        xb = np.stack(imgs,0)\n","                        ps = km.predict(xb, verbose=0)\n","                        probs_list.append(ps); preds_list.append(np.argmax(ps,axis=1))\n","                    probs = np.vstack(probs_list); preds = np.concatenate(preds_list)\n","                preds_map[f\"keras::{name}\"] = {'probs':probs, 'preds':preds}\n","                print(\" - Keras preds:\", name)\n","            except Exception as e:\n","                print(\"  Failed Keras predict for\", name, e)\n","    # PyTorch\n","    if 'pytorch_factories' in globals():\n","        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        val_tf_pt = globals().get('val_tf_pt', None)\n","        if val_tf_pt is None:\n","            from torchvision import transforms\n","            val_tf_pt = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","        for name, fac in pytorch_factories.items():\n","            ckpt_path = os.path.join(OUT_ROOT, f'{name}_best.pth')\n","            if not os.path.exists(ckpt_path):\n","                print(\" - skip torch:\", name, \" (checkpoint missing )\")\n","                continue\n","            try:\n","                # reuse pytorch_predict_ckpt if present\n","                if 'pytorch_predict_ckpt' in globals():\n","                    probs, preds = pytorch_predict_ckpt(fac, ckpt_path, test_paths_local, batch=32)\n","                else:\n","                    # minimal local predict:\n","                    model = fac(num_classes).to(device)\n","                    st = torch.load(ckpt_path, map_location=device)\n","                    try: model.load_state_dict(st)\n","                    except:\n","                        if isinstance(st, dict) and 'state_dict' in st: model.load_state_dict(st['state_dict'])\n","                        else: model.load_state_dict(st, strict=False)\n","                    model.eval()\n","                    all_probs=[]; all_preds=[]\n","                    for i in range(0,len(test_paths_local),32):\n","                        batch = test_paths_local[i:i+32]; imgs=[]\n","                        for p in batch:\n","                            im = Image.open(p).convert('RGB'); t = val_tf_pt(im); imgs.append(t)\n","                        xb = torch.stack(imgs).to(device)\n","                        with torch.no_grad():\n","                            logits = model(xb); probs_b = torch.softmax(logits, dim=1).cpu().numpy(); preds_b = probs_b.argmax(axis=1)\n","                        all_probs.append(probs_b); all_preds.append(preds_b)\n","                    probs = np.vstack(all_probs); preds = np.concatenate(all_preds)\n","                preds_map[f\"torch::{name}\"] = {'probs':probs, 'preds':preds}\n","                print(\" - Torch preds:\", name)\n","            except Exception as e:\n","                print(\"  Failed Torch predict for\", name, e)\n","\n","# ---- Build summary metrics (accuracy, macro_f1) ----\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","summary_rows = []\n","y_true = np.array([lbl for _,lbl in test_items])\n","\n","for model_key, d in preds_map.items():\n","    preds = np.array(d['preds'])\n","    acc = float(accuracy_score(y_true, preds))\n","    macro_f1 = float(f1_score(y_true, preds, average='macro', zero_division=0))\n","    macro_prec = float(precision_score(y_true, preds, average='macro', zero_division=0))\n","    macro_rec = float(recall_score(y_true, preds, average='macro', zero_division=0))\n","    summary_rows.append({'model': model_key, 'accuracy': acc, 'macro_precision': macro_prec, 'macro_recall': macro_rec, 'macro_f1': macro_f1})\n","\n","summary_df = pd.DataFrame(summary_rows).sort_values('macro_f1', ascending=False).reset_index(drop=True)\n","\n","# ---- Parameter counts ----\n","param_rows = []\n","# Keras params\n","if 'trained_classifiers' in globals():\n","    for name, km in trained_classifiers.items():\n","        try:\n","            params = km.count_params()\n","        except Exception:\n","            # fallback count trainable & non-trainable\n","            params = sum([np.prod(w.shape) for w in km.weights])\n","        param_rows.append({'model': f'keras::{name}', 'params': int(params)})\n","# PyTorch params\n","if 'pytorch_factories' in globals():\n","    for name, fac in pytorch_factories.items():\n","        try:\n","            m = fac(num_classes)\n","            pcount = sum([p.numel() for p in m.parameters()])\n","            param_rows.append({'model': f'torch::{name}', 'params': int(pcount)})\n","        except Exception as e:\n","            print(\"Failed param count for\", name, e)\n","\n","df_params = pd.DataFrame(param_rows)\n","\n","# merge param info into summary\n","summary_df = summary_df.merge(df_params, how='left', left_on='model', right_on='model')\n","summary_df['params_M'] = (summary_df['params'] / 1e6).round(3)\n","\n","# ---- Inference timing (approx avg ms per image) ----\n","def measure_inference_time_keras(km, paths, n_samples=50, batch=8):\n","    # sample n_samples images\n","    sel = paths[:n_samples]\n","    # warmup\n","    _ = None\n","    t0 = time.time()\n","    for i in range(0,len(sel), batch):\n","        batch_p = sel[i:i+batch]; imgs=[]\n","        for p in batch_p:\n","            im = Image.open(p).resize(IMG_SIZE); a = np.array(im).astype('float32')/255.0; imgs.append(a)\n","        xb = np.stack(imgs,0)\n","        _ = km.predict(xb, verbose=0)\n","    t1 = time.time()\n","    total = t1 - t0\n","    avg_ms = (total / len(sel)) * 1000.0\n","    return avg_ms\n","\n","def measure_inference_time_torch(factory, ckpt, paths, n_samples=50, batch=8):\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model = factory(num_classes).to(device)\n","    st = torch.load(ckpt, map_location=device)\n","    try: model.load_state_dict(st)\n","    except:\n","        if isinstance(st, dict) and 'state_dict' in st: model.load_state_dict(st['state_dict'])\n","        else: model.load_state_dict(st, strict=False)\n","    model.eval()\n","    val_tf = globals().get('val_tf_pt', None)\n","    if val_tf is None:\n","        from torchvision import transforms\n","        val_tf = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","    sel = paths[:n_samples]\n","    # warmup + measure\n","    t0 = time.time()\n","    with torch.no_grad():\n","        for i in range(0,len(sel), batch):\n","            batch_p = sel[i:i+batch]; imgs=[]\n","            for p in batch_p:\n","                im = Image.open(p).convert('RGB'); t = val_tf(im); imgs.append(t)\n","            xb = torch.stack(imgs).to(device)\n","            _ = model(xb)\n","    t1 = time.time()\n","    total = t1 - t0\n","    avg_ms = (total / len(sel)) * 1000.0\n","    return avg_ms\n","\n","# Only measure for models where we can (limit to top N to save time)\n","measure_limit = 6   # number of models to time (set smaller if compute-limited)\n","timing_rows = []\n","model_list_for_timing = summary_df['model'].tolist()[:measure_limit]\n","print(\"Measuring inference time for (up to) {} models: {}\".format(len(model_list_for_timing), model_list_for_timing))\n","for mk in model_list_for_timing:\n","    try:\n","        if mk.startswith('keras::') and 'trained_classifiers' in globals():\n","            kn = mk.split('::',1)[1]\n","            km = trained_classifiers.get(kn)\n","            if km is None:\n","                timing_rows.append({'model':mk, 'inf_ms':np.nan}); continue\n","            avg_ms = measure_inference_time_keras(km, [p for p,_ in test_items], n_samples=50, batch=8)\n","            timing_rows.append({'model':mk, 'inf_ms': round(avg_ms,2)})\n","        elif mk.startswith('torch::') and 'pytorch_factories' in globals():\n","            tn = mk.split('::',1)[1]\n","            fac = pytorch_factories.get(tn)\n","            ckpt = os.path.join(OUT_ROOT, f'{tn}_best.pth')\n","            if fac is None or not os.path.exists(ckpt):\n","                timing_rows.append({'model':mk, 'inf_ms':np.nan}); continue\n","            avg_ms = measure_inference_time_torch(fac, ckpt, [p for p,_ in test_items], n_samples=50, batch=8)\n","            timing_rows.append({'model':mk, 'inf_ms': round(avg_ms,2)})\n","        else:\n","            timing_rows.append({'model':mk, 'inf_ms':np.nan})\n","    except Exception as e:\n","        print(\"Timing failed for\", mk, e)\n","        timing_rows.append({'model':mk, 'inf_ms':np.nan})\n","\n","df_time = pd.DataFrame(timing_rows)\n","summary_df = summary_df.merge(df_time, on='model', how='left')\n","\n","# ---- Save CSV and show table ----\n","summary_df = summary_df[['model','accuracy','macro_precision','macro_recall','macro_f1','params','params_M','inf_ms']]\n","summary_df.to_csv(OUT_CSV, index=False)\n","print(\"Saved summary CSV:\", OUT_CSV)\n","display(summary_df.sort_values('macro_f1', ascending=False).reset_index(drop=True))\n","\n","# ---- Plots: Accuracy, Macro-F1, Params (log) ----\n","plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n","sns.barplot(data=summary_df.sort_values('accuracy', ascending=False), x='model', y='accuracy')\n","plt.xticks(rotation=45, ha='right'); plt.title('Model test accuracy'); plt.tight_layout()\n","plt.savefig(os.path.join(PLOTS_DIR, 'summary_accuracy_bar.png')); plt.close()\n","\n","plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n","sns.barplot(data=summary_df.sort_values('macro_f1', ascending=False), x='model', y='macro_f1')\n","plt.xticks(rotation=45, ha='right'); plt.title('Model macro F1'); plt.tight_layout()\n","plt.savefig(os.path.join(PLOTS_DIR, 'summary_macrof1_bar.png')); plt.close()\n","\n","# Params plot (log scale)\n","plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n","sns.barplot(data=summary_df.sort_values('params_M', ascending=False), x='model', y='params_M')\n","plt.yscale('log')\n","plt.xticks(rotation=45, ha='right'); plt.title('Model Params (millions, log scale)'); plt.tight_layout()\n","plt.savefig(os.path.join(PLOTS_DIR, 'summary_params_bar.png')); plt.close()\n","\n","# Inference time plot (if available)\n","if 'inf_ms' in summary_df.columns and summary_df['inf_ms'].notna().any():\n","    plt.figure(figsize=(max(6, len(summary_df)*0.6),4))\n","    sns.barplot(data=summary_df.sort_values('inf_ms', ascending=True), x='model', y='inf_ms')\n","    plt.xticks(rotation=45, ha='right'); plt.ylabel('Avg inference (ms/image)'); plt.title('Inference time (approx)'); plt.tight_layout()\n","    plt.savefig(os.path.join(PLOTS_DIR,'summary_inference_time.png')); plt.close()\n","\n","print(\"Saved plots to\", PLOTS_DIR)\n"],"metadata":{"id":"znwYxt8Y4TM-","executionInfo":{"status":"ok","timestamp":1763837603471,"user_tz":-330,"elapsed":50676,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"colab":{"base_uri":"https://localhost:8080/","height":342},"outputId":"a81de7c2-8ba1-41aa-9790-6431966b8fe2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Measuring inference time for (up to) 6 models: ['torch::swin_tiny', 'torch::deit_small_distilled', 'torch::coatnet_0', 'keras::MobileNetV2', 'keras::ResNet152', 'keras::ResNet50_feats']\n","Saved summary CSV: /content/experiments/models_comparison_summary.csv\n"]},{"output_type":"display_data","data":{"text/plain":["                         model  accuracy  macro_precision  macro_recall  \\\n","0             torch::swin_tiny  0.989916         0.989132      0.989375   \n","1  torch::deit_small_distilled  0.979832         0.980064      0.978173   \n","2             torch::coatnet_0  0.979832         0.980611      0.977031   \n","3           keras::MobileNetV2  0.936134         0.940285      0.937722   \n","4             keras::ResNet152  0.171429         0.178767      0.178919   \n","5        keras::ResNet50_feats  0.112605         0.110102      0.112459   \n","6           keras::MobileNetV3  0.062185         0.005104      0.044423   \n","\n","   macro_f1    params  params_M  inf_ms  \n","0  0.988827  27550114    27.550    6.28  \n","1  0.977827  21697232    21.697    4.87  \n","2  0.977778  26697322    26.697    6.96  \n","3  0.935304   2427112     2.427  436.70  \n","4  0.122368  58638376    58.638  323.06  \n","5  0.065966  24122536    24.123  115.82  \n","6  0.008888   3124520     3.125     NaN  "],"text/html":["\n","  <div id=\"df-9b5637dc-e0fc-4f03-8018-1635966ea767\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","      <th>macro_precision</th>\n","      <th>macro_recall</th>\n","      <th>macro_f1</th>\n","      <th>params</th>\n","      <th>params_M</th>\n","      <th>inf_ms</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>torch::swin_tiny</td>\n","      <td>0.989916</td>\n","      <td>0.989132</td>\n","      <td>0.989375</td>\n","      <td>0.988827</td>\n","      <td>27550114</td>\n","      <td>27.550</td>\n","      <td>6.28</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>torch::deit_small_distilled</td>\n","      <td>0.979832</td>\n","      <td>0.980064</td>\n","      <td>0.978173</td>\n","      <td>0.977827</td>\n","      <td>21697232</td>\n","      <td>21.697</td>\n","      <td>4.87</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>torch::coatnet_0</td>\n","      <td>0.979832</td>\n","      <td>0.980611</td>\n","      <td>0.977031</td>\n","      <td>0.977778</td>\n","      <td>26697322</td>\n","      <td>26.697</td>\n","      <td>6.96</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>keras::MobileNetV2</td>\n","      <td>0.936134</td>\n","      <td>0.940285</td>\n","      <td>0.937722</td>\n","      <td>0.935304</td>\n","      <td>2427112</td>\n","      <td>2.427</td>\n","      <td>436.70</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>keras::ResNet152</td>\n","      <td>0.171429</td>\n","      <td>0.178767</td>\n","      <td>0.178919</td>\n","      <td>0.122368</td>\n","      <td>58638376</td>\n","      <td>58.638</td>\n","      <td>323.06</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>keras::ResNet50_feats</td>\n","      <td>0.112605</td>\n","      <td>0.110102</td>\n","      <td>0.112459</td>\n","      <td>0.065966</td>\n","      <td>24122536</td>\n","      <td>24.123</td>\n","      <td>115.82</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>keras::MobileNetV3</td>\n","      <td>0.062185</td>\n","      <td>0.005104</td>\n","      <td>0.044423</td>\n","      <td>0.008888</td>\n","      <td>3124520</td>\n","      <td>3.125</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b5637dc-e0fc-4f03-8018-1635966ea767')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9b5637dc-e0fc-4f03-8018-1635966ea767 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9b5637dc-e0fc-4f03-8018-1635966ea767');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f4010002-d457-40a6-bf11-79c178f00411\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f4010002-d457-40a6-bf11-79c178f00411')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f4010002-d457-40a6-bf11-79c178f00411 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(\\\"Saved plots to\\\", PLOTS_DIR)\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"torch::swin_tiny\",\n          \"torch::deit_small_distilled\",\n          \"keras::ResNet50_feats\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4589648096024044,\n        \"min\": 0.06218487394957983,\n        \"max\": 0.9899159663865547,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9899159663865547,\n          0.9798319327731092,\n          0.06218487394957983\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.47043126443615196,\n        \"min\": 0.005103954197807385,\n        \"max\": 0.9891321178821177,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.9891321178821177,\n          0.9800642066267067,\n          0.11010202683094153\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.46087965401305897,\n        \"min\": 0.044423076923076926,\n        \"max\": 0.9893750000000001,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.9893750000000001,\n          0.9781727994227994,\n          0.11245945682284693\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4847094125034284,\n        \"min\": 0.00888829748633848,\n        \"max\": 0.988826643592278,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.988826643592278,\n          0.9778271526258535,\n          0.06596633667905104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18815718,\n        \"min\": 2427112,\n        \"max\": 58638376,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          27550114,\n          21697232,\n          24122536\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"params_M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.815529089152978,\n        \"min\": 2.427,\n        \"max\": 58.638,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          27.55,\n          21.697,\n          24.123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inf_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 187.34294055732835,\n        \"min\": 4.87,\n        \"max\": 436.7,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          6.28,\n          4.87,\n          115.82\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved plots to /content/experiments/plots\n"]}]},{"cell_type":"code","source":["# Pairwise McNemar tests\n","names = list(all_preds.keys()); n=len(names)\n","pvals = np.ones((n,n)); stats = np.zeros((n,n))\n","for i in range(n):\n","    for j in range(i+1, n):\n","        a = all_preds[names[i]]['preds']; b = all_preds[names[j]]['preds']\n","        a_corr = (a==y_test); b_corr = (b==y_test)\n","        n01 = int(np.logical_and(a_corr==True, b_corr==False).sum())\n","        n10 = int(np.logical_and(a_corr==False, b_corr==True).sum())\n","        table = [[int(np.logical_and(a_corr==True,b_corr==True).sum()), n01],[n10, int(np.logical_and(a_corr==False,b_corr==False).sum())]]\n","        res = mcnemar(table, exact=False)\n","        pvals[i,j] = pvals[j,i] = float(res.pvalue)\n","        stats[i,j] = stats[j,i] = float(res.statistic)\n","pd.DataFrame(pvals, index=names, columns=names).to_csv(os.path.join(OUT_ROOT,'mcnemar_pvalues.csv'))\n","pd.DataFrame(stats, index=names, columns=names).to_csv(os.path.join(OUT_ROOT,'mcnemar_stats.csv'))\n","print(\"Saved McNemar p-values and stats to\", OUT_ROOT)\n"],"metadata":{"id":"gVmko4tHndD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Robust Grad-CAM auto-run for top PyTorch models\n","# Paste & run this cell (replaces previous Grad-CAM cell)\n","# -------------------------\n","import os, math, traceback\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import torch\n","from torchvision import transforms\n","\n","# ---------- Robust Grad-CAM function (supports conv outputs and square-token ViT-like outputs) ----------\n","def save_gradcam_grid_pytorch(factory, ckpt, layer_name, paths, save_path, max_images=12, img_size=(224,224), device='cuda'):\n","    \"\"\"\n","    Compute Grad-CAM overlays for up to max_images and save as a grid.\n","    factory: callable(num_classes)->model\n","    ckpt: path to checkpoint .pth\n","    layer_name: exact module name from model.named_modules() to hook\n","    paths: list of image file paths\n","    \"\"\"\n","    # load model and weights\n","    model = factory(num_classes).to(device)\n","    st = torch.load(ckpt, map_location=device)\n","    # handle common wrappers\n","    if isinstance(st, dict) and 'state_dict' in st:\n","        st = st['state_dict']\n","    try:\n","        model.load_state_dict(st)\n","    except Exception:\n","        # try non-strict\n","        try:\n","            model.load_state_dict(st, strict=False)\n","        except Exception:\n","            print(\"Failed to load checkpoint strictly or loosely for\", ckpt)\n","    model.eval()\n","\n","    modules = dict(model.named_modules())\n","    if layer_name not in modules:\n","        print(f\"[GradCAM] Layer '{layer_name}' not found in model modules. Available tail modules: {list(modules.keys())[-40:]}\")\n","        return\n","\n","    target = modules[layer_name]\n","\n","    # holders for hooks\n","    features_holder = {'feat': None}\n","    grads_holder = {'grad': None}\n","\n","    def forward_hook(module, inp, out):\n","        # store first tensor output if tuple/list\n","        if isinstance(out, torch.Tensor):\n","            features_holder['feat'] = out.detach()\n","        elif isinstance(out, (list, tuple)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n","            features_holder['feat'] = out[0].detach()\n","        else:\n","            features_holder['feat'] = None\n","\n","    # use full backward hook when available to avoid partial-grad warning\n","    def backward_hook(module, grad_input, grad_output):\n","        if isinstance(grad_output, torch.Tensor):\n","            grads_holder['grad'] = grad_output.detach()\n","        elif isinstance(grad_output, (list, tuple)) and len(grad_output)>0 and isinstance(grad_output[0], torch.Tensor):\n","            grads_holder['grad'] = grad_output[0].detach()\n","        else:\n","            grads_holder['grad'] = None\n","\n","    # register hooks\n","    try:\n","        fh = target.register_forward_hook(forward_hook)\n","        # prefer full backward hook API\n","        bh = target.register_full_backward_hook(backward_hook)\n","    except Exception:\n","        fh = target.register_forward_hook(forward_hook)\n","        bh = target.register_backward_hook(backward_hook)\n","\n","    overlays = []\n","    tf_img = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","\n","    def compute_cam(feat, grad, target_hw):\n","        \"\"\"\n","        feat, grad: tensors without batch dim (C,H,W) or (C,L) or (C,)\n","        returns: 2D numpy array resized to target_hw or None\n","        \"\"\"\n","        if feat is None or grad is None:\n","            return None\n","\n","        # remove batch if present\n","        if feat.ndim == 4 and feat.shape[0] == 1: feat = feat[0]\n","        if grad.ndim == 4 and grad.shape[0] == 1: grad = grad[0]\n","\n","        # case A: spatial (C,H,W)\n","        if feat.ndim == 3 and grad.ndim == 3:\n","            # channel weights: global avg pool of grads\n","            w = grad.mean(dim=(1,2), keepdim=True)    # (C,1,1)\n","            cam = (w * feat).sum(dim=0).cpu().numpy() # (H,W)\n","        # case B: tokens/sequence (C,L) -> try to reshape to square\n","        elif feat.ndim == 2 and grad.ndim == 2:\n","            L = feat.shape[1]\n","            s = int(np.round(np.sqrt(L)))\n","            if s*s != L:\n","                return None\n","            # token importance\n","            w = grad.mean(dim=1)                        # (C,)\n","            token_scores = (w.unsqueeze(1) * feat).sum(dim=0).cpu().numpy()  # (L,)\n","            cam = token_scores.reshape(s, s)\n","        else:\n","            return None\n","\n","        cam = np.maximum(cam, 0)\n","        if cam.max() <= 1e-9:\n","            return None\n","        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-9)\n","        cam_resized = cv2.resize(cam, (target_hw[1], target_hw[0]))  # cv2(width,height)\n","        return cam_resized\n","\n","    count = 0\n","    for p in paths:\n","        if count >= max_images: break\n","        try:\n","            pil = Image.open(p).convert('RGB').resize(img_size)\n","        except Exception:\n","            continue\n","\n","        x = tf_img(pil).unsqueeze(0).to(device)\n","        x.requires_grad_(True)\n","        features_holder['feat'] = None; grads_holder['grad'] = None\n","\n","        out = model(x)\n","        # predicted class score\n","        pred = int(out.argmax(dim=1).item())\n","        score = out[0, pred]\n","        model.zero_grad()\n","        try:\n","            score.backward(retain_graph=True)\n","        except Exception:\n","            # sometimes backward fails for certain models; try without retain\n","            score.backward()\n","\n","        feat = features_holder.get('feat', None)\n","        grad = grads_holder.get('grad', None)\n","\n","        # remove batch dim if present\n","        if isinstance(feat, torch.Tensor) and feat.ndim == 4 and feat.shape[0] == 1:\n","            feat_proc = feat[0]\n","        else:\n","            feat_proc = feat\n","        if isinstance(grad, torch.Tensor) and grad.ndim == 4 and grad.shape[0] == 1:\n","            grad_proc = grad[0]\n","        else:\n","            grad_proc = grad\n","\n","        cam = compute_cam(feat_proc, grad_proc, target_hw=img_size)\n","        if cam is None:\n","            # skip and continue\n","            print(f\"[GradCAM] skipping image {p}: unsupported feat/grad shapes -> feat {None if feat is None else tuple(feat.shape)}, grad {None if grad is None else tuple(grad.shape)}\")\n","            continue\n","\n","        # colorize and overlay\n","        heat = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n","        heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0\n","        base = np.array(pil).astype(np.float32)/255.0\n","        overlay = np.clip(0.5 * base + 0.5 * heat, 0, 1)\n","        overlays.append((overlay*255).astype(np.uint8))\n","        count += 1\n","\n","    # remove hooks\n","    try:\n","        fh.remove(); bh.remove()\n","    except Exception:\n","        pass\n","\n","    if len(overlays) == 0:\n","        print(\"[GradCAM] No overlays created for layer\", layer_name)\n","        return\n","\n","    cols = 4; rows = math.ceil(len(overlays)/cols)\n","    grid = Image.new('RGB', (cols*img_size[0], rows*img_size[1]))\n","    for i, arr in enumerate(overlays):\n","        r = i // cols; c = i % cols\n","        grid.paste(Image.fromarray(arr), (c*img_size[0], r*img_size[1]))\n","    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","    grid.save(save_path)\n","    print(\"[GradCAM] Saved grid to\", save_path)\n","\n","\n","# ---------- Helper to automatically find a spatial module by testing forward pass ----------\n","def find_spatial_module_name(factory, sample_image_path, num_classes_local=num_classes, device='cuda', img_size=(224,224)):\n","    \"\"\"\n","    Tries modules one-by-one: registers a forward hook, runs single forward and checks the feature shape.\n","    Returns the first module name whose forward output is a 4D tensor with H>1 and W>1.\n","    \"\"\"\n","    model = factory(num_classes_local).to(device)\n","    model.eval()\n","    modules = list(model.named_modules())\n","\n","    tf_img = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","    pil = Image.open(sample_image_path).convert('RGB').resize(img_size)\n","    x = tf_img(pil).unsqueeze(0).to(device)\n","\n","    found = None\n","    for name, module in modules:\n","        # skip trivial root module\n","        if name == '':\n","            continue\n","        feat_holder = {'out': None}\n","        def fh(m, inp, out):\n","            if isinstance(out, torch.Tensor):\n","                feat_holder['out'] = out\n","            elif isinstance(out, (list,tuple)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n","                feat_holder['out'] = out[0]\n","            else:\n","                feat_holder['out'] = None\n","        h = module.register_forward_hook(fh)\n","        try:\n","            with torch.no_grad():\n","                _ = model(x)\n","            out_t = feat_holder['out']\n","            if isinstance(out_t, torch.Tensor):\n","                # remove batch if present\n","                t = out_t\n","                if t.ndim == 4 and t.shape[0] == 1:\n","                    t = t[0]\n","                if t.ndim == 3:\n","                    # (C,H,W) -> spatial found\n","                    _, H, W = t.shape\n","                    if H > 1 and W > 1:\n","                        found = name\n","                        h.remove()\n","                        break\n","        except Exception:\n","            # if forward failed for this module, continue\n","            pass\n","        h.remove()\n","    # clean up\n","    del model\n","    torch.cuda.empty_cache()\n","    return found\n","\n","# ---------- Run automatic Grad-CAM for top torch models ----------\n","top3 = summary_df['model'].tolist()[:3]\n","for m in top3:\n","    if not m.startswith('torch::'):\n","        continue\n","    mn = m.split('::',1)[1]\n","    if mn not in pytorch_factories:\n","        print(\"[GradCAM] no factory for\", mn); continue\n","    fac = pytorch_factories[mn]\n","    ckpt = os.path.join(OUT_ROOT, f'{mn}_best.pth')\n","    if not os.path.exists(ckpt):\n","        print(\"[GradCAM] checkpoint missing for\", mn, ckpt); continue\n","\n","    print(f\"[GradCAM] Trying model {mn} with checkpoint {ckpt}\")\n","    # choose a sample image to probe (first test image)\n","    sample_img = test_paths[0] if 'test_paths' in globals() and len(test_paths)>0 else test_items[0][0]\n","    try:\n","        chosen_layer = find_spatial_module_name(fac, sample_img, num_classes_local=num_classes, device=DEVICE, img_size=IMG_SIZE)\n","        if chosen_layer is None:\n","            print(f\"[GradCAM] No spatial module automatically found for {mn}; you can choose a conv-like module name manually.\")\n","            continue\n","        print(f\"[GradCAM] Auto-chosen layer for {mn}: {chosen_layer}\")\n","        save_path = os.path.join(PLOTS_DIR, f'{mn}_gradcam_grid.png')\n","        save_gradcam_grid_pytorch(fac, ckpt, chosen_layer, test_paths, save_path, max_images=12, img_size=IMG_SIZE, device=DEVICE)\n","    except Exception as e:\n","        print(\"[GradCAM] Error for model\", mn, e)\n","        traceback.print_exc()\n","\n","print(\"Grad-CAM auto-run complete. Check\", PLOTS_DIR)\n"],"metadata":{"id":"9HuPqXFinoO3","executionInfo":{"status":"ok","timestamp":1763837772647,"user_tz":-330,"elapsed":22205,"user":{"displayName":"Khushboo Kumari","userId":"08877604357129947344"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"08511d37-6550-4ce8-c256-fd5bc1d888bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[GradCAM] Trying model swin_tiny with checkpoint /content/experiments/swin_tiny_best.pth\n","[GradCAM] Auto-chosen layer for swin_tiny: patch_embed\n","[GradCAM] Saved grid to /content/experiments/plots/swin_tiny_gradcam_grid.png\n","[GradCAM] Trying model deit_small_distilled with checkpoint /content/experiments/deit_small_distilled_best.pth\n","[GradCAM] Auto-chosen layer for deit_small_distilled: patch_embed\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/479.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/1010.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3862.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2889.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/669.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5533.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2326.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4167.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4582.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3462.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2237.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4348.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3742.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3931.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5816.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4098.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3204.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1673.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2849.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3060.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3402.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5555.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/872.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5314.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5051.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4890.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2495.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1394.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3810.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/921.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3534.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2754.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/367.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2165.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1124.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3015.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/735.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2319.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2567.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/1530.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4201.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5099.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/136.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/394.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5152.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1548.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5467.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1150.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1935.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3579.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2842.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3491.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4747.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2579.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2711.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4255.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/201.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5812.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3676.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2853.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5322.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4197.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3188.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3504.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1727.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/762.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/196.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3668.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5726.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1055.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5751.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/690.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4077.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3735.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5613.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2756.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3044.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/525.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4467.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3400.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1189.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1724.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1791.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2312.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4336.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3449.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2485.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5379.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1718.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2835.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3162.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/1011.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2402.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5309.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5205.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1556.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4669.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3379.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2134.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2267.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2413.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5445.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2069.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/851.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5360.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/1012.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1396.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1151.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/404.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5804.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2375.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5864.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4728.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2585.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4398.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2606.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1122.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5759.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1699.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1942.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1338.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4593.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/366.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2723.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4745.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5510.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/401.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4590.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5546.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/148.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3163.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2194.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3785.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3618.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/832.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5784.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1125.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/854.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1339.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5728.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4114.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3774.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3913.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3853.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3090.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3578.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2978.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/382.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2166.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1938.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3928.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4852.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1185.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2519.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1164.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/174.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2446.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3270.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/258.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3827.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2970.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2053.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/537.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1323.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2901.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3749.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/1446.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3615.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4286.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2940.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3472.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2967.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5113.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3539.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5427.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5931.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3285.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/532.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4320.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2500.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2572.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3554.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5944.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2187.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2515.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/312.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3555.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/3982.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5439.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5928.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4840.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4655.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4830.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5301.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2595.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2566.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4985.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/262.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/640.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5650.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4936.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3353.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/126.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1945.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3733.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/727.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1931.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4361.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1801.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1908.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/453.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3724.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1786.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5194.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3764.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2600.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1260.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3909.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4441.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4678.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/244.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1560.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/313.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4824.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1828.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/874.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2278.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2879.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3059.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5578.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4346.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4474.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2766.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5539.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/285.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/1040.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3545.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3257.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5755.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5411.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/821.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2720.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1946.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4770.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5398.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/173.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2301.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2811.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2051.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1983.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3435.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/568.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3077.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2478.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2072.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2819.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3956.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2794.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5668.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5778.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4559.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1574.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4427.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2132.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4413.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1743.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/42.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4080.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4422.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3746.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1861.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4011.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/943.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2074.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2482.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4378.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2727.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4423.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1899.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/712.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/876.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4554.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3650.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1744.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/288.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5102.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1973.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2984.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3465.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/686.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5565.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5376.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3167.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4278.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3171.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4121.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4391.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5768.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/920.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5007.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/4996.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5808.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4156.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3969.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2217.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4353.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/610.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1948.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2489.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2268.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2952.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5149.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/1442.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3968.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5556.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3628.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1708.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4869.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3250.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/440.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1809.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1678.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1682.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/828.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5615.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1050.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5738.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4420.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2196.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/1029.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1928.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1623.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2416.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5554.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5745.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3419.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4642.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3523.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/622.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/642.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/119.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1717.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5706.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5086.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3961.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5014.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/711.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2852.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3358.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/791.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4764.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/423.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/2002.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2587.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3938.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/661.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4386.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1133.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1375.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4480.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4562.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4472.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3016.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4371.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4366.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4525.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2125.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4581.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2981.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2131.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1333.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1158.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4407.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4571.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3886.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5634.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5139.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2337.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5109.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/22.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4880.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1922.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/1021.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2753.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4791.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1573.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1807.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4817.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3492.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5292.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/609.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5352.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4113.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2986.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5068.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/92.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3302.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/1006.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1835.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4437.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5033.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/1448.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4974.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2820.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3881.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4421.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3955.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2763.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1216.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5690.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4295.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mango/5030.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1097.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/445.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4302.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1878.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1887.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/816.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1275.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4904.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4829.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5236.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3591.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5936.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3669.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3116.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2251.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4094.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4122.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4047.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5404.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/351.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5211.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3779.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3430.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3680.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5704.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4681.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/516.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5413.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1842.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1604.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5589.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1654.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4317.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2944.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3880.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/605.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5521.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2047.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/227.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3219.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/355.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2096.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3387.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1582.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/293.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/219.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3790.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/1003.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2514.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/1441.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4042.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nagadali/2373.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3295.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4615.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/24.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5789.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2129.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3413.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2100.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5441.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1859.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2891.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3920.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4984.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5422.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/3340.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/471.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5942.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5791.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/318.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1991.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4754.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2698.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/193.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4761.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4483.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5375.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Insulin/5798.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Gauva/2730.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1344.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4610.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4377.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/13.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1559.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4506.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/3004.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1635.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5876.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5412.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3245.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5470.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4132.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3943.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1819.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4117.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5926.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5318.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4419.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5891.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/575.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3593.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5408.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1902.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4788.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4734.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/535.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel/4165.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1246.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5919.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/955.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2574.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3176.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2184.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4560.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/1430.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2542.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4584.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Rose/1962.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/2496.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Arali/4428.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3631.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/73.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nooni/2881.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1161.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3105.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pomegranate/3050.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ganike/3705.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2899.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/1458.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3143.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5657.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Neem/1882.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon_grass/5525.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amruta_Balli/671.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Doddapatre/5435.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5241.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4703.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Hibiscus/1166.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2279.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Honge/3202.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2247.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/123.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Geranium/1109.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Brahmi/4766.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3758.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2547.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Henna/4638.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3499.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3640.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Basale/3794.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Aloevera/4355.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2590.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2103.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Lemon/5161.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4580.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/778.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2888.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Tulasi/3576.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5853.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Wood_sorel/4943.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Betel_Nut/1700.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2923.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1300.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pepper/5256.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Mint/4009.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4572.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4614.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2277.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1405.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/548.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Jasmine/3469.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Pappaya/2154.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashoka/570.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Nithyapushpa/2982.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Sapota/1589.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ashwagandha/2200.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Bamboo/756.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Castor/4533.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Curry_Leaf/3869.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Ekka/2540.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Amla/1380.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Avacado/5700.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] skipping image /content/Medicinal plant dataset/Raktachandini/5932.jpg: unsupported feat/grad shapes -> feat (1, 196, 384), grad (1, 196, 384)\n","[GradCAM] No overlays created for layer patch_embed\n","[GradCAM] Trying model coatnet_0 with checkpoint /content/experiments/coatnet_0_best.pth\n","[GradCAM] Auto-chosen layer for coatnet_0: stem\n","[GradCAM] Saved grid to /content/experiments/plots/coatnet_0_gradcam_grid.png\n","Grad-CAM auto-run complete. Check /content/experiments/plots\n"]}]},{"cell_type":"code","source":["# ============================================\n","#   UNIVERSAL GRAD-CAM (CNN + ViT + Swin)\n","#   - Saves individual images\n","#   - Saves grid\n","# ============================================\n","import cv2, math, os\n","from PIL import Image\n","import numpy as np\n","import torch\n","\n","def save_gradcam_grid_pytorch(factory, ckpt, layer_name, paths, save_path,\n","                              max_images=12, img_size=(224,224),\n","                              device=DEVICE):\n","\n","    # -----------------------\n","    # Load model + checkpoint\n","    # -----------------------\n","    model = factory(num_classes).to(device)\n","    st = torch.load(ckpt, map_location=device)\n","\n","    if isinstance(st, dict) and 'state_dict' in st:\n","        st = st['state_dict']\n","\n","    try: model.load_state_dict(st)\n","    except: model.load_state_dict(st, strict=False)\n","\n","    model.eval()\n","\n","    # -----------------------------------\n","    # Find chosen layer\n","    # -----------------------------------\n","    modules = dict(model.named_modules())\n","    if layer_name not in modules:\n","        print(\"Layer\", layer_name, \"not found. Available:\", list(modules.keys())[-30:])\n","        return\n","\n","    target = modules[layer_name]\n","\n","    # holders\n","    feat_holder = {'feat': None}\n","    grad_holder = {'grad': None}\n","\n","    # forward hook\n","    def fh(m, inp, out):\n","        if isinstance(out, torch.Tensor):\n","            feat_holder['feat'] = out.detach()\n","        elif isinstance(out, (tuple,list)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n","            feat_holder['feat'] = out[0].detach()\n","\n","    # backward hook\n","    def bh(m, gin, gout):\n","        g = gout[0] if isinstance(gout, (tuple,list)) else gout\n","        if isinstance(g, torch.Tensor):\n","            grad_holder['grad'] = g.detach()\n","\n","    # Try full backward hook (new) or fallback\n","    try:\n","        h1 = target.register_forward_hook(fh)\n","        h2 = target.register_full_backward_hook(bh)\n","    except:\n","        h1 = target.register_forward_hook(fh)\n","        h2 = target.register_backward_hook(bh)\n","\n","    # -----------------------\n","    # Preprocessing\n","    # -----------------------\n","    tf_img = transforms.Compose([\n","        transforms.Resize(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","    ])\n","\n","    overlays = []\n","    individual_dir = save_path.replace(\".png\", \"_individual\")\n","    os.makedirs(individual_dir, exist_ok=True)\n","\n","    # -----------------------\n","    # Grad-CAM utility\n","    # -----------------------\n","    def make_cam(feat, grad, target_hw):\n","\n","        if feat is None or grad is None:\n","            return None\n","\n","        # remove batch\n","        if feat.ndim == 4 and feat.shape[0] == 1: feat = feat[0]\n","        if grad.ndim == 4 and grad.shape[0] == 1: grad = grad[0]\n","\n","        # -------------------------------\n","        # Case B: ViT-like (L,C) tokens\n","        # -------------------------------\n","        if feat.ndim == 3 and feat.shape[0] == 1 and feat.shape[2] > feat.shape[1]:\n","            # (1,L,C) ‚Üí remove batch ‚Üí (L,C)\n","            feat = feat[0]\n","            grad = grad[0] if grad is not None and grad.ndim==3 else grad\n","\n","        # (L,C) ‚Üí transpose ‚Üí (C,L)\n","        if feat.ndim == 2 and feat.shape[1] > feat.shape[0]:\n","            feat = feat.permute(1,0)   # (C,L)\n","            if grad is not None:\n","                grad = grad.permute(1,0)\n","\n","        # -----------------------------------\n","        # Case 1: CNN-style (C,H,W)\n","        # -----------------------------------\n","        if feat.ndim == 3 and feat.shape[1] > 1 and feat.shape[2] > 1:\n","            w = grad.mean(dim=(1,2), keepdim=True)\n","            cam = (w * feat).sum(dim=0).cpu().numpy()\n","\n","        # -----------------------------------\n","        # Case 2: Tokens ‚Äî (C,L)\n","        # -----------------------------------\n","        elif feat.ndim == 2:\n","            C,L = feat.shape\n","            s = int(np.sqrt(L))\n","            if s*s != L: return None\n","            w = grad.mean(dim=1)\n","            cam = (w.unsqueeze(1) * feat).sum(dim=0).cpu().numpy()\n","            cam = cam.reshape(s,s)\n","        else:\n","            return None\n","\n","        # normalize\n","        cam = np.maximum(cam,0)\n","        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-9)\n","        cam = cv2.resize(cam, (target_hw[1], target_hw[0]))\n","\n","        return cam\n","\n","    # -----------------------\n","    # Process each test image\n","    # -----------------------\n","    idx = 0\n","    for p in paths[:max_images]:\n","        try:\n","            pil = Image.open(p).convert(\"RGB\").resize(img_size)\n","        except:\n","            continue\n","\n","        x = tf_img(pil).unsqueeze(0).to(device)\n","        x.requires_grad_(True)\n","\n","        feat_holder['feat'] = None\n","        grad_holder['grad'] = None\n","\n","        out = model(x)\n","        pred = int(out.argmax(1))\n","        score = out[0,pred]\n","\n","        model.zero_grad()\n","        try: score.backward(retain_graph=True)\n","        except: score.backward()\n","\n","        feat = feat_holder['feat']\n","        grad = grad_holder['grad']\n","\n","        cam = make_cam(feat, grad, img_size)\n","        if cam is None:\n","            print(\"‚ö†Ô∏è CAM failed for:\", p)\n","            continue\n","\n","        # make overlay\n","        heat = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n","        heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB) / 255.0\n","        base = np.array(pil) / 255.0\n","        overlay = (0.5*base + 0.5*heat)\n","        overlay = np.clip(overlay*255,0,255).astype(np.uint8)\n","\n","        overlays.append(overlay)\n","\n","        # save individual\n","        Image.fromarray(overlay).save(os.path.join(individual_dir, f\"img_{idx+1:02d}.png\"))\n","        idx += 1\n","\n","    # remove hooks\n","    try:\n","        h1.remove(); h2.remove()\n","    except:\n","        pass\n","\n","    if len(overlays)==0:\n","        print(\"‚ùå No Grad-CAM images created for:\", layer_name)\n","        return\n","\n","    # -----------------------\n","    # Make grid\n","    # -----------------------\n","    cols = 4\n","    rows = math.ceil(len(overlays)/cols)\n","    grid = Image.new(\"RGB\", (cols*img_size[0], rows*img_size[1]))\n","\n","    for i,arr in enumerate(overlays):\n","        r = i//cols\n","        c = i%cols\n","        grid.paste(Image.fromarray(arr), (c*img_size[0], r*img_size[1]))\n","\n","    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","    grid.save(save_path)\n","    print(\"‚úÖ Saved Grad-CAM grid:\", save_path)\n","    print(\"üìÇ Individual images saved to:\", individual_dir)\n"],"metadata":{"id":"7bRVf-F0Ab5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üîµ NEW: Replacement Grad-CAM function that RETURNS overlays, paths, preds\n","# Paste this cell into your notebook to replace the old save_gradcam_grid_pytorch OR keep it as a new function.\n","\n","import cv2, math, os\n","from PIL import Image\n","import numpy as np\n","import torch\n","from torchvision import transforms\n","\n","def save_gradcam_grid_pytorch_return(factory, ckpt, layer_name, paths, save_path,\n","                                     max_images=12, img_size=(224,224),\n","                                     device=DEVICE, num_classes=num_classes, idx_to_class=idx_to_class):\n","    \"\"\"\n","    Similar to your original save_gradcam_grid_pytorch but returns:\n","       overlays -> list of numpy arrays (H,W,3) uint8\n","       paths_used -> list of source image paths\n","       preds -> list of predicted class indices (ints)\n","    Also saves individual images and a grid as before.\n","    \"\"\"\n","    # -----------------------\n","    # Load model + checkpoint\n","    # -----------------------\n","    model = factory(num_classes).to(device)\n","    st = torch.load(ckpt, map_location=device)\n","\n","    if isinstance(st, dict) and 'state_dict' in st:\n","        st = st['state_dict']\n","\n","    try: model.load_state_dict(st)\n","    except: model.load_state_dict(st, strict=False)\n","\n","    model.eval()\n","\n","    # -----------------------------------\n","    # Find chosen layer\n","    # -----------------------------------\n","    modules = dict(model.named_modules())\n","    if layer_name not in modules:\n","        print(\"Layer\", layer_name, \"not found. Available (last 30):\", list(modules.keys())[-30:])\n","        return [], [], []\n","\n","    target = modules[layer_name]\n","\n","    # holders\n","    feat_holder = {'feat': None}\n","    grad_holder = {'grad': None}\n","\n","    # forward hook\n","    def fh(m, inp, out):\n","        if isinstance(out, torch.Tensor):\n","            feat_holder['feat'] = out.detach()\n","        elif isinstance(out, (tuple,list)) and len(out)>0 and isinstance(out[0], torch.Tensor):\n","            feat_holder['feat'] = out[0].detach()\n","\n","    # backward hook\n","    def bh(m, gin, gout):\n","        g = gout[0] if isinstance(gout, (tuple,list)) else gout\n","        if isinstance(g, torch.Tensor):\n","            grad_holder['grad'] = g.detach()\n","\n","    # Try full backward hook (new) or fallback\n","    try:\n","        h1 = target.register_forward_hook(fh)\n","        h2 = target.register_full_backward_hook(bh)\n","    except:\n","        h1 = target.register_forward_hook(fh)\n","        h2 = target.register_backward_hook(bh)\n","\n","    # -----------------------\n","    # Preprocessing\n","    # -----------------------\n","    tf_img = transforms.Compose([\n","        transforms.Resize(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","    ])\n","\n","    overlays = []\n","    preds = []\n","    paths_used = []\n","    individual_dir = save_path.replace(\".png\", \"_individual\")\n","    os.makedirs(individual_dir, exist_ok=True)\n","\n","    # -----------------------\n","    # Grad-CAM utility\n","    # -----------------------\n","    def make_cam(feat, grad, target_hw):\n","\n","        if feat is None or grad is None:\n","            return None\n","\n","        # remove batch\n","        if feat.ndim == 4 and feat.shape[0] == 1: feat = feat[0]\n","        if grad is not None and grad.ndim == 4 and grad.shape[0] == 1: grad = grad[0]\n","\n","        # -------------------------------\n","        # Case B: ViT-like (L,C) tokens\n","        # -------------------------------\n","        if feat.ndim == 3 and feat.shape[0] == 1 and feat.shape[2] > feat.shape[1]:\n","            # (1,L,C) ‚Üí remove batch ‚Üí (L,C)\n","            feat = feat[0]\n","            grad = grad[0] if grad is not None and grad.ndim==3 else grad\n","\n","        # (L,C) ‚Üí transpose ‚Üí (C,L)\n","        if feat.ndim == 2 and feat.shape[1] > feat.shape[0]:\n","            feat = feat.permute(1,0)   # (C,L)\n","            if grad is not None:\n","                grad = grad.permute(1,0)\n","\n","        # -----------------------------------\n","        # Case 1: CNN-style (C,H,W)\n","        # -----------------------------------\n","        if feat.ndim == 3 and feat.shape[1] > 1 and feat.shape[2] > 1:\n","            w = grad.mean(dim=(1,2), keepdim=True)\n","            cam = (w * feat).sum(dim=0).cpu().numpy()\n","\n","        # -----------------------------------\n","        # Case 2: Tokens ‚Äî (C,L)\n","        # -----------------------------------\n","        elif feat.ndim == 2:\n","            C,L = feat.shape\n","            s = int(np.sqrt(L))\n","            if s*s != L: return None\n","            w = grad.mean(dim=1)\n","            cam = (w.unsqueeze(1) * feat).sum(dim=0).cpu().numpy()\n","            cam = cam.reshape(s,s)\n","        else:\n","            return None\n","\n","        # normalize\n","        cam = np.maximum(cam,0)\n","        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-9)\n","        cam = cv2.resize(cam, (target_hw[1], target_hw[0]))\n","\n","        return cam\n","\n","    # -----------------------\n","    # Process each test image\n","    # -----------------------\n","    idx = 0\n","    for p in paths[:max_images]:\n","        try:\n","            pil = Image.open(p).convert(\"RGB\").resize(img_size)\n","        except Exception as e:\n","            print(\"Skipping\", p, \" ‚Äî read error:\", e)\n","            continue\n","\n","        x = tf_img(pil).unsqueeze(0).to(device)\n","        x.requires_grad_(True)\n","\n","        feat_holder['feat'] = None\n","        grad_holder['grad'] = None\n","\n","        out = model(x)\n","        pred = int(out.argmax(1))\n","        score = out[0,pred]\n","\n","        model.zero_grad()\n","        try: score.backward(retain_graph=True)\n","        except: score.backward()\n","\n","        feat = feat_holder['feat']\n","        grad = grad_holder['grad']\n","\n","        cam = make_cam(feat, grad, img_size)\n","        if cam is None:\n","            print(\"‚ö†Ô∏è CAM failed for:\", p)\n","            continue\n","\n","        # make overlay\n","        heat = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n","        heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB) / 255.0\n","        base = np.array(pil) / 255.0\n","        overlay = (0.5*base + 0.5*heat)\n","        overlay = np.clip(overlay*255,0,255).astype(np.uint8)\n","\n","        overlays.append(overlay)\n","        preds.append(pred)\n","        paths_used.append(p)\n","\n","        # save individual\n","        Image.fromarray(overlay).save(os.path.join(individual_dir, f\"img_{idx+1:02d}.png\"))\n","        idx += 1\n","\n","    # remove hooks\n","    try:\n","        h1.remove(); h2.remove()\n","    except:\n","        pass\n","\n","    if len(overlays)==0:\n","        print(\"‚ùå No Grad-CAM images created for:\", layer_name)\n","        return [], [], []\n","\n","    # -----------------------\n","    # Make grid\n","    # -----------------------\n","    cols = 4\n","    rows = math.ceil(len(overlays)/cols)\n","    grid = Image.new(\"RGB\", (cols*img_size[0], rows*img_size[1]))\n","\n","    for i,arr in enumerate(overlays):\n","        r = i//cols\n","        c = i%cols\n","        grid.paste(Image.fromarray(arr), (c*img_size[0], r*img_size[1]))\n","\n","    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","    grid.save(save_path)\n","    print(\"‚úÖ Saved Grad-CAM grid:\", save_path)\n","    print(\"üìÇ Individual images saved to:\", individual_dir)\n","\n","    # üîµ NEW: return overlays, source paths and preds\n","    return overlays, paths_used, preds\n","\n","\n","# üîµ NEW: Interactive slider viewer that shows filename + predicted class\n","def interactive_overlay_slider_with_labels(overlays, paths, preds, idx_to_class_map=None, start=0, figsize=(6,6)):\n","    \"\"\"\n","    overlays: list of numpy arrays (H,W,3) uint8\n","    paths: list of corresponding source file paths\n","    preds: list of predicted class indices\n","    idx_to_class_map: dict mapping index -> class name (optional)\n","    \"\"\"\n","    try:\n","        from ipywidgets import widgets, interact\n","        import matplotlib.pyplot as plt\n","        import numpy as np\n","    except Exception as e:\n","        print(\"ipywidgets not available. Install with: !pip install ipywidgets\")\n","        print(\"Error:\", e)\n","        return\n","\n","    if len(overlays) == 0:\n","        print(\"No overlays to display.\")\n","        return\n","\n","    max_index = len(overlays) - 1\n","\n","    def _show(i=0):\n","        plt.figure(figsize=figsize)\n","        img = overlays[int(i)]\n","        if img.dtype != np.uint8:\n","            img_show = (img * 255).astype('uint8')\n","        else:\n","            img_show = img\n","        title_parts = [f\"{int(i)+1}/{len(overlays)}\"]\n","        # filename\n","        try:\n","            fname = os.path.basename(paths[int(i)])\n","            title_parts.append(fname)\n","        except:\n","            pass\n","        # predicted label\n","        try:\n","            lab = idx_to_class_map[preds[int(i)]] if idx_to_class_map is not None else preds[int(i)]\n","            title_parts.append(f\"pred: {lab}\")\n","        except:\n","            title_parts.append(f\"pred: {preds[int(i)]}\")\n","        plt.imshow(img_show)\n","        plt.axis('off')\n","        plt.title(\"  |  \".join(title_parts))\n","        plt.show()\n","\n","    interact(_show,\n","             i=widgets.IntSlider(min=0, max=max_index, step=1, value=start, description='Index'))\n"],"metadata":{"id":"0py0Df8-XHU4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # ============================================================\n","# # FEW-SHOT PLOTTING & EXPORT (READY TO PASTE)\n","# # Paste this right after the few-shot cell that produced accs_5shot & accs_1shot\n","# # ============================================================\n","\n","# ### >>> NEW\n","# import os\n","# import pandas as pd\n","# import matplotlib.pyplot as plt\n","# import seaborn as sns\n","# import numpy as np\n","\n","# ### >>> UPDATED\n","# # use existing PLOTS_DIR and OUT_ROOT variables from your notebook\n","# PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(OUT_ROOT, 'plots'))\n","# os.makedirs(PLOTS_DIR, exist_ok=True)\n","\n","# ### >>> NEW\n","# # Ensure accs_5shot and accs_1shot exist\n","# if 'accs_5shot' not in globals() or 'accs_1shot' not in globals():\n","#     raise RuntimeError(\"accs_5shot or accs_1shot not found. Run the few-shot cell first.\")\n","\n","# # Stats\n","# accs5 = np.array(accs_5shot)\n","# accs1 = np.array(accs_1shot)\n","\n","# summary = {\n","#     'setting': ['5-way-5-shot', '5-way-1-shot'],\n","#     'mean_acc': [accs5.mean(), accs1.mean()],\n","#     'std_acc':  [accs5.std(ddof=1), accs1.std(ddof=1)],\n","#     'median':   [np.median(accs5), np.median(accs1)],\n","#     'n_episodes':[len(accs5), len(accs1)]\n","# }\n","# df_summary = pd.DataFrame(summary)\n","# ### >>> NEW\n","# # Save summary CSV\n","# csv_out = os.path.join(OUT_ROOT, 'fewshot_summary.csv')\n","# df_summary.to_csv(csv_out, index=False)\n","# print(\"Saved few-shot summary CSV ->\", csv_out)\n","\n","# # ---------- Plot 1: Histogram + KDE overlay ----------\n","# plt.figure(figsize=(8,5))\n","# sns.histplot(accs5, label='5-shot', stat='density', kde=True, alpha=0.6)\n","# sns.histplot(accs1, label='1-shot', stat='density', kde=True, alpha=0.6)\n","# plt.xlabel('Episode Accuracy')\n","# plt.title('Few-Shot Episode Accuracy Distribution')\n","# plt.legend()\n","# hist_path = os.path.join(PLOTS_DIR, 'fewshot_accuracy_histogram.png')\n","# plt.tight_layout(); plt.savefig(hist_path); plt.close()\n","# print(\"Saved histogram ->\", hist_path)\n","\n","# # ---------- Plot 2: Boxplot comparison ----------\n","# plt.figure(figsize=(6,5))\n","# sns.boxplot(data=[accs5, accs1])\n","# plt.xticks([0,1], ['5-way-5-shot', '5-way-1-shot'])\n","# plt.ylabel('Episode Accuracy')\n","# plt.title('Few-Shot Accuracy Boxplot')\n","# box_path = os.path.join(PLOTS_DIR, 'fewshot_accuracy_boxplot.png')\n","# plt.tight_layout(); plt.savefig(box_path); plt.close()\n","# print(\"Saved boxplot ->\", box_path)\n","\n","# # ---------- Plot 3: Mean ¬± STD bar plot ----------\n","# plt.figure(figsize=(6,4))\n","# means = [accs5.mean()*100, accs1.mean()*100]\n","# stds  = [accs5.std(ddof=1)*100, accs1.std(ddof=1)*100]\n","# sns.barplot(x=['5-shot','1-shot'], y=means, yerr=stds, capsize=0.15)\n","# plt.ylabel('Mean Accuracy (%)')\n","# plt.title('Few-Shot Mean Accuracy ¬± Std')\n","# for i, v in enumerate(means):\n","#     plt.text(i, v + stds[i] + 0.5, f\"{v:.2f}% ¬± {stds[i]:.2f}%\", ha='center')\n","# bar_path = os.path.join(PLOTS_DIR, 'fewshot_mean_std_bar.png')\n","# plt.tight_layout(); plt.savefig(bar_path); plt.close()\n","# print(\"Saved mean¬±std bar ->\", bar_path)\n","\n","# # ---------- Plot 4: Episode-level scatter (optional insight) ----------\n","# plt.figure(figsize=(10,4))\n","# plt.plot(np.arange(len(accs5)), np.sort(accs5)[::-1], label='5-shot (sorted)', alpha=0.8)\n","# plt.plot(np.arange(len(accs1)), np.sort(accs1)[::-1], label='1-shot (sorted)', alpha=0.8)\n","# plt.xlabel('Episode index (sorted)')\n","# plt.ylabel('Accuracy')\n","# plt.title('Sorted Episode Accuracies (descending)')\n","# plt.legend()\n","# scatter_path = os.path.join(PLOTS_DIR, 'fewshot_sorted_episodes.png')\n","# plt.tight_layout(); plt.savefig(scatter_path); plt.close()\n","# print(\"Saved sorted-episodes plot ->\", scatter_path)\n","\n","# # ---------- Print quick summary to notebook output ----------\n","# print(\"\\nFew-shot summary (printed):\")\n","# print(df_summary.to_string(index=False))\n","\n","# ### >>> NEW\n","# # Optional: add results to your combined summary CSV if it exists\n","# combined_csv = os.path.join(OUT_ROOT, 'combined_summary.csv')\n","# if os.path.exists(combined_csv):\n","#     try:\n","#         combined = pd.read_csv(combined_csv)\n","#         fewshot_row = {\n","#             'model': 'fewshot::swin_proto' if BACKBONE_NAME.startswith('swin') else f'fewshot::{BACKBONE_NAME}_proto',\n","#             'accuracy': df_summary.loc[df_summary['setting']=='5-way-5-shot','mean_acc'].values[0],\n","#             'macro_precision': np.nan, 'macro_recall': np.nan, 'macro_f1': np.nan\n","#         }\n","#         combined = pd.concat([combined, pd.DataFrame([fewshot_row])], ignore_index=True)\n","#         combined.to_csv(os.path.join(OUT_ROOT,'combined_summary_with_fewshot.csv'), index=False)\n","#         print(\"Appended few-shot summary to combined_summary -> combined_summary_with_fewshot.csv\")\n","#     except Exception as e:\n","#         print(\"Could not append to combined_summary:\", e)\n","\n","# print(\"\\nAll few-shot plots saved in:\", PLOTS_DIR)\n"],"metadata":{"id":"UIMQsSwIgtV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # ============================================================\n","# #  FEW-SHOT PROTOTYPE NETWORK (READY TO PASTE)\n","# #  - integrates with your existing PyTorch pipeline\n","# #  - few-shot 5-way-5-shot & 5-way-1-shot classification\n","# #  - uses your existing timm backbones (Swin/DeiT/CoAtNet)\n","# # ============================================================\n","\n","# ### >>> NEW\n","# import random, math\n","# import numpy as np\n","# import torch, torch.nn as nn, torch.optim as optim\n","# from torchvision import transforms\n","# from PIL import Image\n","# from collections import defaultdict\n","\n","# device = DEVICE  # uses your existing device\n","\n","# ### >>> NEW\n","# # ------------------------------------------------------------\n","# # 1. BUILD EMBEDDING EXTRACTOR (BACKBONE WITHOUT CLASSIFIER)\n","# # ------------------------------------------------------------\n","# def make_embedding_extractor(factory, embed_dim=512):\n","#     \"\"\"\n","#     Converts any timm model into an embedding extractor by removing the classifier.\n","#     \"\"\"\n","#     model = factory(num_classes)  # your factory already expects num_classes\n","#     model.reset_classifier(0) if hasattr(model, \"reset_classifier\") else None\n","\n","#     # Projection head to get fixed-dim embeddings\n","#     feat_dim = getattr(model, \"num_features\", embed_dim)\n","#     proj = nn.Sequential(\n","#         nn.AdaptiveAvgPool2d(1),\n","#         nn.Flatten(),\n","#         nn.Linear(feat_dim, embed_dim)\n","#     )\n","\n","#     model = model.to(device)\n","#     proj = proj.to(device)\n","\n","#     # Freeze backbone, train projection only\n","#     for p in model.parameters():\n","#         p.requires_grad = False\n","#     for p in proj.parameters():\n","#         p.requires_grad = True\n","\n","#     return model, proj\n","\n","\n","# ### >>> NEW\n","# # Choose backbone (you can switch between swin_tiny, deit_small, coatnet)\n","# BACKBONE_NAME = \"swin_tiny\"   # CHANGE if you want deit_small_distilled / coatnet_0\n","# backbone_factory = pytorch_factories[BACKBONE_NAME]\n","\n","# backbone, proj_head = make_embedding_extractor(backbone_factory, embed_dim=512)\n","\n","\n","# # ------------------------------------------------------------\n","# # 2. FEW-SHOT EPISODE SAMPLER\n","# # ------------------------------------------------------------\n","# ### >>> NEW\n","# class FewShotDataset:\n","#     def __init__(self, items, img_size=(224,224)):\n","#         self.items = items\n","#         self.img_size = img_size\n","\n","#         self.by_class = defaultdict(list)\n","#         for p, lbl in items:\n","#             self.by_class[lbl].append(p)\n","\n","#         self.classes = sorted(self.by_class.keys())\n","\n","#         self.transform = transforms.Compose([\n","#             transforms.Resize(img_size),\n","#             transforms.RandomResizedCrop(img_size[0], scale=(0.7,1.0)),\n","#             transforms.RandomHorizontalFlip(),\n","#             transforms.ColorJitter(0.3,0.3,0.2,0.05),\n","#             transforms.ToTensor(),\n","#             transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","#         ])\n","\n","#     # sample N-way/K-shot episode\n","#     def sample_episode(self, N_way=5, K_shot=5, Q_query=5):\n","#         chosen = random.sample(self.classes, N_way)\n","#         support, support_labels = [], []\n","#         query, query_labels = [], []\n","\n","#         for i, cls in enumerate(chosen):\n","#             imgs = random.sample(self.by_class[cls], K_shot + Q_query)\n","#             support.extend(imgs[:K_shot])\n","#             query.extend(imgs[K_shot:])\n","#             support_labels.extend([i]*K_shot)\n","#             query_labels.extend([i]*Q_query)\n","\n","#         return support, support_labels, query, query_labels\n","\n","\n","# fs_train = FewShotDataset(train_items, img_size=IMG_SIZE)\n","# fs_val   = FewShotDataset(val_items, img_size=IMG_SIZE)\n","\n","\n","# # ------------------------------------------------------------\n","# # 3. EMBEDDING FUNCTION\n","# # ------------------------------------------------------------\n","# ### >>> NEW\n","# def embed_batch(backbone, proj, paths):\n","#     backbone.eval(); proj.eval()\n","#     embeds = []\n","#     tf = transforms.Compose([\n","#         transforms.Resize(IMG_SIZE),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n","#     ])\n","\n","#     with torch.no_grad():\n","#         for p in paths:\n","#             im = Image.open(p).convert(\"RGB\")\n","#             x = tf(im).unsqueeze(0).to(device)\n","\n","#             feats = backbone.forward_features(x) if hasattr(backbone,\"forward_features\") else backbone(x)\n","#             embedding = proj(feats)\n","#             embedding = nn.functional.normalize(embedding, dim=1)\n","\n","#             embeds.append(embedding.cpu().numpy())\n","\n","#     return np.vstack(embeds)\n","\n","\n","# # ------------------------------------------------------------\n","# # 4. PROTOTYPE CREATION\n","# # ------------------------------------------------------------\n","# ### >>> NEW\n","# def compute_prototypes(support_emb, support_labels, N_way):\n","#     D = support_emb.shape[1]\n","#     prototypes = np.zeros((N_way, D), dtype=np.float32)\n","\n","#     for c in range(N_way):\n","#         idxs = [i for i,l in enumerate(support_labels) if l == c]\n","#         prototypes[c] = support_emb[idxs].mean(axis=0)\n","\n","#     # normalize\n","#     prototypes /= (np.linalg.norm(prototypes, axis=1, keepdims=True) + 1e-9)\n","#     return prototypes\n","\n","\n","# # ------------------------------------------------------------\n","# # 5. EVALUATE SINGLE EPISODE\n","# # ------------------------------------------------------------\n","# ### >>> NEW\n","# def evaluate_episode(backbone, proj, fs_dataset, N_way=5, K_shot=5, Q_query=5):\n","#     support, s_lbl, query, q_lbl = fs_dataset.sample_episode(N_way, K_shot, Q_query)\n","\n","#     s_emb = embed_batch(backbone, proj, support)\n","#     q_emb = embed_batch(backbone, proj, query)\n","#     prototypes = compute_prototypes(s_emb, s_lbl, N_way)\n","\n","#     # distances\n","#     logits = q_emb @ prototypes.T\n","#     preds = np.argmax(logits, axis=1)\n","#     acc = (preds == np.array(q_lbl)).mean()\n","\n","#     return acc\n","\n","\n","# # ------------------------------------------------------------\n","# # 6. FULL FEW-SHOT EVALUATION (RUN THIS CELL)\n","# # ------------------------------------------------------------\n","# ### >>> NEW\n","# print(\"\\n==================================================\")\n","# print(\" FEW-SHOT TESTING (5-way-5-shot and 5-way-1-shot)\")\n","# print(\"==================================================\\n\")\n","\n","# episodes = 200  # you can increase to 1000\n","\n","# # 5-way-5-shot\n","# accs_5shot = []\n","# for _ in range(episodes):\n","#     accs_5shot.append(evaluate_episode(backbone, proj_head, fs_val, N_way=5, K_shot=5, Q_query=10))\n","\n","# # 5-way-1-shot\n","# accs_1shot = []\n","# for _ in range(episodes):\n","#     accs_1shot.append(evaluate_episode(backbone, proj_head, fs_val, N_way=5, K_shot=1, Q_query=10))\n","\n","\n","# print(f\"5-way 5-shot accuracy: {np.mean(accs_5shot)*100:.2f}%\")\n","# print(f\"5-way 1-shot accuracy: {np.mean(accs_1shot)*100:.2f}%\")\n"],"metadata":{"id":"wLQrQ5XugBsP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # ============================================================\n","# # HYBRID FUSION FEW-SHOT PROTOTYPE NETWORK\n","# # - Fuse embeddings from Swin, DeiT, CoAtNet\n","# # - Evaluate 5-way 5-shot and 5-way 1-shot\n","# # - Produces plots and appends to combined_summary if present\n","# # Paste this cell AFTER your existing FEW-SHOT block\n","# # ============================================================\n","\n","# import numpy as np, os\n","# from tqdm import tqdm\n","\n","# # --- 1) Build embedding extractors (frozen backbones + small proj heads) ---\n","# # use embed_dim 256 per backbone to keep fused dim manageable\n","# EMBED_DIM = 256\n","\n","# # create three independent embedding extractors (backbone + proj)\n","# backbone_swin, proj_swin = make_embedding_extractor(pytorch_factories['swin_tiny'], embed_dim=EMBED_DIM)\n","# backbone_deit, proj_deit = make_embedding_extractor(pytorch_factories['deit_small_distilled'], embed_dim=EMBED_DIM)\n","# backbone_coat, proj_coat = make_embedding_extractor(pytorch_factories['coatnet_0'], embed_dim=EMBED_DIM)\n","\n","# # Put them in eval mode (proj heads are trainable by default in your extractor but here we assume evaluation-only)\n","# backbone_swin.eval(); proj_swin.eval()\n","# backbone_deit.eval(); proj_deit.eval()\n","# backbone_coat.eval(); proj_coat.eval()\n","\n","# # --- 2) fused-embedding helper that calls your existing embed_batch() ---\n","# def embed_fusion(paths):\n","#     \"\"\"\n","#     Given list of image file paths -> returns L2-normalized fused embeddings (N x (3*EMBED_DIM))\n","#     Uses embed_batch(backbone, proj, paths) already defined in notebook.\n","#     \"\"\"\n","#     # compute each embedding (these return numpy arrays N x EMBED_DIM)\n","#     emb_s = embed_batch(backbone_swin, proj_swin, paths)\n","#     emb_d = embed_batch(backbone_deit, proj_deit, paths)\n","#     emb_c = embed_batch(backbone_coat, proj_coat, paths)\n","\n","#     # concat\n","#     fused = np.concatenate([emb_s, emb_d, emb_c], axis=1)\n","#     # L2 normalize per-sample\n","#     fused = fused / (np.linalg.norm(fused, axis=1, keepdims=True) + 1e-9)\n","#     return fused\n","\n","# # --- 3) reuse compute_prototypes (already in notebook) ---\n","# # compute_prototypes(support_emb, support_labels, N_way) -> prototypes normalized\n","\n","# # --- 4) evaluate a single fusion episode ---\n","# def evaluate_episode_fusion(fs_dataset, N_way=5, K_shot=5, Q_query=10):\n","#     support, s_lbl, query, q_lbl = fs_dataset.sample_episode(N_way=N_way, K_shot=K_shot, Q_query=Q_query)\n","#     s_emb = embed_fusion(support)\n","#     q_emb = embed_fusion(query)\n","\n","#     prototypes = compute_prototypes(s_emb, s_lbl, N_way)\n","#     logits = q_emb @ prototypes.T   # cosine-like scores because embeddings normalized\n","#     preds = np.argmax(logits, axis=1)\n","#     acc = (preds == np.array(q_lbl)).mean()\n","#     return acc\n","\n","# # --- 5) Run evaluation (episodes) ---\n","# episodes = 200   # increase to 1000 for final runs if you have time\n","\n","# print(\"\\nRunning Hybrid-Fusion few-shot evaluation (this may take a while)...\")\n","# accs_fusion_5shot = []\n","# for _ in tqdm(range(episodes), desc='fusion 5-shot'):\n","#     accs_fusion_5shot.append(evaluate_episode_fusion(fs_val, N_way=5, K_shot=5, Q_query=10))\n","\n","# accs_fusion_1shot = []\n","# for _ in tqdm(range(episodes), desc='fusion 1-shot'):\n","#     accs_fusion_1shot.append(evaluate_episode_fusion(fs_val, N_way=5, K_shot=1, Q_query=10))\n","\n","# # --- 6) Summarize & save results + plots (integrates with your PLOTS_DIR + OUT_ROOT) ---\n","# import matplotlib.pyplot as plt\n","# import seaborn as sns\n","# import pandas as pd\n","\n","# PLOTS_DIR = globals().get('PLOTS_DIR', os.path.join(OUT_ROOT, 'plots'))\n","# os.makedirs(PLOTS_DIR, exist_ok=True)\n","\n","# acc5 = np.array(accs_fusion_5shot)\n","# acc1 = np.array(accs_fusion_1shot)\n","\n","# summary = {\n","#     'setting': ['5-way-5-shot', '5-way-1-shot'],\n","#     'mean_acc': [acc5.mean(), acc1.mean()],\n","#     'std_acc':  [acc5.std(ddof=1), acc1.std(ddof=1)],\n","#     'median':   [np.median(acc5), np.median(acc1)],\n","#     'n_episodes':[len(acc5), len(acc1)]\n","# }\n","# df_summary = pd.DataFrame(summary)\n","# csv_out = os.path.join(OUT_ROOT, 'fewshot_hybrid_fusion_summary.csv')\n","# df_summary.to_csv(csv_out, index=False)\n","# print(\"Saved fusion summary CSV ->\", csv_out)\n","\n","# # Histogram\n","# plt.figure(figsize=(8,5))\n","# sns.histplot(acc5, label='fusion 5-shot', stat='density', kde=True, alpha=0.6)\n","# sns.histplot(acc1, label='fusion 1-shot', stat='density', kde=True, alpha=0.6)\n","# plt.xlabel('Episode Accuracy'); plt.title('Fusion Few-Shot Episode Accuracy')\n","# plt.legend()\n","# hist_path = os.path.join(PLOTS_DIR, 'fusion_fewshot_accuracy_histogram.png')\n","# plt.tight_layout(); plt.savefig(hist_path); plt.close()\n","# print(\"Saved histogram ->\", hist_path)\n","\n","# # Boxplot\n","# plt.figure(figsize=(6,5))\n","# sns.boxplot(data=[acc5, acc1])\n","# plt.xticks([0,1], ['5-way-5-shot', '5-way-1-shot'])\n","# plt.ylabel('Episode Accuracy')\n","# plt.title('Fusion Few-Shot Accuracy Boxplot')\n","# box_path = os.path.join(PLOTS_DIR, 'fusion_fewshot_boxplot.png')\n","# plt.tight_layout(); plt.savefig(box_path); plt.close()\n","# print(\"Saved boxplot ->\", box_path)\n","\n","# # Mean ¬± std bar\n","# plt.figure(figsize=(6,4))\n","# means = [acc5.mean()*100, acc1.mean()*100]\n","# stds  = [acc5.std(ddof=1)*100, acc1.std(ddof=1)*100]\n","# sns.barplot(x=['5-shot','1-shot'], y=means, yerr=stds, capsize=0.15)\n","# plt.ylabel('Mean Accuracy (%)'); plt.title('Fusion Few-Shot Mean Accuracy ¬± Std')\n","# for i, v in enumerate(means):\n","#     plt.text(i, v + stds[i] + 0.5, f\"{v:.2f}% ¬± {stds[i]:.2f}%\", ha='center')\n","# bar_path = os.path.join(PLOTS_DIR, 'fusion_fewshot_mean_std_bar.png')\n","# plt.tight_layout(); plt.savefig(bar_path); plt.close()\n","# print(\"Saved mean¬±std bar ->\", bar_path)\n","\n","# # Print summary\n","# print(\"\\nHybrid Fusion few-shot summary:\")\n","# print(df_summary.to_string(index=False))\n","\n","# # --- 7) Append to combined_summary.csv if present (safe) ---\n","# combined_csv = os.path.join(OUT_ROOT, 'combined_summary.csv')\n","# try:\n","#     if os.path.exists(combined_csv):\n","#         combined = pd.read_csv(combined_csv)\n","#         fewshot_row = {\n","#             'model': 'fewshot::hybrid_fusion',\n","#             'accuracy': float(df_summary.loc[df_summary['setting']=='5-way-5-shot','mean_acc'].values[0]),\n","#             'macro_precision': np.nan, 'macro_recall': np.nan, 'macro_f1': np.nan\n","#         }\n","#         combined = pd.concat([combined, pd.DataFrame([fewshot_row])], ignore_index=True)\n","#         combined.to_csv(os.path.join(OUT_ROOT,'combined_summary_with_fewshot_hybrid.csv'), index=False)\n","#         print(\"Appended fusion summary to combined_summary -> combined_summary_with_fewshot_hybrid.csv\")\n","# except Exception as e:\n","#     print(\"Could not append to combined_summary:\", e)\n","\n","# print(\"\\nFusion few-shot artifacts saved in:\", PLOTS_DIR)\n"],"metadata":{"id":"FVq-7U_q5VNc"},"execution_count":null,"outputs":[]}]}